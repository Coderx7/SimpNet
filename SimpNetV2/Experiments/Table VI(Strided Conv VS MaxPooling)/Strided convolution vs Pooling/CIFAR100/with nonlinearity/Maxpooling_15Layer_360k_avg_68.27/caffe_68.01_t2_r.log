
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate 
I1211 13:47:55.822448  8328 caffe.cpp:219] Using GPUs 0
I1211 13:47:56.008464  8328 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 13:47:56.307523  8328 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:47:56.324530  8328 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 13:47:56.325029  8328 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:47:56.325546  8328 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:47:56.325546  8328 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added1
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added2
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 13:47:56.326045  8328 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 13:47:56.326045  8328 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 13:47:56.421749  8328 layer_factory.cpp:58] Creating layer cifar
I1211 13:47:56.427762  8328 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 13:47:56.427762  8328 net.cpp:84] Creating Layer cifar
I1211 13:47:56.427762  8328 net.cpp:380] cifar -> data
I1211 13:47:56.427762  8328 net.cpp:380] cifar -> label
I1211 13:47:56.428764  8328 data_layer.cpp:45] output data size: 100,3,32,32
I1211 13:47:56.440760  8328 net.cpp:122] Setting up cifar
I1211 13:47:56.440760  8328 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 13:47:56.440760  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.440760  8328 net.cpp:137] Memory required for data: 1229200
I1211 13:47:56.440760  8328 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 13:47:56.440760  8328 net.cpp:84] Creating Layer label_cifar_1_split
I1211 13:47:56.440760  8328 net.cpp:406] label_cifar_1_split <- label
I1211 13:47:56.440760  8328 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 13:47:56.440760  8328 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 13:47:56.440760  8328 net.cpp:122] Setting up label_cifar_1_split
I1211 13:47:56.440760  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.440760  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.440760  8328 net.cpp:137] Memory required for data: 1230000
I1211 13:47:56.440760  8328 layer_factory.cpp:58] Creating layer conv1
I1211 13:47:56.440760  8328 net.cpp:84] Creating Layer conv1
I1211 13:47:56.440760  8328 net.cpp:406] conv1 <- data
I1211 13:47:56.440760  8328 net.cpp:380] conv1 -> conv1
I1211 13:47:56.441763 14664 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:47:56.695796  8328 net.cpp:122] Setting up conv1
I1211 13:47:56.696799  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.696799  8328 net.cpp:137] Memory required for data: 13518000
I1211 13:47:56.696799  8328 layer_factory.cpp:58] Creating layer bn1
I1211 13:47:56.696799  8328 net.cpp:84] Creating Layer bn1
I1211 13:47:56.696799  8328 net.cpp:406] bn1 <- conv1
I1211 13:47:56.696799  8328 net.cpp:367] bn1 -> conv1 (in-place)
I1211 13:47:56.696799  8328 net.cpp:122] Setting up bn1
I1211 13:47:56.696799  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.696799  8328 net.cpp:137] Memory required for data: 25806000
I1211 13:47:56.696799  8328 layer_factory.cpp:58] Creating layer scale1
I1211 13:47:56.696799  8328 net.cpp:84] Creating Layer scale1
I1211 13:47:56.696799  8328 net.cpp:406] scale1 <- conv1
I1211 13:47:56.696799  8328 net.cpp:367] scale1 -> conv1 (in-place)
I1211 13:47:56.696799  8328 layer_factory.cpp:58] Creating layer scale1
I1211 13:47:56.696799  8328 net.cpp:122] Setting up scale1
I1211 13:47:56.696799  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.696799  8328 net.cpp:137] Memory required for data: 38094000
I1211 13:47:56.696799  8328 layer_factory.cpp:58] Creating layer relu1
I1211 13:47:56.696799  8328 net.cpp:84] Creating Layer relu1
I1211 13:47:56.696799  8328 net.cpp:406] relu1 <- conv1
I1211 13:47:56.696799  8328 net.cpp:367] relu1 -> conv1 (in-place)
I1211 13:47:56.696799  8328 net.cpp:122] Setting up relu1
I1211 13:47:56.696799  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.696799  8328 net.cpp:137] Memory required for data: 50382000
I1211 13:47:56.696799  8328 layer_factory.cpp:58] Creating layer conv1_0
I1211 13:47:56.696799  8328 net.cpp:84] Creating Layer conv1_0
I1211 13:47:56.696799  8328 net.cpp:406] conv1_0 <- conv1
I1211 13:47:56.696799  8328 net.cpp:380] conv1_0 -> conv1_0
I1211 13:47:56.698798  8328 net.cpp:122] Setting up conv1_0
I1211 13:47:56.698798  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.698798  8328 net.cpp:137] Memory required for data: 66766000
I1211 13:47:56.698798  8328 layer_factory.cpp:58] Creating layer bn1_0
I1211 13:47:56.698798  8328 net.cpp:84] Creating Layer bn1_0
I1211 13:47:56.698798  8328 net.cpp:406] bn1_0 <- conv1_0
I1211 13:47:56.698798  8328 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 13:47:56.698798  8328 net.cpp:122] Setting up bn1_0
I1211 13:47:56.698798  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.698798  8328 net.cpp:137] Memory required for data: 83150000
I1211 13:47:56.698798  8328 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:47:56.698798  8328 net.cpp:84] Creating Layer scale1_0
I1211 13:47:56.698798  8328 net.cpp:406] scale1_0 <- conv1_0
I1211 13:47:56.698798  8328 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 13:47:56.698798  8328 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:47:56.698798  8328 net.cpp:122] Setting up scale1_0
I1211 13:47:56.698798  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.698798  8328 net.cpp:137] Memory required for data: 99534000
I1211 13:47:56.698798  8328 layer_factory.cpp:58] Creating layer relu1_0
I1211 13:47:56.698798  8328 net.cpp:84] Creating Layer relu1_0
I1211 13:47:56.698798  8328 net.cpp:406] relu1_0 <- conv1_0
I1211 13:47:56.698798  8328 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 13:47:56.699800  8328 net.cpp:122] Setting up relu1_0
I1211 13:47:56.699800  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.699800  8328 net.cpp:137] Memory required for data: 115918000
I1211 13:47:56.699800  8328 layer_factory.cpp:58] Creating layer conv2
I1211 13:47:56.699800  8328 net.cpp:84] Creating Layer conv2
I1211 13:47:56.699800  8328 net.cpp:406] conv2 <- conv1_0
I1211 13:47:56.699800  8328 net.cpp:380] conv2 -> conv2
I1211 13:47:56.700799  8328 net.cpp:122] Setting up conv2
I1211 13:47:56.700799  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.700799  8328 net.cpp:137] Memory required for data: 132302000
I1211 13:47:56.700799  8328 layer_factory.cpp:58] Creating layer bn2
I1211 13:47:56.700799  8328 net.cpp:84] Creating Layer bn2
I1211 13:47:56.700799  8328 net.cpp:406] bn2 <- conv2
I1211 13:47:56.700799  8328 net.cpp:367] bn2 -> conv2 (in-place)
I1211 13:47:56.700799  8328 net.cpp:122] Setting up bn2
I1211 13:47:56.700799  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.700799  8328 net.cpp:137] Memory required for data: 148686000
I1211 13:47:56.700799  8328 layer_factory.cpp:58] Creating layer scale2
I1211 13:47:56.700799  8328 net.cpp:84] Creating Layer scale2
I1211 13:47:56.700799  8328 net.cpp:406] scale2 <- conv2
I1211 13:47:56.700799  8328 net.cpp:367] scale2 -> conv2 (in-place)
I1211 13:47:56.700799  8328 layer_factory.cpp:58] Creating layer scale2
I1211 13:47:56.700799  8328 net.cpp:122] Setting up scale2
I1211 13:47:56.700799  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.700799  8328 net.cpp:137] Memory required for data: 165070000
I1211 13:47:56.700799  8328 layer_factory.cpp:58] Creating layer relu2
I1211 13:47:56.700799  8328 net.cpp:84] Creating Layer relu2
I1211 13:47:56.700799  8328 net.cpp:406] relu2 <- conv2
I1211 13:47:56.700799  8328 net.cpp:367] relu2 -> conv2 (in-place)
I1211 13:47:56.700799  8328 net.cpp:122] Setting up relu2
I1211 13:47:56.700799  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.700799  8328 net.cpp:137] Memory required for data: 181454000
I1211 13:47:56.700799  8328 layer_factory.cpp:58] Creating layer conv2_1
I1211 13:47:56.700799  8328 net.cpp:84] Creating Layer conv2_1
I1211 13:47:56.700799  8328 net.cpp:406] conv2_1 <- conv2
I1211 13:47:56.700799  8328 net.cpp:380] conv2_1 -> conv2_1
I1211 13:47:56.701799  8328 net.cpp:122] Setting up conv2_1
I1211 13:47:56.701799  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.701799  8328 net.cpp:137] Memory required for data: 197838000
I1211 13:47:56.701799  8328 layer_factory.cpp:58] Creating layer bn2_1
I1211 13:47:56.701799  8328 net.cpp:84] Creating Layer bn2_1
I1211 13:47:56.701799  8328 net.cpp:406] bn2_1 <- conv2_1
I1211 13:47:56.702800  8328 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 13:47:56.702800  8328 net.cpp:122] Setting up bn2_1
I1211 13:47:56.702800  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.702800  8328 net.cpp:137] Memory required for data: 214222000
I1211 13:47:56.702800  8328 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:47:56.702800  8328 net.cpp:84] Creating Layer scale2_1
I1211 13:47:56.702800  8328 net.cpp:406] scale2_1 <- conv2_1
I1211 13:47:56.702800  8328 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 13:47:56.702800  8328 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:47:56.702800  8328 net.cpp:122] Setting up scale2_1
I1211 13:47:56.702800  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.702800  8328 net.cpp:137] Memory required for data: 230606000
I1211 13:47:56.702800  8328 layer_factory.cpp:58] Creating layer relu2_1
I1211 13:47:56.702800  8328 net.cpp:84] Creating Layer relu2_1
I1211 13:47:56.702800  8328 net.cpp:406] relu2_1 <- conv2_1
I1211 13:47:56.702800  8328 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 13:47:56.702800  8328 net.cpp:122] Setting up relu2_1
I1211 13:47:56.702800  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.702800  8328 net.cpp:137] Memory required for data: 246990000
I1211 13:47:56.702800  8328 layer_factory.cpp:58] Creating layer conv2_2
I1211 13:47:56.702800  8328 net.cpp:84] Creating Layer conv2_2
I1211 13:47:56.702800  8328 net.cpp:406] conv2_2 <- conv2_1
I1211 13:47:56.702800  8328 net.cpp:380] conv2_2 -> conv2_2
I1211 13:47:56.705796  8328 net.cpp:122] Setting up conv2_2
I1211 13:47:56.705796  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.705796  8328 net.cpp:137] Memory required for data: 267470000
I1211 13:47:56.705796  8328 layer_factory.cpp:58] Creating layer bn2_2
I1211 13:47:56.705796  8328 net.cpp:84] Creating Layer bn2_2
I1211 13:47:56.705796  8328 net.cpp:406] bn2_2 <- conv2_2
I1211 13:47:56.705796  8328 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 13:47:56.705796  8328 net.cpp:122] Setting up bn2_2
I1211 13:47:56.705796  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.705796  8328 net.cpp:137] Memory required for data: 287950000
I1211 13:47:56.705796  8328 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:47:56.705796  8328 net.cpp:84] Creating Layer scale2_2
I1211 13:47:56.705796  8328 net.cpp:406] scale2_2 <- conv2_2
I1211 13:47:56.705796  8328 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 13:47:56.706797  8328 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:47:56.706797  8328 net.cpp:122] Setting up scale2_2
I1211 13:47:56.706797  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.706797  8328 net.cpp:137] Memory required for data: 308430000
I1211 13:47:56.706797  8328 layer_factory.cpp:58] Creating layer relu2_2
I1211 13:47:56.706797  8328 net.cpp:84] Creating Layer relu2_2
I1211 13:47:56.706797  8328 net.cpp:406] relu2_2 <- conv2_2
I1211 13:47:56.706797  8328 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 13:47:56.706797  8328 net.cpp:122] Setting up relu2_2
I1211 13:47:56.706797  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.706797  8328 net.cpp:137] Memory required for data: 328910000
I1211 13:47:56.706797  8328 layer_factory.cpp:58] Creating layer newconv_added1
I1211 13:47:56.706797  8328 net.cpp:84] Creating Layer newconv_added1
I1211 13:47:56.706797  8328 net.cpp:406] newconv_added1 <- conv2_2
I1211 13:47:56.706797  8328 net.cpp:380] newconv_added1 -> newconv_added1
I1211 13:47:56.707799  8328 net.cpp:122] Setting up newconv_added1
I1211 13:47:56.707799  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.707799  8328 net.cpp:137] Memory required for data: 349390000
I1211 13:47:56.707799  8328 layer_factory.cpp:58] Creating layer bn_added1
I1211 13:47:56.707799  8328 net.cpp:84] Creating Layer bn_added1
I1211 13:47:56.708799  8328 net.cpp:406] bn_added1 <- newconv_added1
I1211 13:47:56.708799  8328 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 13:47:56.708799  8328 net.cpp:122] Setting up bn_added1
I1211 13:47:56.708799  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.708799  8328 net.cpp:137] Memory required for data: 369870000
I1211 13:47:56.708799  8328 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:47:56.708799  8328 net.cpp:84] Creating Layer scale_added1
I1211 13:47:56.708799  8328 net.cpp:406] scale_added1 <- newconv_added1
I1211 13:47:56.708799  8328 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 13:47:56.708799  8328 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:47:56.708799  8328 net.cpp:122] Setting up scale_added1
I1211 13:47:56.708799  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.708799  8328 net.cpp:137] Memory required for data: 390350000
I1211 13:47:56.708799  8328 layer_factory.cpp:58] Creating layer relu_added1
I1211 13:47:56.708799  8328 net.cpp:84] Creating Layer relu_added1
I1211 13:47:56.708799  8328 net.cpp:406] relu_added1 <- newconv_added1
I1211 13:47:56.708799  8328 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 13:47:56.708799  8328 net.cpp:122] Setting up relu_added1
I1211 13:47:56.708799  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.708799  8328 net.cpp:137] Memory required for data: 410830000
I1211 13:47:56.708799  8328 layer_factory.cpp:58] Creating layer pool2_1
I1211 13:47:56.708799  8328 net.cpp:84] Creating Layer pool2_1
I1211 13:47:56.708799  8328 net.cpp:406] pool2_1 <- newconv_added1
I1211 13:47:56.708799  8328 net.cpp:380] pool2_1 -> pool2_1
I1211 13:47:56.709800  8328 net.cpp:122] Setting up pool2_1
I1211 13:47:56.709800  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.709800  8328 net.cpp:137] Memory required for data: 415950000
I1211 13:47:56.709800  8328 layer_factory.cpp:58] Creating layer conv3
I1211 13:47:56.709800  8328 net.cpp:84] Creating Layer conv3
I1211 13:47:56.709800  8328 net.cpp:406] conv3 <- pool2_1
I1211 13:47:56.709800  8328 net.cpp:380] conv3 -> conv3
I1211 13:47:56.710808  8328 net.cpp:122] Setting up conv3
I1211 13:47:56.710808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.710808  8328 net.cpp:137] Memory required for data: 421070000
I1211 13:47:56.710808  8328 layer_factory.cpp:58] Creating layer bn3
I1211 13:47:56.710808  8328 net.cpp:84] Creating Layer bn3
I1211 13:47:56.710808  8328 net.cpp:406] bn3 <- conv3
I1211 13:47:56.710808  8328 net.cpp:367] bn3 -> conv3 (in-place)
I1211 13:47:56.710808  8328 net.cpp:122] Setting up bn3
I1211 13:47:56.710808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.710808  8328 net.cpp:137] Memory required for data: 426190000
I1211 13:47:56.710808  8328 layer_factory.cpp:58] Creating layer scale3
I1211 13:47:56.710808  8328 net.cpp:84] Creating Layer scale3
I1211 13:47:56.710808  8328 net.cpp:406] scale3 <- conv3
I1211 13:47:56.710808  8328 net.cpp:367] scale3 -> conv3 (in-place)
I1211 13:47:56.710808  8328 layer_factory.cpp:58] Creating layer scale3
I1211 13:47:56.710808  8328 net.cpp:122] Setting up scale3
I1211 13:47:56.710808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.710808  8328 net.cpp:137] Memory required for data: 431310000
I1211 13:47:56.710808  8328 layer_factory.cpp:58] Creating layer relu3
I1211 13:47:56.710808  8328 net.cpp:84] Creating Layer relu3
I1211 13:47:56.710808  8328 net.cpp:406] relu3 <- conv3
I1211 13:47:56.711799  8328 net.cpp:367] relu3 -> conv3 (in-place)
I1211 13:47:56.711799  8328 net.cpp:122] Setting up relu3
I1211 13:47:56.711799  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.711799  8328 net.cpp:137] Memory required for data: 436430000
I1211 13:47:56.711799  8328 layer_factory.cpp:58] Creating layer conv3_1
I1211 13:47:56.711799  8328 net.cpp:84] Creating Layer conv3_1
I1211 13:47:56.711799  8328 net.cpp:406] conv3_1 <- conv3
I1211 13:47:56.711799  8328 net.cpp:380] conv3_1 -> conv3_1
I1211 13:47:56.712808  8328 net.cpp:122] Setting up conv3_1
I1211 13:47:56.712808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.712808  8328 net.cpp:137] Memory required for data: 441550000
I1211 13:47:56.712808  8328 layer_factory.cpp:58] Creating layer bn3_1
I1211 13:47:56.712808  8328 net.cpp:84] Creating Layer bn3_1
I1211 13:47:56.712808  8328 net.cpp:406] bn3_1 <- conv3_1
I1211 13:47:56.712808  8328 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 13:47:56.713309  8328 net.cpp:122] Setting up bn3_1
I1211 13:47:56.713309  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.713309  8328 net.cpp:137] Memory required for data: 446670000
I1211 13:47:56.713309  8328 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:47:56.713309  8328 net.cpp:84] Creating Layer scale3_1
I1211 13:47:56.713309  8328 net.cpp:406] scale3_1 <- conv3_1
I1211 13:47:56.713309  8328 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 13:47:56.713309  8328 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:47:56.713309  8328 net.cpp:122] Setting up scale3_1
I1211 13:47:56.713309  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.713309  8328 net.cpp:137] Memory required for data: 451790000
I1211 13:47:56.713309  8328 layer_factory.cpp:58] Creating layer relu3_1
I1211 13:47:56.713309  8328 net.cpp:84] Creating Layer relu3_1
I1211 13:47:56.713309  8328 net.cpp:406] relu3_1 <- conv3_1
I1211 13:47:56.713309  8328 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 13:47:56.713809  8328 net.cpp:122] Setting up relu3_1
I1211 13:47:56.713809  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.713809  8328 net.cpp:137] Memory required for data: 456910000
I1211 13:47:56.713809  8328 layer_factory.cpp:58] Creating layer conv4
I1211 13:47:56.713809  8328 net.cpp:84] Creating Layer conv4
I1211 13:47:56.713809  8328 net.cpp:406] conv4 <- conv3_1
I1211 13:47:56.713809  8328 net.cpp:380] conv4 -> conv4
I1211 13:47:56.714808  8328 net.cpp:122] Setting up conv4
I1211 13:47:56.714808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.714808  8328 net.cpp:137] Memory required for data: 462030000
I1211 13:47:56.714808  8328 layer_factory.cpp:58] Creating layer bn4
I1211 13:47:56.714808  8328 net.cpp:84] Creating Layer bn4
I1211 13:47:56.714808  8328 net.cpp:406] bn4 <- conv4
I1211 13:47:56.714808  8328 net.cpp:367] bn4 -> conv4 (in-place)
I1211 13:47:56.715308  8328 net.cpp:122] Setting up bn4
I1211 13:47:56.715308  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.715308  8328 net.cpp:137] Memory required for data: 467150000
I1211 13:47:56.715308  8328 layer_factory.cpp:58] Creating layer scale4
I1211 13:47:56.715308  8328 net.cpp:84] Creating Layer scale4
I1211 13:47:56.715308  8328 net.cpp:406] scale4 <- conv4
I1211 13:47:56.715308  8328 net.cpp:367] scale4 -> conv4 (in-place)
I1211 13:47:56.715308  8328 layer_factory.cpp:58] Creating layer scale4
I1211 13:47:56.715308  8328 net.cpp:122] Setting up scale4
I1211 13:47:56.715308  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.715308  8328 net.cpp:137] Memory required for data: 472270000
I1211 13:47:56.715308  8328 layer_factory.cpp:58] Creating layer relu4
I1211 13:47:56.715308  8328 net.cpp:84] Creating Layer relu4
I1211 13:47:56.715308  8328 net.cpp:406] relu4 <- conv4
I1211 13:47:56.715308  8328 net.cpp:367] relu4 -> conv4 (in-place)
I1211 13:47:56.715308  8328 net.cpp:122] Setting up relu4
I1211 13:47:56.715308  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.715308  8328 net.cpp:137] Memory required for data: 477390000
I1211 13:47:56.715308  8328 layer_factory.cpp:58] Creating layer conv4_1
I1211 13:47:56.715809  8328 net.cpp:84] Creating Layer conv4_1
I1211 13:47:56.715809  8328 net.cpp:406] conv4_1 <- conv4
I1211 13:47:56.715809  8328 net.cpp:380] conv4_1 -> conv4_1
I1211 13:47:56.716809  8328 net.cpp:122] Setting up conv4_1
I1211 13:47:56.716809  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.716809  8328 net.cpp:137] Memory required for data: 482510000
I1211 13:47:56.716809  8328 layer_factory.cpp:58] Creating layer bn4_1
I1211 13:47:56.716809  8328 net.cpp:84] Creating Layer bn4_1
I1211 13:47:56.716809  8328 net.cpp:406] bn4_1 <- conv4_1
I1211 13:47:56.716809  8328 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 13:47:56.717308  8328 net.cpp:122] Setting up bn4_1
I1211 13:47:56.717308  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.717308  8328 net.cpp:137] Memory required for data: 487630000
I1211 13:47:56.717308  8328 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:47:56.717308  8328 net.cpp:84] Creating Layer scale4_1
I1211 13:47:56.717308  8328 net.cpp:406] scale4_1 <- conv4_1
I1211 13:47:56.717308  8328 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 13:47:56.717308  8328 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:47:56.717308  8328 net.cpp:122] Setting up scale4_1
I1211 13:47:56.717308  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.717308  8328 net.cpp:137] Memory required for data: 492750000
I1211 13:47:56.717308  8328 layer_factory.cpp:58] Creating layer relu4_1
I1211 13:47:56.717308  8328 net.cpp:84] Creating Layer relu4_1
I1211 13:47:56.717308  8328 net.cpp:406] relu4_1 <- conv4_1
I1211 13:47:56.717308  8328 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 13:47:56.717808  8328 net.cpp:122] Setting up relu4_1
I1211 13:47:56.717808  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.717808  8328 net.cpp:137] Memory required for data: 497870000
I1211 13:47:56.717808  8328 layer_factory.cpp:58] Creating layer conv4_2
I1211 13:47:56.717808  8328 net.cpp:84] Creating Layer conv4_2
I1211 13:47:56.717808  8328 net.cpp:406] conv4_2 <- conv4_1
I1211 13:47:56.717808  8328 net.cpp:380] conv4_2 -> conv4_2
I1211 13:47:56.719317  8328 net.cpp:122] Setting up conv4_2
I1211 13:47:56.719317  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.719317  8328 net.cpp:137] Memory required for data: 503809200
I1211 13:47:56.719317  8328 layer_factory.cpp:58] Creating layer bn4_2
I1211 13:47:56.719317  8328 net.cpp:84] Creating Layer bn4_2
I1211 13:47:56.719317  8328 net.cpp:406] bn4_2 <- conv4_2
I1211 13:47:56.719317  8328 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 13:47:56.719317  8328 net.cpp:122] Setting up bn4_2
I1211 13:47:56.719317  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.719317  8328 net.cpp:137] Memory required for data: 509748400
I1211 13:47:56.719317  8328 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:47:56.719317  8328 net.cpp:84] Creating Layer scale4_2
I1211 13:47:56.719317  8328 net.cpp:406] scale4_2 <- conv4_2
I1211 13:47:56.719317  8328 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 13:47:56.719317  8328 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:47:56.719317  8328 net.cpp:122] Setting up scale4_2
I1211 13:47:56.719317  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.719317  8328 net.cpp:137] Memory required for data: 515687600
I1211 13:47:56.719317  8328 layer_factory.cpp:58] Creating layer relu4_2
I1211 13:47:56.719317  8328 net.cpp:84] Creating Layer relu4_2
I1211 13:47:56.719317  8328 net.cpp:406] relu4_2 <- conv4_2
I1211 13:47:56.719317  8328 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 13:47:56.719808  8328 net.cpp:122] Setting up relu4_2
I1211 13:47:56.719808  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.719808  8328 net.cpp:137] Memory required for data: 521626800
I1211 13:47:56.719808  8328 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 13:47:56.719808  8328 net.cpp:84] Creating Layer added_new_conv2
I1211 13:47:56.719808  8328 net.cpp:406] added_new_conv2 <- conv4_2
I1211 13:47:56.720309  8328 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 13:47:56.721309  8328 net.cpp:122] Setting up added_new_conv2
I1211 13:47:56.721309  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.721309  8328 net.cpp:137] Memory required for data: 527566000
I1211 13:47:56.721309  8328 layer_factory.cpp:58] Creating layer bn_added2
I1211 13:47:56.721309  8328 net.cpp:84] Creating Layer bn_added2
I1211 13:47:56.721309  8328 net.cpp:406] bn_added2 <- added_new_conv2
I1211 13:47:56.721309  8328 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.721309  8328 net.cpp:122] Setting up bn_added2
I1211 13:47:56.721309  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.721309  8328 net.cpp:137] Memory required for data: 533505200
I1211 13:47:56.721309  8328 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:47:56.721309  8328 net.cpp:84] Creating Layer scale_added2
I1211 13:47:56.721309  8328 net.cpp:406] scale_added2 <- added_new_conv2
I1211 13:47:56.721309  8328 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.721309  8328 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:47:56.721808  8328 net.cpp:122] Setting up scale_added2
I1211 13:47:56.721808  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.721808  8328 net.cpp:137] Memory required for data: 539444400
I1211 13:47:56.721808  8328 layer_factory.cpp:58] Creating layer relu_added2
I1211 13:47:56.721808  8328 net.cpp:84] Creating Layer relu_added2
I1211 13:47:56.721808  8328 net.cpp:406] relu_added2 <- added_new_conv2
I1211 13:47:56.721808  8328 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.721808  8328 net.cpp:122] Setting up relu_added2
I1211 13:47:56.722309  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.722309  8328 net.cpp:137] Memory required for data: 545383600
I1211 13:47:56.722309  8328 layer_factory.cpp:58] Creating layer pool4_2
I1211 13:47:56.722309  8328 net.cpp:84] Creating Layer pool4_2
I1211 13:47:56.722309  8328 net.cpp:406] pool4_2 <- added_new_conv2
I1211 13:47:56.722309  8328 net.cpp:380] pool4_2 -> pool4_2
I1211 13:47:56.722309  8328 net.cpp:122] Setting up pool4_2
I1211 13:47:56.722309  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.722309  8328 net.cpp:137] Memory required for data: 546868400
I1211 13:47:56.722309  8328 layer_factory.cpp:58] Creating layer conv4_0
I1211 13:47:56.722309  8328 net.cpp:84] Creating Layer conv4_0
I1211 13:47:56.722309  8328 net.cpp:406] conv4_0 <- pool4_2
I1211 13:47:56.722309  8328 net.cpp:380] conv4_0 -> conv4_0
I1211 13:47:56.723309  8328 net.cpp:122] Setting up conv4_0
I1211 13:47:56.723309  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.723309  8328 net.cpp:137] Memory required for data: 548353200
I1211 13:47:56.723309  8328 layer_factory.cpp:58] Creating layer bn4_0
I1211 13:47:56.723309  8328 net.cpp:84] Creating Layer bn4_0
I1211 13:47:56.723309  8328 net.cpp:406] bn4_0 <- conv4_0
I1211 13:47:56.723809  8328 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 13:47:56.723809  8328 net.cpp:122] Setting up bn4_0
I1211 13:47:56.723809  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.723809  8328 net.cpp:137] Memory required for data: 549838000
I1211 13:47:56.723809  8328 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:47:56.723809  8328 net.cpp:84] Creating Layer scale4_0
I1211 13:47:56.723809  8328 net.cpp:406] scale4_0 <- conv4_0
I1211 13:47:56.723809  8328 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 13:47:56.723809  8328 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:47:56.723809  8328 net.cpp:122] Setting up scale4_0
I1211 13:47:56.723809  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.723809  8328 net.cpp:137] Memory required for data: 551322800
I1211 13:47:56.723809  8328 layer_factory.cpp:58] Creating layer relu4_0
I1211 13:47:56.723809  8328 net.cpp:84] Creating Layer relu4_0
I1211 13:47:56.723809  8328 net.cpp:406] relu4_0 <- conv4_0
I1211 13:47:56.723809  8328 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 13:47:56.724308  8328 net.cpp:122] Setting up relu4_0
I1211 13:47:56.724308  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.724308  8328 net.cpp:137] Memory required for data: 552807600
I1211 13:47:56.724308  8328 layer_factory.cpp:58] Creating layer conv11
I1211 13:47:56.724308  8328 net.cpp:84] Creating Layer conv11
I1211 13:47:56.724308  8328 net.cpp:406] conv11 <- conv4_0
I1211 13:47:56.724308  8328 net.cpp:380] conv11 -> conv11
I1211 13:47:56.725808  8328 net.cpp:122] Setting up conv11
I1211 13:47:56.725808  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.725808  8328 net.cpp:137] Memory required for data: 554599600
I1211 13:47:56.725808  8328 layer_factory.cpp:58] Creating layer bn_conv11
I1211 13:47:56.725808  8328 net.cpp:84] Creating Layer bn_conv11
I1211 13:47:56.725808  8328 net.cpp:406] bn_conv11 <- conv11
I1211 13:47:56.725808  8328 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 13:47:56.725808  8328 net.cpp:122] Setting up bn_conv11
I1211 13:47:56.725808  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.725808  8328 net.cpp:137] Memory required for data: 556391600
I1211 13:47:56.725808  8328 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:47:56.725808  8328 net.cpp:84] Creating Layer scale_conv11
I1211 13:47:56.725808  8328 net.cpp:406] scale_conv11 <- conv11
I1211 13:47:56.725808  8328 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 13:47:56.725808  8328 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:47:56.725808  8328 net.cpp:122] Setting up scale_conv11
I1211 13:47:56.726308  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.726308  8328 net.cpp:137] Memory required for data: 558183600
I1211 13:47:56.726308  8328 layer_factory.cpp:58] Creating layer relu_conv11
I1211 13:47:56.726308  8328 net.cpp:84] Creating Layer relu_conv11
I1211 13:47:56.726308  8328 net.cpp:406] relu_conv11 <- conv11
I1211 13:47:56.726308  8328 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 13:47:56.726308  8328 net.cpp:122] Setting up relu_conv11
I1211 13:47:56.726308  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.726308  8328 net.cpp:137] Memory required for data: 559975600
I1211 13:47:56.726308  8328 layer_factory.cpp:58] Creating layer conv12
I1211 13:47:56.726308  8328 net.cpp:84] Creating Layer conv12
I1211 13:47:56.726308  8328 net.cpp:406] conv12 <- conv11
I1211 13:47:56.726308  8328 net.cpp:380] conv12 -> conv12
I1211 13:47:56.727308  8328 net.cpp:122] Setting up conv12
I1211 13:47:56.727308  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.727308  8328 net.cpp:137] Memory required for data: 562279600
I1211 13:47:56.727308  8328 layer_factory.cpp:58] Creating layer bn_conv12
I1211 13:47:56.727308  8328 net.cpp:84] Creating Layer bn_conv12
I1211 13:47:56.727308  8328 net.cpp:406] bn_conv12 <- conv12
I1211 13:47:56.727308  8328 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 13:47:56.728477  8328 net.cpp:122] Setting up bn_conv12
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 564583600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer scale_conv12
I1211 13:47:56.728477  8328 net.cpp:406] scale_conv12 <- conv12
I1211 13:47:56.728477  8328 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:47:56.728477  8328 net.cpp:122] Setting up scale_conv12
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 566887600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer relu_conv12
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer relu_conv12
I1211 13:47:56.728477  8328 net.cpp:406] relu_conv12 <- conv12
I1211 13:47:56.728477  8328 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 13:47:56.728477  8328 net.cpp:122] Setting up relu_conv12
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 569191600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer poolcp6
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer poolcp6
I1211 13:47:56.728477  8328 net.cpp:406] poolcp6 <- conv12
I1211 13:47:56.728477  8328 net.cpp:380] poolcp6 -> poolcp6
I1211 13:47:56.728477  8328 net.cpp:122] Setting up poolcp6
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 569227600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer ip1
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer ip1
I1211 13:47:56.728477  8328 net.cpp:406] ip1 <- poolcp6
I1211 13:47:56.728477  8328 net.cpp:380] ip1 -> ip1
I1211 13:47:56.728477  8328 net.cpp:122] Setting up ip1
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 569267600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 13:47:56.728477  8328 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 13:47:56.728477  8328 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 13:47:56.728477  8328 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 13:47:56.728477  8328 net.cpp:122] Setting up ip1_ip1_0_split
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 569347600
I1211 13:47:56.728477  8328 layer_factory.cpp:58] Creating layer accuracy_training
I1211 13:47:56.728477  8328 net.cpp:84] Creating Layer accuracy_training
I1211 13:47:56.728477  8328 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 13:47:56.728477  8328 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 13:47:56.728477  8328 net.cpp:380] accuracy_training -> accuracy_training
I1211 13:47:56.728477  8328 net.cpp:122] Setting up accuracy_training
I1211 13:47:56.728477  8328 net.cpp:129] Top shape: (1)
I1211 13:47:56.728477  8328 net.cpp:137] Memory required for data: 569347604
I1211 13:47:56.729476  8328 layer_factory.cpp:58] Creating layer loss
I1211 13:47:56.729476  8328 net.cpp:84] Creating Layer loss
I1211 13:47:56.729476  8328 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 13:47:56.729476  8328 net.cpp:406] loss <- label_cifar_1_split_1
I1211 13:47:56.729476  8328 net.cpp:380] loss -> loss
I1211 13:47:56.729476  8328 layer_factory.cpp:58] Creating layer loss
I1211 13:47:56.729476  8328 net.cpp:122] Setting up loss
I1211 13:47:56.729476  8328 net.cpp:129] Top shape: (1)
I1211 13:47:56.729476  8328 net.cpp:132]     with loss weight 1
I1211 13:47:56.729476  8328 net.cpp:137] Memory required for data: 569347608
I1211 13:47:56.729476  8328 net.cpp:198] loss needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:200] accuracy_training does not need backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] ip1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] poolcp6 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu_conv12 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale_conv12 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn_conv12 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv12 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu_conv11 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale_conv11 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn_conv11 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv11 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu4_0 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale4_0 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn4_0 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv4_0 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] pool4_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu_added2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale_added2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn_added2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] added_new_conv2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu4_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale4_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn4_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv4_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu4_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale4_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn4_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv4_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu4 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale4 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn4 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv4 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu3_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale3_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn3_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv3_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu3 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale3 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn3 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv3 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] pool2_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu_added1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale_added1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn_added1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] newconv_added1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu2_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale2_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn2_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv2_2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu2_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale2_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn2_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv2_1 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] bn2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] conv2 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] relu1_0 needs backward computation.
I1211 13:47:56.729476  8328 net.cpp:198] scale1_0 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] bn1_0 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] conv1_0 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] relu1 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] scale1 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] bn1 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:198] conv1 needs backward computation.
I1211 13:47:56.730476  8328 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 13:47:56.730476  8328 net.cpp:200] cifar does not need backward computation.
I1211 13:47:56.730476  8328 net.cpp:242] This network produces output accuracy_training
I1211 13:47:56.730476  8328 net.cpp:242] This network produces output loss
I1211 13:47:56.730476  8328 net.cpp:255] Network initialization done.
I1211 13:47:56.730476  8328 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:47:56.730476  8328 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:47:56.730476  8328 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added1
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added2
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 13:47:56.731477  8328 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 13:47:56.731477  8328 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 13:47:56.731477  8328 layer_factory.cpp:58] Creating layer cifar
I1211 13:47:56.734477  8328 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 13:47:56.734477  8328 net.cpp:84] Creating Layer cifar
I1211 13:47:56.734477  8328 net.cpp:380] cifar -> data
I1211 13:47:56.734477  8328 net.cpp:380] cifar -> label
I1211 13:47:56.735476  8328 data_layer.cpp:45] output data size: 100,3,32,32
I1211 13:47:56.742473  8328 net.cpp:122] Setting up cifar
I1211 13:47:56.742473  8328 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 13:47:56.742473  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.742473  8328 net.cpp:137] Memory required for data: 1229200
I1211 13:47:56.742473  8328 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 13:47:56.742473  8328 net.cpp:84] Creating Layer label_cifar_1_split
I1211 13:47:56.742473  8328 net.cpp:406] label_cifar_1_split <- label
I1211 13:47:56.742473  8328 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 13:47:56.742473  8328 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 13:47:56.742473  8328 net.cpp:122] Setting up label_cifar_1_split
I1211 13:47:56.742473  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.742473  8328 net.cpp:129] Top shape: 100 (100)
I1211 13:47:56.742473  8328 net.cpp:137] Memory required for data: 1230000
I1211 13:47:56.742473  8328 layer_factory.cpp:58] Creating layer conv1
I1211 13:47:56.742473  8328 net.cpp:84] Creating Layer conv1
I1211 13:47:56.742473  8328 net.cpp:406] conv1 <- data
I1211 13:47:56.742473  8328 net.cpp:380] conv1 -> conv1
I1211 13:47:56.744484  8328 net.cpp:122] Setting up conv1
I1211 13:47:56.744484  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.744484  8328 net.cpp:137] Memory required for data: 13518000
I1211 13:47:56.744484  8328 layer_factory.cpp:58] Creating layer bn1
I1211 13:47:56.744484  8328 net.cpp:84] Creating Layer bn1
I1211 13:47:56.744484  8328 net.cpp:406] bn1 <- conv1
I1211 13:47:56.744484  8328 net.cpp:367] bn1 -> conv1 (in-place)
I1211 13:47:56.744484 12664 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 13:47:56.744484  8328 net.cpp:122] Setting up bn1
I1211 13:47:56.744484  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.744484  8328 net.cpp:137] Memory required for data: 25806000
I1211 13:47:56.744484  8328 layer_factory.cpp:58] Creating layer scale1
I1211 13:47:56.744484  8328 net.cpp:84] Creating Layer scale1
I1211 13:47:56.744484  8328 net.cpp:406] scale1 <- conv1
I1211 13:47:56.744484  8328 net.cpp:367] scale1 -> conv1 (in-place)
I1211 13:47:56.744484  8328 layer_factory.cpp:58] Creating layer scale1
I1211 13:47:56.744484  8328 net.cpp:122] Setting up scale1
I1211 13:47:56.744484  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.744484  8328 net.cpp:137] Memory required for data: 38094000
I1211 13:47:56.744484  8328 layer_factory.cpp:58] Creating layer relu1
I1211 13:47:56.744484  8328 net.cpp:84] Creating Layer relu1
I1211 13:47:56.744484  8328 net.cpp:406] relu1 <- conv1
I1211 13:47:56.744484  8328 net.cpp:367] relu1 -> conv1 (in-place)
I1211 13:47:56.745476  8328 net.cpp:122] Setting up relu1
I1211 13:47:56.745476  8328 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 13:47:56.745476  8328 net.cpp:137] Memory required for data: 50382000
I1211 13:47:56.745476  8328 layer_factory.cpp:58] Creating layer conv1_0
I1211 13:47:56.745476  8328 net.cpp:84] Creating Layer conv1_0
I1211 13:47:56.745476  8328 net.cpp:406] conv1_0 <- conv1
I1211 13:47:56.745476  8328 net.cpp:380] conv1_0 -> conv1_0
I1211 13:47:56.746476  8328 net.cpp:122] Setting up conv1_0
I1211 13:47:56.746476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.746476  8328 net.cpp:137] Memory required for data: 66766000
I1211 13:47:56.746476  8328 layer_factory.cpp:58] Creating layer bn1_0
I1211 13:47:56.746476  8328 net.cpp:84] Creating Layer bn1_0
I1211 13:47:56.746476  8328 net.cpp:406] bn1_0 <- conv1_0
I1211 13:47:56.746476  8328 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 13:47:56.747476  8328 net.cpp:122] Setting up bn1_0
I1211 13:47:56.747476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.747476  8328 net.cpp:137] Memory required for data: 83150000
I1211 13:47:56.747476  8328 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:47:56.747476  8328 net.cpp:84] Creating Layer scale1_0
I1211 13:47:56.747476  8328 net.cpp:406] scale1_0 <- conv1_0
I1211 13:47:56.747476  8328 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 13:47:56.747476  8328 layer_factory.cpp:58] Creating layer scale1_0
I1211 13:47:56.747476  8328 net.cpp:122] Setting up scale1_0
I1211 13:47:56.747476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.747476  8328 net.cpp:137] Memory required for data: 99534000
I1211 13:47:56.747476  8328 layer_factory.cpp:58] Creating layer relu1_0
I1211 13:47:56.747476  8328 net.cpp:84] Creating Layer relu1_0
I1211 13:47:56.747476  8328 net.cpp:406] relu1_0 <- conv1_0
I1211 13:47:56.747476  8328 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 13:47:56.747476  8328 net.cpp:122] Setting up relu1_0
I1211 13:47:56.747476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.747476  8328 net.cpp:137] Memory required for data: 115918000
I1211 13:47:56.747476  8328 layer_factory.cpp:58] Creating layer conv2
I1211 13:47:56.747476  8328 net.cpp:84] Creating Layer conv2
I1211 13:47:56.747476  8328 net.cpp:406] conv2 <- conv1_0
I1211 13:47:56.747476  8328 net.cpp:380] conv2 -> conv2
I1211 13:47:56.749475  8328 net.cpp:122] Setting up conv2
I1211 13:47:56.749475  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.749475  8328 net.cpp:137] Memory required for data: 132302000
I1211 13:47:56.749475  8328 layer_factory.cpp:58] Creating layer bn2
I1211 13:47:56.749475  8328 net.cpp:84] Creating Layer bn2
I1211 13:47:56.749475  8328 net.cpp:406] bn2 <- conv2
I1211 13:47:56.749475  8328 net.cpp:367] bn2 -> conv2 (in-place)
I1211 13:47:56.749475  8328 net.cpp:122] Setting up bn2
I1211 13:47:56.749475  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.749475  8328 net.cpp:137] Memory required for data: 148686000
I1211 13:47:56.749475  8328 layer_factory.cpp:58] Creating layer scale2
I1211 13:47:56.749475  8328 net.cpp:84] Creating Layer scale2
I1211 13:47:56.749475  8328 net.cpp:406] scale2 <- conv2
I1211 13:47:56.749475  8328 net.cpp:367] scale2 -> conv2 (in-place)
I1211 13:47:56.749475  8328 layer_factory.cpp:58] Creating layer scale2
I1211 13:47:56.749475  8328 net.cpp:122] Setting up scale2
I1211 13:47:56.749475  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.749475  8328 net.cpp:137] Memory required for data: 165070000
I1211 13:47:56.749475  8328 layer_factory.cpp:58] Creating layer relu2
I1211 13:47:56.749475  8328 net.cpp:84] Creating Layer relu2
I1211 13:47:56.749475  8328 net.cpp:406] relu2 <- conv2
I1211 13:47:56.749475  8328 net.cpp:367] relu2 -> conv2 (in-place)
I1211 13:47:56.749475  8328 net.cpp:122] Setting up relu2
I1211 13:47:56.749475  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.749475  8328 net.cpp:137] Memory required for data: 181454000
I1211 13:47:56.749475  8328 layer_factory.cpp:58] Creating layer conv2_1
I1211 13:47:56.750476  8328 net.cpp:84] Creating Layer conv2_1
I1211 13:47:56.750476  8328 net.cpp:406] conv2_1 <- conv2
I1211 13:47:56.750476  8328 net.cpp:380] conv2_1 -> conv2_1
I1211 13:47:56.751476  8328 net.cpp:122] Setting up conv2_1
I1211 13:47:56.751476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.751476  8328 net.cpp:137] Memory required for data: 197838000
I1211 13:47:56.751476  8328 layer_factory.cpp:58] Creating layer bn2_1
I1211 13:47:56.751476  8328 net.cpp:84] Creating Layer bn2_1
I1211 13:47:56.751476  8328 net.cpp:406] bn2_1 <- conv2_1
I1211 13:47:56.751476  8328 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 13:47:56.751476  8328 net.cpp:122] Setting up bn2_1
I1211 13:47:56.751476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.751476  8328 net.cpp:137] Memory required for data: 214222000
I1211 13:47:56.751476  8328 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:47:56.751476  8328 net.cpp:84] Creating Layer scale2_1
I1211 13:47:56.751476  8328 net.cpp:406] scale2_1 <- conv2_1
I1211 13:47:56.751476  8328 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 13:47:56.751476  8328 layer_factory.cpp:58] Creating layer scale2_1
I1211 13:47:56.752476  8328 net.cpp:122] Setting up scale2_1
I1211 13:47:56.752476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.752476  8328 net.cpp:137] Memory required for data: 230606000
I1211 13:47:56.752476  8328 layer_factory.cpp:58] Creating layer relu2_1
I1211 13:47:56.752476  8328 net.cpp:84] Creating Layer relu2_1
I1211 13:47:56.752476  8328 net.cpp:406] relu2_1 <- conv2_1
I1211 13:47:56.752476  8328 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 13:47:56.752476  8328 net.cpp:122] Setting up relu2_1
I1211 13:47:56.752476  8328 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 13:47:56.752476  8328 net.cpp:137] Memory required for data: 246990000
I1211 13:47:56.752476  8328 layer_factory.cpp:58] Creating layer conv2_2
I1211 13:47:56.752476  8328 net.cpp:84] Creating Layer conv2_2
I1211 13:47:56.752476  8328 net.cpp:406] conv2_2 <- conv2_1
I1211 13:47:56.752476  8328 net.cpp:380] conv2_2 -> conv2_2
I1211 13:47:56.753474  8328 net.cpp:122] Setting up conv2_2
I1211 13:47:56.753474  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.753474  8328 net.cpp:137] Memory required for data: 267470000
I1211 13:47:56.753474  8328 layer_factory.cpp:58] Creating layer bn2_2
I1211 13:47:56.753474  8328 net.cpp:84] Creating Layer bn2_2
I1211 13:47:56.753474  8328 net.cpp:406] bn2_2 <- conv2_2
I1211 13:47:56.753474  8328 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 13:47:56.754477  8328 net.cpp:122] Setting up bn2_2
I1211 13:47:56.754477  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.754477  8328 net.cpp:137] Memory required for data: 287950000
I1211 13:47:56.754477  8328 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:47:56.754477  8328 net.cpp:84] Creating Layer scale2_2
I1211 13:47:56.754477  8328 net.cpp:406] scale2_2 <- conv2_2
I1211 13:47:56.754477  8328 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 13:47:56.754477  8328 layer_factory.cpp:58] Creating layer scale2_2
I1211 13:47:56.754477  8328 net.cpp:122] Setting up scale2_2
I1211 13:47:56.754477  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.754477  8328 net.cpp:137] Memory required for data: 308430000
I1211 13:47:56.754477  8328 layer_factory.cpp:58] Creating layer relu2_2
I1211 13:47:56.754477  8328 net.cpp:84] Creating Layer relu2_2
I1211 13:47:56.754477  8328 net.cpp:406] relu2_2 <- conv2_2
I1211 13:47:56.754477  8328 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 13:47:56.754477  8328 net.cpp:122] Setting up relu2_2
I1211 13:47:56.754477  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.754477  8328 net.cpp:137] Memory required for data: 328910000
I1211 13:47:56.754477  8328 layer_factory.cpp:58] Creating layer newconv_added1
I1211 13:47:56.754477  8328 net.cpp:84] Creating Layer newconv_added1
I1211 13:47:56.754477  8328 net.cpp:406] newconv_added1 <- conv2_2
I1211 13:47:56.754477  8328 net.cpp:380] newconv_added1 -> newconv_added1
I1211 13:47:56.756475  8328 net.cpp:122] Setting up newconv_added1
I1211 13:47:56.756475  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.756475  8328 net.cpp:137] Memory required for data: 349390000
I1211 13:47:56.756475  8328 layer_factory.cpp:58] Creating layer bn_added1
I1211 13:47:56.756475  8328 net.cpp:84] Creating Layer bn_added1
I1211 13:47:56.756475  8328 net.cpp:406] bn_added1 <- newconv_added1
I1211 13:47:56.756475  8328 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 13:47:56.756475  8328 net.cpp:122] Setting up bn_added1
I1211 13:47:56.756475  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.756475  8328 net.cpp:137] Memory required for data: 369870000
I1211 13:47:56.756475  8328 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:47:56.756475  8328 net.cpp:84] Creating Layer scale_added1
I1211 13:47:56.756475  8328 net.cpp:406] scale_added1 <- newconv_added1
I1211 13:47:56.756475  8328 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 13:47:56.756475  8328 layer_factory.cpp:58] Creating layer scale_added1
I1211 13:47:56.756475  8328 net.cpp:122] Setting up scale_added1
I1211 13:47:56.756475  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.756475  8328 net.cpp:137] Memory required for data: 390350000
I1211 13:47:56.756475  8328 layer_factory.cpp:58] Creating layer relu_added1
I1211 13:47:56.756475  8328 net.cpp:84] Creating Layer relu_added1
I1211 13:47:56.756475  8328 net.cpp:406] relu_added1 <- newconv_added1
I1211 13:47:56.756475  8328 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 13:47:56.757477  8328 net.cpp:122] Setting up relu_added1
I1211 13:47:56.757477  8328 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 13:47:56.757477  8328 net.cpp:137] Memory required for data: 410830000
I1211 13:47:56.757477  8328 layer_factory.cpp:58] Creating layer pool2_1
I1211 13:47:56.757477  8328 net.cpp:84] Creating Layer pool2_1
I1211 13:47:56.757477  8328 net.cpp:406] pool2_1 <- newconv_added1
I1211 13:47:56.757477  8328 net.cpp:380] pool2_1 -> pool2_1
I1211 13:47:56.757477  8328 net.cpp:122] Setting up pool2_1
I1211 13:47:56.757477  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.757477  8328 net.cpp:137] Memory required for data: 415950000
I1211 13:47:56.757477  8328 layer_factory.cpp:58] Creating layer conv3
I1211 13:47:56.757477  8328 net.cpp:84] Creating Layer conv3
I1211 13:47:56.757477  8328 net.cpp:406] conv3 <- pool2_1
I1211 13:47:56.757477  8328 net.cpp:380] conv3 -> conv3
I1211 13:47:56.758477  8328 net.cpp:122] Setting up conv3
I1211 13:47:56.758477  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.758477  8328 net.cpp:137] Memory required for data: 421070000
I1211 13:47:56.758477  8328 layer_factory.cpp:58] Creating layer bn3
I1211 13:47:56.758477  8328 net.cpp:84] Creating Layer bn3
I1211 13:47:56.758477  8328 net.cpp:406] bn3 <- conv3
I1211 13:47:56.758477  8328 net.cpp:367] bn3 -> conv3 (in-place)
I1211 13:47:56.758477  8328 net.cpp:122] Setting up bn3
I1211 13:47:56.758477  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.758477  8328 net.cpp:137] Memory required for data: 426190000
I1211 13:47:56.758477  8328 layer_factory.cpp:58] Creating layer scale3
I1211 13:47:56.758477  8328 net.cpp:84] Creating Layer scale3
I1211 13:47:56.758477  8328 net.cpp:406] scale3 <- conv3
I1211 13:47:56.758477  8328 net.cpp:367] scale3 -> conv3 (in-place)
I1211 13:47:56.758477  8328 layer_factory.cpp:58] Creating layer scale3
I1211 13:47:56.759476  8328 net.cpp:122] Setting up scale3
I1211 13:47:56.759476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.759476  8328 net.cpp:137] Memory required for data: 431310000
I1211 13:47:56.759476  8328 layer_factory.cpp:58] Creating layer relu3
I1211 13:47:56.759476  8328 net.cpp:84] Creating Layer relu3
I1211 13:47:56.759476  8328 net.cpp:406] relu3 <- conv3
I1211 13:47:56.759476  8328 net.cpp:367] relu3 -> conv3 (in-place)
I1211 13:47:56.759476  8328 net.cpp:122] Setting up relu3
I1211 13:47:56.759476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.759476  8328 net.cpp:137] Memory required for data: 436430000
I1211 13:47:56.759476  8328 layer_factory.cpp:58] Creating layer conv3_1
I1211 13:47:56.759476  8328 net.cpp:84] Creating Layer conv3_1
I1211 13:47:56.759476  8328 net.cpp:406] conv3_1 <- conv3
I1211 13:47:56.759476  8328 net.cpp:380] conv3_1 -> conv3_1
I1211 13:47:56.761476  8328 net.cpp:122] Setting up conv3_1
I1211 13:47:56.761476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.761476  8328 net.cpp:137] Memory required for data: 441550000
I1211 13:47:56.761476  8328 layer_factory.cpp:58] Creating layer bn3_1
I1211 13:47:56.761476  8328 net.cpp:84] Creating Layer bn3_1
I1211 13:47:56.761476  8328 net.cpp:406] bn3_1 <- conv3_1
I1211 13:47:56.761476  8328 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 13:47:56.761476  8328 net.cpp:122] Setting up bn3_1
I1211 13:47:56.761476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.761476  8328 net.cpp:137] Memory required for data: 446670000
I1211 13:47:56.761476  8328 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:47:56.761476  8328 net.cpp:84] Creating Layer scale3_1
I1211 13:47:56.761476  8328 net.cpp:406] scale3_1 <- conv3_1
I1211 13:47:56.761476  8328 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 13:47:56.761476  8328 layer_factory.cpp:58] Creating layer scale3_1
I1211 13:47:56.761476  8328 net.cpp:122] Setting up scale3_1
I1211 13:47:56.761476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.761476  8328 net.cpp:137] Memory required for data: 451790000
I1211 13:47:56.761476  8328 layer_factory.cpp:58] Creating layer relu3_1
I1211 13:47:56.761476  8328 net.cpp:84] Creating Layer relu3_1
I1211 13:47:56.761476  8328 net.cpp:406] relu3_1 <- conv3_1
I1211 13:47:56.761476  8328 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 13:47:56.762476  8328 net.cpp:122] Setting up relu3_1
I1211 13:47:56.762476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.762476  8328 net.cpp:137] Memory required for data: 456910000
I1211 13:47:56.762476  8328 layer_factory.cpp:58] Creating layer conv4
I1211 13:47:56.762476  8328 net.cpp:84] Creating Layer conv4
I1211 13:47:56.762476  8328 net.cpp:406] conv4 <- conv3_1
I1211 13:47:56.762476  8328 net.cpp:380] conv4 -> conv4
I1211 13:47:56.763484  8328 net.cpp:122] Setting up conv4
I1211 13:47:56.763484  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.763484  8328 net.cpp:137] Memory required for data: 462030000
I1211 13:47:56.763484  8328 layer_factory.cpp:58] Creating layer bn4
I1211 13:47:56.763484  8328 net.cpp:84] Creating Layer bn4
I1211 13:47:56.763484  8328 net.cpp:406] bn4 <- conv4
I1211 13:47:56.763484  8328 net.cpp:367] bn4 -> conv4 (in-place)
I1211 13:47:56.763484  8328 net.cpp:122] Setting up bn4
I1211 13:47:56.763484  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.763484  8328 net.cpp:137] Memory required for data: 467150000
I1211 13:47:56.763484  8328 layer_factory.cpp:58] Creating layer scale4
I1211 13:47:56.763484  8328 net.cpp:84] Creating Layer scale4
I1211 13:47:56.763484  8328 net.cpp:406] scale4 <- conv4
I1211 13:47:56.763484  8328 net.cpp:367] scale4 -> conv4 (in-place)
I1211 13:47:56.763484  8328 layer_factory.cpp:58] Creating layer scale4
I1211 13:47:56.763484  8328 net.cpp:122] Setting up scale4
I1211 13:47:56.763484  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.763484  8328 net.cpp:137] Memory required for data: 472270000
I1211 13:47:56.763484  8328 layer_factory.cpp:58] Creating layer relu4
I1211 13:47:56.763484  8328 net.cpp:84] Creating Layer relu4
I1211 13:47:56.763484  8328 net.cpp:406] relu4 <- conv4
I1211 13:47:56.763484  8328 net.cpp:367] relu4 -> conv4 (in-place)
I1211 13:47:56.764475  8328 net.cpp:122] Setting up relu4
I1211 13:47:56.764475  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.764475  8328 net.cpp:137] Memory required for data: 477390000
I1211 13:47:56.764475  8328 layer_factory.cpp:58] Creating layer conv4_1
I1211 13:47:56.764475  8328 net.cpp:84] Creating Layer conv4_1
I1211 13:47:56.764475  8328 net.cpp:406] conv4_1 <- conv4
I1211 13:47:56.764475  8328 net.cpp:380] conv4_1 -> conv4_1
I1211 13:47:56.765475  8328 net.cpp:122] Setting up conv4_1
I1211 13:47:56.765475  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.765475  8328 net.cpp:137] Memory required for data: 482510000
I1211 13:47:56.765475  8328 layer_factory.cpp:58] Creating layer bn4_1
I1211 13:47:56.765475  8328 net.cpp:84] Creating Layer bn4_1
I1211 13:47:56.765475  8328 net.cpp:406] bn4_1 <- conv4_1
I1211 13:47:56.765475  8328 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 13:47:56.765475  8328 net.cpp:122] Setting up bn4_1
I1211 13:47:56.765475  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.765475  8328 net.cpp:137] Memory required for data: 487630000
I1211 13:47:56.765475  8328 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:47:56.766476  8328 net.cpp:84] Creating Layer scale4_1
I1211 13:47:56.766476  8328 net.cpp:406] scale4_1 <- conv4_1
I1211 13:47:56.766476  8328 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 13:47:56.766476  8328 layer_factory.cpp:58] Creating layer scale4_1
I1211 13:47:56.766476  8328 net.cpp:122] Setting up scale4_1
I1211 13:47:56.766476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.766476  8328 net.cpp:137] Memory required for data: 492750000
I1211 13:47:56.766476  8328 layer_factory.cpp:58] Creating layer relu4_1
I1211 13:47:56.766476  8328 net.cpp:84] Creating Layer relu4_1
I1211 13:47:56.766476  8328 net.cpp:406] relu4_1 <- conv4_1
I1211 13:47:56.766476  8328 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 13:47:56.766476  8328 net.cpp:122] Setting up relu4_1
I1211 13:47:56.766476  8328 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 13:47:56.766476  8328 net.cpp:137] Memory required for data: 497870000
I1211 13:47:56.766476  8328 layer_factory.cpp:58] Creating layer conv4_2
I1211 13:47:56.766476  8328 net.cpp:84] Creating Layer conv4_2
I1211 13:47:56.766476  8328 net.cpp:406] conv4_2 <- conv4_1
I1211 13:47:56.766476  8328 net.cpp:380] conv4_2 -> conv4_2
I1211 13:47:56.768476  8328 net.cpp:122] Setting up conv4_2
I1211 13:47:56.768476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.768476  8328 net.cpp:137] Memory required for data: 503809200
I1211 13:47:56.768476  8328 layer_factory.cpp:58] Creating layer bn4_2
I1211 13:47:56.768476  8328 net.cpp:84] Creating Layer bn4_2
I1211 13:47:56.768476  8328 net.cpp:406] bn4_2 <- conv4_2
I1211 13:47:56.768476  8328 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 13:47:56.768476  8328 net.cpp:122] Setting up bn4_2
I1211 13:47:56.768476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.768476  8328 net.cpp:137] Memory required for data: 509748400
I1211 13:47:56.768476  8328 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:47:56.768476  8328 net.cpp:84] Creating Layer scale4_2
I1211 13:47:56.768476  8328 net.cpp:406] scale4_2 <- conv4_2
I1211 13:47:56.768476  8328 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 13:47:56.768476  8328 layer_factory.cpp:58] Creating layer scale4_2
I1211 13:47:56.768476  8328 net.cpp:122] Setting up scale4_2
I1211 13:47:56.768476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.768476  8328 net.cpp:137] Memory required for data: 515687600
I1211 13:47:56.768476  8328 layer_factory.cpp:58] Creating layer relu4_2
I1211 13:47:56.768476  8328 net.cpp:84] Creating Layer relu4_2
I1211 13:47:56.768476  8328 net.cpp:406] relu4_2 <- conv4_2
I1211 13:47:56.768476  8328 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 13:47:56.768476  8328 net.cpp:122] Setting up relu4_2
I1211 13:47:56.768476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.768476  8328 net.cpp:137] Memory required for data: 521626800
I1211 13:47:56.768476  8328 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 13:47:56.768476  8328 net.cpp:84] Creating Layer added_new_conv2
I1211 13:47:56.769476  8328 net.cpp:406] added_new_conv2 <- conv4_2
I1211 13:47:56.769476  8328 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 13:47:56.770476  8328 net.cpp:122] Setting up added_new_conv2
I1211 13:47:56.770476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.770476  8328 net.cpp:137] Memory required for data: 527566000
I1211 13:47:56.770476  8328 layer_factory.cpp:58] Creating layer bn_added2
I1211 13:47:56.770476  8328 net.cpp:84] Creating Layer bn_added2
I1211 13:47:56.770476  8328 net.cpp:406] bn_added2 <- added_new_conv2
I1211 13:47:56.770476  8328 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.770476  8328 net.cpp:122] Setting up bn_added2
I1211 13:47:56.770476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.770476  8328 net.cpp:137] Memory required for data: 533505200
I1211 13:47:56.770476  8328 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:47:56.770476  8328 net.cpp:84] Creating Layer scale_added2
I1211 13:47:56.770476  8328 net.cpp:406] scale_added2 <- added_new_conv2
I1211 13:47:56.770476  8328 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.770476  8328 layer_factory.cpp:58] Creating layer scale_added2
I1211 13:47:56.770476  8328 net.cpp:122] Setting up scale_added2
I1211 13:47:56.770476  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.770476  8328 net.cpp:137] Memory required for data: 539444400
I1211 13:47:56.770476  8328 layer_factory.cpp:58] Creating layer relu_added2
I1211 13:47:56.770476  8328 net.cpp:84] Creating Layer relu_added2
I1211 13:47:56.771472  8328 net.cpp:406] relu_added2 <- added_new_conv2
I1211 13:47:56.771472  8328 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 13:47:56.771472  8328 net.cpp:122] Setting up relu_added2
I1211 13:47:56.771472  8328 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 13:47:56.771472  8328 net.cpp:137] Memory required for data: 545383600
I1211 13:47:56.771472  8328 layer_factory.cpp:58] Creating layer pool4_2
I1211 13:47:56.771472  8328 net.cpp:84] Creating Layer pool4_2
I1211 13:47:56.771472  8328 net.cpp:406] pool4_2 <- added_new_conv2
I1211 13:47:56.771472  8328 net.cpp:380] pool4_2 -> pool4_2
I1211 13:47:56.771472  8328 net.cpp:122] Setting up pool4_2
I1211 13:47:56.771472  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.771472  8328 net.cpp:137] Memory required for data: 546868400
I1211 13:47:56.771472  8328 layer_factory.cpp:58] Creating layer conv4_0
I1211 13:47:56.771472  8328 net.cpp:84] Creating Layer conv4_0
I1211 13:47:56.771472  8328 net.cpp:406] conv4_0 <- pool4_2
I1211 13:47:56.771472  8328 net.cpp:380] conv4_0 -> conv4_0
I1211 13:47:56.773473  8328 net.cpp:122] Setting up conv4_0
I1211 13:47:56.773473  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.773473  8328 net.cpp:137] Memory required for data: 548353200
I1211 13:47:56.773473  8328 layer_factory.cpp:58] Creating layer bn4_0
I1211 13:47:56.774473  8328 net.cpp:84] Creating Layer bn4_0
I1211 13:47:56.774473  8328 net.cpp:406] bn4_0 <- conv4_0
I1211 13:47:56.774473  8328 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 13:47:56.774473  8328 net.cpp:122] Setting up bn4_0
I1211 13:47:56.774473  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.774473  8328 net.cpp:137] Memory required for data: 549838000
I1211 13:47:56.774473  8328 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:47:56.774473  8328 net.cpp:84] Creating Layer scale4_0
I1211 13:47:56.774473  8328 net.cpp:406] scale4_0 <- conv4_0
I1211 13:47:56.774473  8328 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 13:47:56.774473  8328 layer_factory.cpp:58] Creating layer scale4_0
I1211 13:47:56.774473  8328 net.cpp:122] Setting up scale4_0
I1211 13:47:56.774473  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.774473  8328 net.cpp:137] Memory required for data: 551322800
I1211 13:47:56.774473  8328 layer_factory.cpp:58] Creating layer relu4_0
I1211 13:47:56.774473  8328 net.cpp:84] Creating Layer relu4_0
I1211 13:47:56.774473  8328 net.cpp:406] relu4_0 <- conv4_0
I1211 13:47:56.774473  8328 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 13:47:56.775473  8328 net.cpp:122] Setting up relu4_0
I1211 13:47:56.775473  8328 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 13:47:56.775473  8328 net.cpp:137] Memory required for data: 552807600
I1211 13:47:56.775473  8328 layer_factory.cpp:58] Creating layer conv11
I1211 13:47:56.775473  8328 net.cpp:84] Creating Layer conv11
I1211 13:47:56.775473  8328 net.cpp:406] conv11 <- conv4_0
I1211 13:47:56.775473  8328 net.cpp:380] conv11 -> conv11
I1211 13:47:56.777475  8328 net.cpp:122] Setting up conv11
I1211 13:47:56.777475  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.777475  8328 net.cpp:137] Memory required for data: 554599600
I1211 13:47:56.777475  8328 layer_factory.cpp:58] Creating layer bn_conv11
I1211 13:47:56.777475  8328 net.cpp:84] Creating Layer bn_conv11
I1211 13:47:56.777475  8328 net.cpp:406] bn_conv11 <- conv11
I1211 13:47:56.777475  8328 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 13:47:56.778473  8328 net.cpp:122] Setting up bn_conv11
I1211 13:47:56.778473  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.778473  8328 net.cpp:137] Memory required for data: 556391600
I1211 13:47:56.778473  8328 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:47:56.778473  8328 net.cpp:84] Creating Layer scale_conv11
I1211 13:47:56.778473  8328 net.cpp:406] scale_conv11 <- conv11
I1211 13:47:56.778473  8328 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 13:47:56.778473  8328 layer_factory.cpp:58] Creating layer scale_conv11
I1211 13:47:56.778473  8328 net.cpp:122] Setting up scale_conv11
I1211 13:47:56.778473  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.778473  8328 net.cpp:137] Memory required for data: 558183600
I1211 13:47:56.778473  8328 layer_factory.cpp:58] Creating layer relu_conv11
I1211 13:47:56.778473  8328 net.cpp:84] Creating Layer relu_conv11
I1211 13:47:56.778473  8328 net.cpp:406] relu_conv11 <- conv11
I1211 13:47:56.778473  8328 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 13:47:56.779474  8328 net.cpp:122] Setting up relu_conv11
I1211 13:47:56.779474  8328 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 13:47:56.779474  8328 net.cpp:137] Memory required for data: 559975600
I1211 13:47:56.779474  8328 layer_factory.cpp:58] Creating layer conv12
I1211 13:47:56.779474  8328 net.cpp:84] Creating Layer conv12
I1211 13:47:56.779474  8328 net.cpp:406] conv12 <- conv11
I1211 13:47:56.779474  8328 net.cpp:380] conv12 -> conv12
I1211 13:47:56.782475  8328 net.cpp:122] Setting up conv12
I1211 13:47:56.782475  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.782475  8328 net.cpp:137] Memory required for data: 562279600
I1211 13:47:56.782475  8328 layer_factory.cpp:58] Creating layer bn_conv12
I1211 13:47:56.782475  8328 net.cpp:84] Creating Layer bn_conv12
I1211 13:47:56.782475  8328 net.cpp:406] bn_conv12 <- conv12
I1211 13:47:56.782475  8328 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 13:47:56.782475  8328 net.cpp:122] Setting up bn_conv12
I1211 13:47:56.782475  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.782475  8328 net.cpp:137] Memory required for data: 564583600
I1211 13:47:56.782475  8328 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:47:56.782475  8328 net.cpp:84] Creating Layer scale_conv12
I1211 13:47:56.782475  8328 net.cpp:406] scale_conv12 <- conv12
I1211 13:47:56.782475  8328 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 13:47:56.782475  8328 layer_factory.cpp:58] Creating layer scale_conv12
I1211 13:47:56.783474  8328 net.cpp:122] Setting up scale_conv12
I1211 13:47:56.783474  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.783474  8328 net.cpp:137] Memory required for data: 566887600
I1211 13:47:56.783474  8328 layer_factory.cpp:58] Creating layer relu_conv12
I1211 13:47:56.783474  8328 net.cpp:84] Creating Layer relu_conv12
I1211 13:47:56.783474  8328 net.cpp:406] relu_conv12 <- conv12
I1211 13:47:56.783474  8328 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 13:47:56.783474  8328 net.cpp:122] Setting up relu_conv12
I1211 13:47:56.783474  8328 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 13:47:56.783474  8328 net.cpp:137] Memory required for data: 569191600
I1211 13:47:56.783474  8328 layer_factory.cpp:58] Creating layer poolcp6
I1211 13:47:56.783474  8328 net.cpp:84] Creating Layer poolcp6
I1211 13:47:56.783474  8328 net.cpp:406] poolcp6 <- conv12
I1211 13:47:56.783474  8328 net.cpp:380] poolcp6 -> poolcp6
I1211 13:47:56.783474  8328 net.cpp:122] Setting up poolcp6
I1211 13:47:56.783474  8328 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 13:47:56.784473  8328 net.cpp:137] Memory required for data: 569227600
I1211 13:47:56.784473  8328 layer_factory.cpp:58] Creating layer ip1
I1211 13:47:56.784473  8328 net.cpp:84] Creating Layer ip1
I1211 13:47:56.784473  8328 net.cpp:406] ip1 <- poolcp6
I1211 13:47:56.784473  8328 net.cpp:380] ip1 -> ip1
I1211 13:47:56.784473  8328 net.cpp:122] Setting up ip1
I1211 13:47:56.784473  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.784473  8328 net.cpp:137] Memory required for data: 569267600
I1211 13:47:56.784473  8328 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 13:47:56.784473  8328 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 13:47:56.784473  8328 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 13:47:56.784473  8328 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 13:47:56.784473  8328 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 13:47:56.784473  8328 net.cpp:122] Setting up ip1_ip1_0_split
I1211 13:47:56.784473  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.784473  8328 net.cpp:129] Top shape: 100 100 (10000)
I1211 13:47:56.784473  8328 net.cpp:137] Memory required for data: 569347600
I1211 13:47:56.784473  8328 layer_factory.cpp:58] Creating layer accuracy
I1211 13:47:56.784473  8328 net.cpp:84] Creating Layer accuracy
I1211 13:47:56.784473  8328 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 13:47:56.784473  8328 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 13:47:56.784473  8328 net.cpp:380] accuracy -> accuracy
I1211 13:47:56.784473  8328 net.cpp:122] Setting up accuracy
I1211 13:47:56.784473  8328 net.cpp:129] Top shape: (1)
I1211 13:47:56.784473  8328 net.cpp:137] Memory required for data: 569347604
I1211 13:47:56.784473  8328 layer_factory.cpp:58] Creating layer loss
I1211 13:47:56.784473  8328 net.cpp:84] Creating Layer loss
I1211 13:47:56.784473  8328 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 13:47:56.784473  8328 net.cpp:406] loss <- label_cifar_1_split_1
I1211 13:47:56.784473  8328 net.cpp:380] loss -> loss
I1211 13:47:56.784473  8328 layer_factory.cpp:58] Creating layer loss
I1211 13:47:56.785475  8328 net.cpp:122] Setting up loss
I1211 13:47:56.785475  8328 net.cpp:129] Top shape: (1)
I1211 13:47:56.785475  8328 net.cpp:132]     with loss weight 1
I1211 13:47:56.785475  8328 net.cpp:137] Memory required for data: 569347608
I1211 13:47:56.785475  8328 net.cpp:198] loss needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:200] accuracy does not need backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] ip1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] poolcp6 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu_conv12 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale_conv12 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn_conv12 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv12 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu_conv11 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale_conv11 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn_conv11 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv11 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu4_0 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale4_0 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn4_0 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv4_0 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] pool4_2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu_added2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale_added2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn_added2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] added_new_conv2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu4_2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale4_2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn4_2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv4_2 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu4_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale4_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn4_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv4_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu4 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale4 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn4 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv4 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu3_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale3_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn3_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv3_1 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] relu3 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] scale3 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] bn3 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] conv3 needs backward computation.
I1211 13:47:56.785475  8328 net.cpp:198] pool2_1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu_added1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale_added1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn_added1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] newconv_added1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu2_2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale2_2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn2_2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] conv2_2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu2_1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale2_1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn2_1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] conv2_1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] conv2 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu1_0 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale1_0 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn1_0 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] conv1_0 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] relu1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] scale1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] bn1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:198] conv1 needs backward computation.
I1211 13:47:56.786473  8328 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 13:47:56.786473  8328 net.cpp:200] cifar does not need backward computation.
I1211 13:47:56.786473  8328 net.cpp:242] This network produces output accuracy
I1211 13:47:56.786473  8328 net.cpp:242] This network produces output loss
I1211 13:47:56.786473  8328 net.cpp:255] Network initialization done.
I1211 13:47:56.786473  8328 solver.cpp:56] Solver scaffolding done.
I1211 13:47:56.792474  8328 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate
I1211 13:47:56.795475  8328 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.caffemodel
I1211 13:47:56.795475  8328 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 13:47:56.795475  8328 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 13:47:56.800477  8328 caffe.cpp:249] Starting Optimization
I1211 13:47:56.800477  8328 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k
I1211 13:47:56.800477  8328 solver.cpp:273] Learning Rate Policy: multistep
I1211 13:47:56.803478  8328 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 13:47:56.805475  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:47:58.378495 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:47:58.438504  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5711
I1211 13:47:58.438504  8328 solver.cpp:397]     Test net output #1: loss = 1.67838 (* 1 = 1.67838 loss)
I1211 13:47:58.555517  8328 solver.cpp:218] Iteration 90000 (-nan iter/s, 1.75298s/100 iters), loss = 0.601179
I1211 13:47:58.555517  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:47:58.555517  8328 solver.cpp:237]     Train net output #1: loss = 0.601179 (* 1 = 0.601179 loss)
I1211 13:47:58.555517  8328 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 13:48:04.899569  8328 solver.cpp:218] Iteration 90100 (15.7619 iter/s, 6.3444s/100 iters), loss = 0.722298
I1211 13:48:04.899569  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:48:04.899569  8328 solver.cpp:237]     Train net output #1: loss = 0.722298 (* 1 = 0.722298 loss)
I1211 13:48:04.899569  8328 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 13:48:11.332669  8328 solver.cpp:218] Iteration 90200 (15.5474 iter/s, 6.43194s/100 iters), loss = 0.520172
I1211 13:48:11.332669  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:48:11.332669  8328 solver.cpp:237]     Train net output #1: loss = 0.520172 (* 1 = 0.520172 loss)
I1211 13:48:11.332669  8328 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 13:48:17.856743  8328 solver.cpp:218] Iteration 90300 (15.3292 iter/s, 6.5235s/100 iters), loss = 0.740615
I1211 13:48:17.856743  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:48:17.856743  8328 solver.cpp:237]     Train net output #1: loss = 0.740615 (* 1 = 0.740615 loss)
I1211 13:48:17.856743  8328 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 13:48:24.260155  8328 solver.cpp:218] Iteration 90400 (15.6182 iter/s, 6.40281s/100 iters), loss = 0.792591
I1211 13:48:24.260155  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 13:48:24.260155  8328 solver.cpp:237]     Train net output #1: loss = 0.792591 (* 1 = 0.792591 loss)
I1211 13:48:24.260155  8328 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 13:48:30.353230 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:48:30.603240  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.caffemodel
I1211 13:48:30.619240  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.solverstate
I1211 13:48:30.624241  8328 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 13:48:30.624241  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:48:32.147387 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:48:32.208387  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6056
I1211 13:48:32.208387  8328 solver.cpp:397]     Test net output #1: loss = 1.53473 (* 1 = 1.53473 loss)
I1211 13:48:32.269390  8328 solver.cpp:218] Iteration 90500 (12.4862 iter/s, 8.00884s/100 iters), loss = 0.602182
I1211 13:48:32.269390  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:48:32.269390  8328 solver.cpp:237]     Train net output #1: loss = 0.602182 (* 1 = 0.602182 loss)
I1211 13:48:32.269390  8328 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 13:48:38.611927  8328 solver.cpp:218] Iteration 90600 (15.7679 iter/s, 6.34199s/100 iters), loss = 0.663725
I1211 13:48:38.611927  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:48:38.611927  8328 solver.cpp:237]     Train net output #1: loss = 0.663725 (* 1 = 0.663725 loss)
I1211 13:48:38.611927  8328 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 13:48:45.021890  8328 solver.cpp:218] Iteration 90700 (15.6014 iter/s, 6.40967s/100 iters), loss = 0.57222
I1211 13:48:45.021890  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:48:45.021890  8328 solver.cpp:237]     Train net output #1: loss = 0.57222 (* 1 = 0.57222 loss)
I1211 13:48:45.021890  8328 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 13:48:51.411365  8328 solver.cpp:218] Iteration 90800 (15.652 iter/s, 6.38897s/100 iters), loss = 0.718976
I1211 13:48:51.411365  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:48:51.411365  8328 solver.cpp:237]     Train net output #1: loss = 0.718976 (* 1 = 0.718976 loss)
I1211 13:48:51.411365  8328 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 13:48:57.795904  8328 solver.cpp:218] Iteration 90900 (15.6626 iter/s, 6.38463s/100 iters), loss = 0.725615
I1211 13:48:57.795904  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:48:57.796900  8328 solver.cpp:237]     Train net output #1: loss = 0.725615 (* 1 = 0.725615 loss)
I1211 13:48:57.796900  8328 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 13:49:03.962057 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:49:04.210700  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.caffemodel
I1211 13:49:04.226699  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.solverstate
I1211 13:49:04.231703  8328 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 13:49:04.231703  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:49:05.747705 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:49:05.807703  8328 solver.cpp:397]     Test net output #0: accuracy = 0.575
I1211 13:49:05.807703  8328 solver.cpp:397]     Test net output #1: loss = 1.61829 (* 1 = 1.61829 loss)
I1211 13:49:05.869217  8328 solver.cpp:218] Iteration 91000 (12.3891 iter/s, 8.07162s/100 iters), loss = 0.712102
I1211 13:49:05.869217  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:49:05.869217  8328 solver.cpp:237]     Train net output #1: loss = 0.712102 (* 1 = 0.712102 loss)
I1211 13:49:05.869217  8328 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 13:49:12.215903  8328 solver.cpp:218] Iteration 91100 (15.7567 iter/s, 6.34649s/100 iters), loss = 0.721683
I1211 13:49:12.215903  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:49:12.215903  8328 solver.cpp:237]     Train net output #1: loss = 0.721683 (* 1 = 0.721683 loss)
I1211 13:49:12.215903  8328 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 13:49:18.581481  8328 solver.cpp:218] Iteration 91200 (15.7102 iter/s, 6.3653s/100 iters), loss = 0.624353
I1211 13:49:18.581481  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:49:18.581481  8328 solver.cpp:237]     Train net output #1: loss = 0.624353 (* 1 = 0.624353 loss)
I1211 13:49:18.581481  8328 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 13:49:24.992993  8328 solver.cpp:218] Iteration 91300 (15.599 iter/s, 6.41068s/100 iters), loss = 0.792919
I1211 13:49:24.992993  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:49:24.992993  8328 solver.cpp:237]     Train net output #1: loss = 0.792919 (* 1 = 0.792919 loss)
I1211 13:49:24.992993  8328 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 13:49:31.412472  8328 solver.cpp:218] Iteration 91400 (15.578 iter/s, 6.41932s/100 iters), loss = 0.763499
I1211 13:49:31.412472  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1211 13:49:31.412472  8328 solver.cpp:237]     Train net output #1: loss = 0.763499 (* 1 = 0.763499 loss)
I1211 13:49:31.412472  8328 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 13:49:37.473008 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:49:37.723021  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.caffemodel
I1211 13:49:37.739023  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.solverstate
I1211 13:49:37.744024  8328 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 13:49:37.744024  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:49:39.260151 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:49:39.321149  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5733
I1211 13:49:39.321149  8328 solver.cpp:397]     Test net output #1: loss = 1.68553 (* 1 = 1.68553 loss)
I1211 13:49:39.382177  8328 solver.cpp:218] Iteration 91500 (12.5489 iter/s, 7.96884s/100 iters), loss = 0.602591
I1211 13:49:39.382177  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:49:39.382177  8328 solver.cpp:237]     Train net output #1: loss = 0.602591 (* 1 = 0.602591 loss)
I1211 13:49:39.382177  8328 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 13:49:45.714643  8328 solver.cpp:218] Iteration 91600 (15.7928 iter/s, 6.33198s/100 iters), loss = 0.870552
I1211 13:49:45.714643  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:49:45.714643  8328 solver.cpp:237]     Train net output #1: loss = 0.870552 (* 1 = 0.870552 loss)
I1211 13:49:45.714643  8328 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 13:49:52.079155  8328 solver.cpp:218] Iteration 91700 (15.7112 iter/s, 6.3649s/100 iters), loss = 0.499938
I1211 13:49:52.079155  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:49:52.079155  8328 solver.cpp:237]     Train net output #1: loss = 0.499938 (* 1 = 0.499938 loss)
I1211 13:49:52.079155  8328 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 13:49:58.422624  8328 solver.cpp:218] Iteration 91800 (15.7661 iter/s, 6.3427s/100 iters), loss = 0.849662
I1211 13:49:58.422624  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 13:49:58.422624  8328 solver.cpp:237]     Train net output #1: loss = 0.849662 (* 1 = 0.849662 loss)
I1211 13:49:58.422624  8328 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 13:50:04.854365  8328 solver.cpp:218] Iteration 91900 (15.5486 iter/s, 6.43144s/100 iters), loss = 0.717419
I1211 13:50:04.854365  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:50:04.854365  8328 solver.cpp:237]     Train net output #1: loss = 0.717419 (* 1 = 0.717419 loss)
I1211 13:50:04.854365  8328 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 13:50:10.891804 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:50:11.141317  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.caffemodel
I1211 13:50:11.156822  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.solverstate
I1211 13:50:11.162823  8328 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 13:50:11.162823  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:50:12.680939 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:50:12.740942  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5898
I1211 13:50:12.740942  8328 solver.cpp:397]     Test net output #1: loss = 1.58501 (* 1 = 1.58501 loss)
I1211 13:50:12.800943  8328 solver.cpp:218] Iteration 92000 (12.5844 iter/s, 7.94634s/100 iters), loss = 0.604833
I1211 13:50:12.801944  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:50:12.801944  8328 solver.cpp:237]     Train net output #1: loss = 0.604833 (* 1 = 0.604833 loss)
I1211 13:50:12.801944  8328 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 13:50:19.155443  8328 solver.cpp:218] Iteration 92100 (15.7388 iter/s, 6.35374s/100 iters), loss = 0.829808
I1211 13:50:19.155443  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:50:19.155443  8328 solver.cpp:237]     Train net output #1: loss = 0.829808 (* 1 = 0.829808 loss)
I1211 13:50:19.155443  8328 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 13:50:25.558842  8328 solver.cpp:218] Iteration 92200 (15.6196 iter/s, 6.40222s/100 iters), loss = 0.671734
I1211 13:50:25.558842  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:50:25.558842  8328 solver.cpp:237]     Train net output #1: loss = 0.671734 (* 1 = 0.671734 loss)
I1211 13:50:25.558842  8328 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 13:50:31.905390  8328 solver.cpp:218] Iteration 92300 (15.7557 iter/s, 6.34692s/100 iters), loss = 0.721761
I1211 13:50:31.906394  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:50:31.906394  8328 solver.cpp:237]     Train net output #1: loss = 0.721761 (* 1 = 0.721761 loss)
I1211 13:50:31.906394  8328 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 13:50:38.246891  8328 solver.cpp:218] Iteration 92400 (15.7716 iter/s, 6.34052s/100 iters), loss = 0.666803
I1211 13:50:38.246891  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:50:38.246891  8328 solver.cpp:237]     Train net output #1: loss = 0.666803 (* 1 = 0.666803 loss)
I1211 13:50:38.246891  8328 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 13:50:44.277346 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:50:44.526362  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.caffemodel
I1211 13:50:44.541363  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.solverstate
I1211 13:50:44.546362  8328 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 13:50:44.547371  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:50:46.061453 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:50:46.121459  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5826
I1211 13:50:46.121459  8328 solver.cpp:397]     Test net output #1: loss = 1.65661 (* 1 = 1.65661 loss)
I1211 13:50:46.182458  8328 solver.cpp:218] Iteration 92500 (12.6024 iter/s, 7.93497s/100 iters), loss = 0.605363
I1211 13:50:46.182458  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:50:46.182458  8328 solver.cpp:237]     Train net output #1: loss = 0.605363 (* 1 = 0.605363 loss)
I1211 13:50:46.182458  8328 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 13:50:52.507917  8328 solver.cpp:218] Iteration 92600 (15.8101 iter/s, 6.32505s/100 iters), loss = 0.730256
I1211 13:50:52.507917  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:50:52.507917  8328 solver.cpp:237]     Train net output #1: loss = 0.730256 (* 1 = 0.730256 loss)
I1211 13:50:52.507917  8328 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 13:50:58.850381  8328 solver.cpp:218] Iteration 92700 (15.7687 iter/s, 6.34169s/100 iters), loss = 0.669677
I1211 13:50:58.850381  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:50:58.850381  8328 solver.cpp:237]     Train net output #1: loss = 0.669677 (* 1 = 0.669677 loss)
I1211 13:50:58.850381  8328 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 13:51:05.183100  8328 solver.cpp:218] Iteration 92800 (15.7921 iter/s, 6.33228s/100 iters), loss = 0.744446
I1211 13:51:05.183100  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:51:05.183100  8328 solver.cpp:237]     Train net output #1: loss = 0.744446 (* 1 = 0.744446 loss)
I1211 13:51:05.183100  8328 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 13:51:11.524267  8328 solver.cpp:218] Iteration 92900 (15.7712 iter/s, 6.34067s/100 iters), loss = 0.737705
I1211 13:51:11.524267  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 13:51:11.524267  8328 solver.cpp:237]     Train net output #1: loss = 0.737705 (* 1 = 0.737705 loss)
I1211 13:51:11.524267  8328 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 13:51:17.590615 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:51:17.847640  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.caffemodel
I1211 13:51:17.864640  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.solverstate
I1211 13:51:17.868640  8328 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 13:51:17.869642  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:51:19.385807 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:51:19.445812  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5891
I1211 13:51:19.445812  8328 solver.cpp:397]     Test net output #1: loss = 1.59537 (* 1 = 1.59537 loss)
I1211 13:51:19.506814  8328 solver.cpp:218] Iteration 93000 (12.5281 iter/s, 7.98203s/100 iters), loss = 0.64818
I1211 13:51:19.506814  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:51:19.506814  8328 solver.cpp:237]     Train net output #1: loss = 0.64818 (* 1 = 0.64818 loss)
I1211 13:51:19.506814  8328 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 13:51:25.849337  8328 solver.cpp:218] Iteration 93100 (15.7663 iter/s, 6.34265s/100 iters), loss = 0.68042
I1211 13:51:25.849337  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 13:51:25.849337  8328 solver.cpp:237]     Train net output #1: loss = 0.68042 (* 1 = 0.68042 loss)
I1211 13:51:25.849337  8328 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 13:51:32.185755  8328 solver.cpp:218] Iteration 93200 (15.7839 iter/s, 6.33556s/100 iters), loss = 0.58325
I1211 13:51:32.185755  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:51:32.185755  8328 solver.cpp:237]     Train net output #1: loss = 0.58325 (* 1 = 0.58325 loss)
I1211 13:51:32.185755  8328 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 13:51:38.577195  8328 solver.cpp:218] Iteration 93300 (15.6455 iter/s, 6.39161s/100 iters), loss = 0.819507
I1211 13:51:38.577195  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:51:38.577195  8328 solver.cpp:237]     Train net output #1: loss = 0.819507 (* 1 = 0.819507 loss)
I1211 13:51:38.577195  8328 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 13:51:44.942642  8328 solver.cpp:218] Iteration 93400 (15.7113 iter/s, 6.36485s/100 iters), loss = 0.648128
I1211 13:51:44.942642  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:51:44.942642  8328 solver.cpp:237]     Train net output #1: loss = 0.648128 (* 1 = 0.648128 loss)
I1211 13:51:44.942642  8328 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 13:51:50.993280 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:51:51.250305  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.caffemodel
I1211 13:51:51.266306  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.solverstate
I1211 13:51:51.271306  8328 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 13:51:51.271306  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:51:52.789425 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:51:52.850430  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5714
I1211 13:51:52.850430  8328 solver.cpp:397]     Test net output #1: loss = 1.69112 (* 1 = 1.69112 loss)
I1211 13:51:52.910428  8328 solver.cpp:218] Iteration 93500 (12.5512 iter/s, 7.96734s/100 iters), loss = 0.733078
I1211 13:51:52.910428  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:51:52.910428  8328 solver.cpp:237]     Train net output #1: loss = 0.733078 (* 1 = 0.733078 loss)
I1211 13:51:52.910428  8328 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 13:51:59.286967  8328 solver.cpp:218] Iteration 93600 (15.6834 iter/s, 6.37617s/100 iters), loss = 0.659414
I1211 13:51:59.286967  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:51:59.286967  8328 solver.cpp:237]     Train net output #1: loss = 0.659414 (* 1 = 0.659414 loss)
I1211 13:51:59.286967  8328 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 13:52:05.656317  8328 solver.cpp:218] Iteration 93700 (15.702 iter/s, 6.36861s/100 iters), loss = 0.523538
I1211 13:52:05.656317  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:52:05.656317  8328 solver.cpp:237]     Train net output #1: loss = 0.523538 (* 1 = 0.523538 loss)
I1211 13:52:05.656317  8328 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 13:52:12.073621  8328 solver.cpp:218] Iteration 93800 (15.585 iter/s, 6.41642s/100 iters), loss = 0.930032
I1211 13:52:12.073621  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 13:52:12.073621  8328 solver.cpp:237]     Train net output #1: loss = 0.930032 (* 1 = 0.930032 loss)
I1211 13:52:12.073621  8328 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 13:52:18.497117  8328 solver.cpp:218] Iteration 93900 (15.5683 iter/s, 6.42332s/100 iters), loss = 0.74345
I1211 13:52:18.497117  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:52:18.497117  8328 solver.cpp:237]     Train net output #1: loss = 0.74345 (* 1 = 0.74345 loss)
I1211 13:52:18.497117  8328 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 13:52:24.653023 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:52:24.904103  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.caffemodel
I1211 13:52:24.921092  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.solverstate
I1211 13:52:24.926117  8328 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 13:52:24.926117  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:52:26.478889 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:52:26.539909  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5645
I1211 13:52:26.539909  8328 solver.cpp:397]     Test net output #1: loss = 1.69802 (* 1 = 1.69802 loss)
I1211 13:52:26.602921  8328 solver.cpp:218] Iteration 94000 (12.3374 iter/s, 8.10545s/100 iters), loss = 0.837742
I1211 13:52:26.602921  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:52:26.602921  8328 solver.cpp:237]     Train net output #1: loss = 0.837742 (* 1 = 0.837742 loss)
I1211 13:52:26.602921  8328 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 13:52:33.090350  8328 solver.cpp:218] Iteration 94100 (15.4154 iter/s, 6.48701s/100 iters), loss = 0.747189
I1211 13:52:33.090350  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:52:33.090350  8328 solver.cpp:237]     Train net output #1: loss = 0.747189 (* 1 = 0.747189 loss)
I1211 13:52:33.090350  8328 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 13:52:39.518607  8328 solver.cpp:218] Iteration 94200 (15.5573 iter/s, 6.42787s/100 iters), loss = 0.599061
I1211 13:52:39.518607  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:52:39.518607  8328 solver.cpp:237]     Train net output #1: loss = 0.599061 (* 1 = 0.599061 loss)
I1211 13:52:39.518607  8328 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 13:52:45.851330  8328 solver.cpp:218] Iteration 94300 (15.7919 iter/s, 6.33235s/100 iters), loss = 0.926431
I1211 13:52:45.851330  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 13:52:45.851330  8328 solver.cpp:237]     Train net output #1: loss = 0.926431 (* 1 = 0.926431 loss)
I1211 13:52:45.851330  8328 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 13:52:52.235608  8328 solver.cpp:218] Iteration 94400 (15.6656 iter/s, 6.38341s/100 iters), loss = 0.744979
I1211 13:52:52.235608  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 13:52:52.235608  8328 solver.cpp:237]     Train net output #1: loss = 0.744979 (* 1 = 0.744979 loss)
I1211 13:52:52.235608  8328 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 13:52:58.377595 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:52:58.627614  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.caffemodel
I1211 13:52:58.643617  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.solverstate
I1211 13:52:58.648618  8328 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 13:52:58.648618  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:53:00.189764 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:53:00.250769  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5796
I1211 13:53:00.250769  8328 solver.cpp:397]     Test net output #1: loss = 1.66045 (* 1 = 1.66045 loss)
I1211 13:53:00.312774  8328 solver.cpp:218] Iteration 94500 (12.3809 iter/s, 8.07695s/100 iters), loss = 0.679506
I1211 13:53:00.312774  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:53:00.312774  8328 solver.cpp:237]     Train net output #1: loss = 0.679506 (* 1 = 0.679506 loss)
I1211 13:53:00.312774  8328 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 13:53:06.804630  8328 solver.cpp:218] Iteration 94600 (15.406 iter/s, 6.49099s/100 iters), loss = 0.765158
I1211 13:53:06.804630  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:53:06.804630  8328 solver.cpp:237]     Train net output #1: loss = 0.765158 (* 1 = 0.765158 loss)
I1211 13:53:06.804630  8328 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 13:53:13.257215  8328 solver.cpp:218] Iteration 94700 (15.4984 iter/s, 6.45227s/100 iters), loss = 0.710765
I1211 13:53:13.257215  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:53:13.257215  8328 solver.cpp:237]     Train net output #1: loss = 0.710765 (* 1 = 0.710765 loss)
I1211 13:53:13.257215  8328 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 13:53:19.732986  8328 solver.cpp:218] Iteration 94800 (15.4417 iter/s, 6.47597s/100 iters), loss = 0.700886
I1211 13:53:19.732986  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:53:19.733988  8328 solver.cpp:237]     Train net output #1: loss = 0.700886 (* 1 = 0.700886 loss)
I1211 13:53:19.733988  8328 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 13:53:26.221518  8328 solver.cpp:218] Iteration 94900 (15.4134 iter/s, 6.48788s/100 iters), loss = 0.752441
I1211 13:53:26.221518  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 13:53:26.221518  8328 solver.cpp:237]     Train net output #1: loss = 0.752441 (* 1 = 0.752441 loss)
I1211 13:53:26.221518  8328 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 13:53:32.347997 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:53:32.606048  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.caffemodel
I1211 13:53:32.623041  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.solverstate
I1211 13:53:32.628041  8328 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 13:53:32.628041  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:53:34.195304 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:53:34.256310  8328 solver.cpp:397]     Test net output #0: accuracy = 0.5802
I1211 13:53:34.256310  8328 solver.cpp:397]     Test net output #1: loss = 1.67245 (* 1 = 1.67245 loss)
I1211 13:53:34.319309  8328 solver.cpp:218] Iteration 95000 (12.3507 iter/s, 8.09673s/100 iters), loss = 0.575764
I1211 13:53:34.319309  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:53:34.319309  8328 solver.cpp:237]     Train net output #1: loss = 0.575764 (* 1 = 0.575764 loss)
I1211 13:53:34.319309  8328 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 13:53:34.319309  8328 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 13:53:40.817945  8328 solver.cpp:218] Iteration 95100 (15.3887 iter/s, 6.49828s/100 iters), loss = 0.599897
I1211 13:53:40.817945  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:53:40.817945  8328 solver.cpp:237]     Train net output #1: loss = 0.599897 (* 1 = 0.599897 loss)
I1211 13:53:40.817945  8328 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 13:53:47.330962  8328 solver.cpp:218] Iteration 95200 (15.355 iter/s, 6.51255s/100 iters), loss = 0.408961
I1211 13:53:47.330962  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:53:47.330962  8328 solver.cpp:237]     Train net output #1: loss = 0.408961 (* 1 = 0.408961 loss)
I1211 13:53:47.330962  8328 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 13:53:53.840823  8328 solver.cpp:218] Iteration 95300 (15.3631 iter/s, 6.5091s/100 iters), loss = 0.669212
I1211 13:53:53.840823  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 13:53:53.840823  8328 solver.cpp:237]     Train net output #1: loss = 0.669212 (* 1 = 0.669212 loss)
I1211 13:53:53.840823  8328 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 13:54:00.316380  8328 solver.cpp:218] Iteration 95400 (15.4435 iter/s, 6.47521s/100 iters), loss = 0.517421
I1211 13:54:00.316380  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 13:54:00.316380  8328 solver.cpp:237]     Train net output #1: loss = 0.517421 (* 1 = 0.517421 loss)
I1211 13:54:00.316380  8328 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 13:54:06.508932 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:54:06.771951  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.caffemodel
I1211 13:54:06.787957  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.solverstate
I1211 13:54:06.792951  8328 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 13:54:06.792951  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:54:08.351634 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:54:08.412142  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6744
I1211 13:54:08.412142  8328 solver.cpp:397]     Test net output #1: loss = 1.19555 (* 1 = 1.19555 loss)
I1211 13:54:08.474140  8328 solver.cpp:218] Iteration 95500 (12.2588 iter/s, 8.15737s/100 iters), loss = 0.527594
I1211 13:54:08.474140  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:54:08.474140  8328 solver.cpp:237]     Train net output #1: loss = 0.527594 (* 1 = 0.527594 loss)
I1211 13:54:08.474140  8328 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 13:54:14.972677  8328 solver.cpp:218] Iteration 95600 (15.3891 iter/s, 6.49812s/100 iters), loss = 0.557608
I1211 13:54:14.972677  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:54:14.972677  8328 solver.cpp:237]     Train net output #1: loss = 0.557608 (* 1 = 0.557608 loss)
I1211 13:54:14.972677  8328 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 13:54:21.491650  8328 solver.cpp:218] Iteration 95700 (15.3421 iter/s, 6.51799s/100 iters), loss = 0.422505
I1211 13:54:21.491650  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:54:21.491650  8328 solver.cpp:237]     Train net output #1: loss = 0.422505 (* 1 = 0.422505 loss)
I1211 13:54:21.491650  8328 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 13:54:27.966326  8328 solver.cpp:218] Iteration 95800 (15.4448 iter/s, 6.47467s/100 iters), loss = 0.525498
I1211 13:54:27.966326  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:54:27.966326  8328 solver.cpp:237]     Train net output #1: loss = 0.525498 (* 1 = 0.525498 loss)
I1211 13:54:27.966326  8328 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 13:54:34.478865  8328 solver.cpp:218] Iteration 95900 (15.3558 iter/s, 6.51218s/100 iters), loss = 0.508067
I1211 13:54:34.478865  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:54:34.478865  8328 solver.cpp:237]     Train net output #1: loss = 0.508067 (* 1 = 0.508067 loss)
I1211 13:54:34.478865  8328 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 13:54:40.587188 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:54:40.837219  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.caffemodel
I1211 13:54:40.853225  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.solverstate
I1211 13:54:40.858227  8328 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 13:54:40.858227  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:54:42.407377 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:54:42.467386  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6743
I1211 13:54:42.467386  8328 solver.cpp:397]     Test net output #1: loss = 1.18568 (* 1 = 1.18568 loss)
I1211 13:54:42.529393  8328 solver.cpp:218] Iteration 96000 (12.4233 iter/s, 8.0494s/100 iters), loss = 0.503644
I1211 13:54:42.529393  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:54:42.529393  8328 solver.cpp:237]     Train net output #1: loss = 0.503644 (* 1 = 0.503644 loss)
I1211 13:54:42.529393  8328 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 13:54:48.960258  8328 solver.cpp:218] Iteration 96100 (15.5498 iter/s, 6.43097s/100 iters), loss = 0.58966
I1211 13:54:48.960258  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 13:54:48.960258  8328 solver.cpp:237]     Train net output #1: loss = 0.58966 (* 1 = 0.58966 loss)
I1211 13:54:48.960258  8328 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 13:54:55.450621  8328 solver.cpp:218] Iteration 96200 (15.4092 iter/s, 6.48963s/100 iters), loss = 0.392153
I1211 13:54:55.450621  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:54:55.450621  8328 solver.cpp:237]     Train net output #1: loss = 0.392153 (* 1 = 0.392153 loss)
I1211 13:54:55.450621  8328 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 13:55:01.949182  8328 solver.cpp:218] Iteration 96300 (15.3895 iter/s, 6.49794s/100 iters), loss = 0.618829
I1211 13:55:01.949182  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 13:55:01.949182  8328 solver.cpp:237]     Train net output #1: loss = 0.618829 (* 1 = 0.618829 loss)
I1211 13:55:01.949182  8328 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 13:55:08.437768  8328 solver.cpp:218] Iteration 96400 (15.4119 iter/s, 6.48849s/100 iters), loss = 0.380172
I1211 13:55:08.437768  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:55:08.437768  8328 solver.cpp:237]     Train net output #1: loss = 0.380172 (* 1 = 0.380172 loss)
I1211 13:55:08.437768  8328 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 13:55:14.601382 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:55:14.857450  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.caffemodel
I1211 13:55:14.872450  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.solverstate
I1211 13:55:14.877450  8328 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 13:55:14.877450  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:55:16.421568 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:55:16.482573  8328 solver.cpp:397]     Test net output #0: accuracy = 0.675
I1211 13:55:16.483575  8328 solver.cpp:397]     Test net output #1: loss = 1.18934 (* 1 = 1.18934 loss)
I1211 13:55:16.543578  8328 solver.cpp:218] Iteration 96500 (12.3371 iter/s, 8.10565s/100 iters), loss = 0.529096
I1211 13:55:16.544579  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:55:16.544579  8328 solver.cpp:237]     Train net output #1: loss = 0.529096 (* 1 = 0.529096 loss)
I1211 13:55:16.544579  8328 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 13:55:22.917058  8328 solver.cpp:218] Iteration 96600 (15.6934 iter/s, 6.37212s/100 iters), loss = 0.479947
I1211 13:55:22.917058  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:55:22.917058  8328 solver.cpp:237]     Train net output #1: loss = 0.479947 (* 1 = 0.479947 loss)
I1211 13:55:22.917058  8328 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 13:55:29.250573  8328 solver.cpp:218] Iteration 96700 (15.7898 iter/s, 6.3332s/100 iters), loss = 0.390883
I1211 13:55:29.250573  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:55:29.250573  8328 solver.cpp:237]     Train net output #1: loss = 0.390883 (* 1 = 0.390883 loss)
I1211 13:55:29.250573  8328 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 13:55:35.591073  8328 solver.cpp:218] Iteration 96800 (15.7718 iter/s, 6.34045s/100 iters), loss = 0.603159
I1211 13:55:35.591073  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 13:55:35.591073  8328 solver.cpp:237]     Train net output #1: loss = 0.603159 (* 1 = 0.603159 loss)
I1211 13:55:35.591073  8328 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 13:55:41.941731  8328 solver.cpp:218] Iteration 96900 (15.7486 iter/s, 6.34978s/100 iters), loss = 0.541512
I1211 13:55:41.941731  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:55:41.941731  8328 solver.cpp:237]     Train net output #1: loss = 0.541512 (* 1 = 0.541512 loss)
I1211 13:55:41.941731  8328 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 13:55:47.978327 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:55:48.228343  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.caffemodel
I1211 13:55:48.242347  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.solverstate
I1211 13:55:48.248347  8328 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 13:55:48.248347  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:55:49.763454 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:55:49.823443  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 13:55:49.823443  8328 solver.cpp:397]     Test net output #1: loss = 1.19103 (* 1 = 1.19103 loss)
I1211 13:55:49.884452  8328 solver.cpp:218] Iteration 97000 (12.5904 iter/s, 7.94259s/100 iters), loss = 0.48562
I1211 13:55:49.884452  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:55:49.884452  8328 solver.cpp:237]     Train net output #1: loss = 0.48562 (* 1 = 0.48562 loss)
I1211 13:55:49.884452  8328 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 13:55:56.272119  8328 solver.cpp:218] Iteration 97100 (15.6769 iter/s, 6.37882s/100 iters), loss = 0.51905
I1211 13:55:56.272119  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:55:56.272119  8328 solver.cpp:237]     Train net output #1: loss = 0.51905 (* 1 = 0.51905 loss)
I1211 13:55:56.272119  8328 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 13:56:02.671835  8328 solver.cpp:218] Iteration 97200 (15.6289 iter/s, 6.39839s/100 iters), loss = 0.375414
I1211 13:56:02.671835  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:56:02.671835  8328 solver.cpp:237]     Train net output #1: loss = 0.375414 (* 1 = 0.375414 loss)
I1211 13:56:02.671835  8328 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 13:56:09.032037  8328 solver.cpp:218] Iteration 97300 (15.7227 iter/s, 6.36022s/100 iters), loss = 0.457286
I1211 13:56:09.032538  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:56:09.032538  8328 solver.cpp:237]     Train net output #1: loss = 0.457286 (* 1 = 0.457286 loss)
I1211 13:56:09.032538  8328 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 13:56:15.390259  8328 solver.cpp:218] Iteration 97400 (15.7287 iter/s, 6.35779s/100 iters), loss = 0.462198
I1211 13:56:15.390259  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:56:15.390259  8328 solver.cpp:237]     Train net output #1: loss = 0.462198 (* 1 = 0.462198 loss)
I1211 13:56:15.390259  8328 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 13:56:21.426316 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:56:21.676913  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.caffemodel
I1211 13:56:21.698912  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.solverstate
I1211 13:56:21.708922  8328 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 13:56:21.709913  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:56:23.229084 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:56:23.289121  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6769
I1211 13:56:23.289121  8328 solver.cpp:397]     Test net output #1: loss = 1.18907 (* 1 = 1.18907 loss)
I1211 13:56:23.350143  8328 solver.cpp:218] Iteration 97500 (12.5635 iter/s, 7.95956s/100 iters), loss = 0.498551
I1211 13:56:23.350143  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:56:23.350143  8328 solver.cpp:237]     Train net output #1: loss = 0.498551 (* 1 = 0.498551 loss)
I1211 13:56:23.350143  8328 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 13:56:29.688727  8328 solver.cpp:218] Iteration 97600 (15.7795 iter/s, 6.33733s/100 iters), loss = 0.434295
I1211 13:56:29.688727  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:56:29.688727  8328 solver.cpp:237]     Train net output #1: loss = 0.434295 (* 1 = 0.434295 loss)
I1211 13:56:29.688727  8328 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 13:56:36.021169  8328 solver.cpp:218] Iteration 97700 (15.7919 iter/s, 6.33235s/100 iters), loss = 0.392011
I1211 13:56:36.021169  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:56:36.021169  8328 solver.cpp:237]     Train net output #1: loss = 0.392011 (* 1 = 0.392011 loss)
I1211 13:56:36.021169  8328 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 13:56:42.356624  8328 solver.cpp:218] Iteration 97800 (15.7839 iter/s, 6.33556s/100 iters), loss = 0.470448
I1211 13:56:42.356624  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:56:42.356624  8328 solver.cpp:237]     Train net output #1: loss = 0.470448 (* 1 = 0.470448 loss)
I1211 13:56:42.357625  8328 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 13:56:48.757119  8328 solver.cpp:218] Iteration 97900 (15.6265 iter/s, 6.39938s/100 iters), loss = 0.459949
I1211 13:56:48.757119  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 13:56:48.757119  8328 solver.cpp:237]     Train net output #1: loss = 0.459949 (* 1 = 0.459949 loss)
I1211 13:56:48.757119  8328 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 13:56:54.864971 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:56:55.122984  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.caffemodel
I1211 13:56:55.140990  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.solverstate
I1211 13:56:55.146991  8328 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 13:56:55.146991  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:56:56.698210 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:56:56.760219  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 13:56:56.760219  8328 solver.cpp:397]     Test net output #1: loss = 1.19218 (* 1 = 1.19218 loss)
I1211 13:56:56.822218  8328 solver.cpp:218] Iteration 98000 (12.3993 iter/s, 8.06497s/100 iters), loss = 0.437312
I1211 13:56:56.822218  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:56:56.822218  8328 solver.cpp:237]     Train net output #1: loss = 0.437312 (* 1 = 0.437312 loss)
I1211 13:56:56.822218  8328 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 13:57:03.278810  8328 solver.cpp:218] Iteration 98100 (15.4888 iter/s, 6.45626s/100 iters), loss = 0.479189
I1211 13:57:03.278810  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:57:03.278810  8328 solver.cpp:237]     Train net output #1: loss = 0.479189 (* 1 = 0.479189 loss)
I1211 13:57:03.278810  8328 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 13:57:09.681373  8328 solver.cpp:218] Iteration 98200 (15.6212 iter/s, 6.40157s/100 iters), loss = 0.286524
I1211 13:57:09.681373  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:57:09.681373  8328 solver.cpp:237]     Train net output #1: loss = 0.286524 (* 1 = 0.286524 loss)
I1211 13:57:09.681373  8328 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 13:57:16.071905  8328 solver.cpp:218] Iteration 98300 (15.6475 iter/s, 6.39079s/100 iters), loss = 0.479615
I1211 13:57:16.071905  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:57:16.071905  8328 solver.cpp:237]     Train net output #1: loss = 0.479615 (* 1 = 0.479615 loss)
I1211 13:57:16.071905  8328 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 13:57:22.506520  8328 solver.cpp:218] Iteration 98400 (15.5433 iter/s, 6.43365s/100 iters), loss = 0.470766
I1211 13:57:22.506520  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:57:22.506520  8328 solver.cpp:237]     Train net output #1: loss = 0.470766 (* 1 = 0.470766 loss)
I1211 13:57:22.506520  8328 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 13:57:28.658908 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:57:28.914925  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.caffemodel
I1211 13:57:28.930925  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.solverstate
I1211 13:57:28.935925  8328 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 13:57:28.935925  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:57:30.463030 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:57:30.524035  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1211 13:57:30.525037  8328 solver.cpp:397]     Test net output #1: loss = 1.19288 (* 1 = 1.19288 loss)
I1211 13:57:30.585036  8328 solver.cpp:218] Iteration 98500 (12.3785 iter/s, 8.07854s/100 iters), loss = 0.378679
I1211 13:57:30.585036  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:57:30.586036  8328 solver.cpp:237]     Train net output #1: loss = 0.378679 (* 1 = 0.378679 loss)
I1211 13:57:30.586036  8328 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 13:57:37.049559  8328 solver.cpp:218] Iteration 98600 (15.4709 iter/s, 6.46376s/100 iters), loss = 0.481419
I1211 13:57:37.049559  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:57:37.049559  8328 solver.cpp:237]     Train net output #1: loss = 0.481419 (* 1 = 0.481419 loss)
I1211 13:57:37.049559  8328 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 13:57:43.523100  8328 solver.cpp:218] Iteration 98700 (15.4495 iter/s, 6.47272s/100 iters), loss = 0.378281
I1211 13:57:43.523100  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 13:57:43.523100  8328 solver.cpp:237]     Train net output #1: loss = 0.378281 (* 1 = 0.378281 loss)
I1211 13:57:43.523100  8328 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 13:57:49.931125  8328 solver.cpp:218] Iteration 98800 (15.6069 iter/s, 6.40743s/100 iters), loss = 0.505772
I1211 13:57:49.931125  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:57:49.931125  8328 solver.cpp:237]     Train net output #1: loss = 0.505772 (* 1 = 0.505772 loss)
I1211 13:57:49.931125  8328 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 13:57:56.286844  8328 solver.cpp:218] Iteration 98900 (15.735 iter/s, 6.35528s/100 iters), loss = 0.408833
I1211 13:57:56.286844  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:57:56.286844  8328 solver.cpp:237]     Train net output #1: loss = 0.408833 (* 1 = 0.408833 loss)
I1211 13:57:56.286844  8328 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 13:58:02.326010 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:58:02.577535  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.caffemodel
I1211 13:58:02.594547  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.solverstate
I1211 13:58:02.599548  8328 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 13:58:02.599548  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:58:04.119485 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:58:04.180995  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6763
I1211 13:58:04.180995  8328 solver.cpp:397]     Test net output #1: loss = 1.19664 (* 1 = 1.19664 loss)
I1211 13:58:04.243016  8328 solver.cpp:218] Iteration 99000 (12.5701 iter/s, 7.9554s/100 iters), loss = 0.420131
I1211 13:58:04.243016  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:58:04.243016  8328 solver.cpp:237]     Train net output #1: loss = 0.42013 (* 1 = 0.42013 loss)
I1211 13:58:04.243016  8328 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 13:58:10.670306  8328 solver.cpp:218] Iteration 99100 (15.5598 iter/s, 6.42682s/100 iters), loss = 0.456542
I1211 13:58:10.670306  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:58:10.670306  8328 solver.cpp:237]     Train net output #1: loss = 0.456542 (* 1 = 0.456542 loss)
I1211 13:58:10.670306  8328 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 13:58:17.037811  8328 solver.cpp:218] Iteration 99200 (15.7055 iter/s, 6.3672s/100 iters), loss = 0.349455
I1211 13:58:17.037811  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:58:17.037811  8328 solver.cpp:237]     Train net output #1: loss = 0.349455 (* 1 = 0.349455 loss)
I1211 13:58:17.037811  8328 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 13:58:23.476264  8328 solver.cpp:218] Iteration 99300 (15.5316 iter/s, 6.43848s/100 iters), loss = 0.483396
I1211 13:58:23.476264  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:58:23.476264  8328 solver.cpp:237]     Train net output #1: loss = 0.483395 (* 1 = 0.483395 loss)
I1211 13:58:23.476264  8328 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 13:58:29.896554  8328 solver.cpp:218] Iteration 99400 (15.5766 iter/s, 6.41989s/100 iters), loss = 0.482786
I1211 13:58:29.896554  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 13:58:29.896554  8328 solver.cpp:237]     Train net output #1: loss = 0.482786 (* 1 = 0.482786 loss)
I1211 13:58:29.896554  8328 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 13:58:36.015903 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:58:36.264931  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.caffemodel
I1211 13:58:36.280931  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.solverstate
I1211 13:58:36.286932  8328 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 13:58:36.286932  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:58:37.802229 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:58:37.863289  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6774
I1211 13:58:37.863289  8328 solver.cpp:397]     Test net output #1: loss = 1.19767 (* 1 = 1.19767 loss)
I1211 13:58:37.924290  8328 solver.cpp:218] Iteration 99500 (12.4584 iter/s, 8.02671s/100 iters), loss = 0.419035
I1211 13:58:37.924290  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 13:58:37.924290  8328 solver.cpp:237]     Train net output #1: loss = 0.419035 (* 1 = 0.419035 loss)
I1211 13:58:37.924290  8328 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 13:58:44.307405  8328 solver.cpp:218] Iteration 99600 (15.6665 iter/s, 6.38304s/100 iters), loss = 0.482086
I1211 13:58:44.307405  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:58:44.307405  8328 solver.cpp:237]     Train net output #1: loss = 0.482086 (* 1 = 0.482086 loss)
I1211 13:58:44.307405  8328 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 13:58:50.651674  8328 solver.cpp:218] Iteration 99700 (15.7643 iter/s, 6.34346s/100 iters), loss = 0.520932
I1211 13:58:50.651674  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:58:50.651674  8328 solver.cpp:237]     Train net output #1: loss = 0.520932 (* 1 = 0.520932 loss)
I1211 13:58:50.651674  8328 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 13:58:57.042160  8328 solver.cpp:218] Iteration 99800 (15.6474 iter/s, 6.39083s/100 iters), loss = 0.433261
I1211 13:58:57.043161  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:58:57.043161  8328 solver.cpp:237]     Train net output #1: loss = 0.433261 (* 1 = 0.433261 loss)
I1211 13:58:57.043161  8328 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 13:59:03.449477  8328 solver.cpp:218] Iteration 99900 (15.6098 iter/s, 6.40625s/100 iters), loss = 0.449488
I1211 13:59:03.449477  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 13:59:03.449477  8328 solver.cpp:237]     Train net output #1: loss = 0.449488 (* 1 = 0.449488 loss)
I1211 13:59:03.449962  8328 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 13:59:09.571456 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:59:09.822474  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.caffemodel
I1211 13:59:09.837474  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.solverstate
I1211 13:59:09.842473  8328 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 13:59:09.842473  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:59:11.361225 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:59:11.421727  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6754
I1211 13:59:11.421727  8328 solver.cpp:397]     Test net output #1: loss = 1.1992 (* 1 = 1.1992 loss)
I1211 13:59:11.482738  8328 solver.cpp:218] Iteration 100000 (12.4492 iter/s, 8.03262s/100 iters), loss = 0.395167
I1211 13:59:11.482738  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 13:59:11.482738  8328 solver.cpp:237]     Train net output #1: loss = 0.395167 (* 1 = 0.395167 loss)
I1211 13:59:11.482738  8328 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 13:59:17.931186  8328 solver.cpp:218] Iteration 100100 (15.5079 iter/s, 6.44833s/100 iters), loss = 0.415834
I1211 13:59:17.931186  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 13:59:17.931186  8328 solver.cpp:237]     Train net output #1: loss = 0.415834 (* 1 = 0.415834 loss)
I1211 13:59:17.931186  8328 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 13:59:24.416285  8328 solver.cpp:218] Iteration 100200 (15.4206 iter/s, 6.48482s/100 iters), loss = 0.31371
I1211 13:59:24.416285  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:59:24.416285  8328 solver.cpp:237]     Train net output #1: loss = 0.31371 (* 1 = 0.31371 loss)
I1211 13:59:24.416285  8328 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 13:59:30.863272  8328 solver.cpp:218] Iteration 100300 (15.5139 iter/s, 6.44584s/100 iters), loss = 0.36069
I1211 13:59:30.863272  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 13:59:30.863272  8328 solver.cpp:237]     Train net output #1: loss = 0.36069 (* 1 = 0.36069 loss)
I1211 13:59:30.863272  8328 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 13:59:37.255549  8328 solver.cpp:218] Iteration 100400 (15.6445 iter/s, 6.39204s/100 iters), loss = 0.407288
I1211 13:59:37.255549  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 13:59:37.255549  8328 solver.cpp:237]     Train net output #1: loss = 0.407288 (* 1 = 0.407288 loss)
I1211 13:59:37.255549  8328 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 13:59:43.313948 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:59:43.563976  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.caffemodel
I1211 13:59:43.578987  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.solverstate
I1211 13:59:43.583986  8328 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 13:59:43.583986  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 13:59:45.102373 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 13:59:45.162369  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6801
I1211 13:59:45.162369  8328 solver.cpp:397]     Test net output #1: loss = 1.20275 (* 1 = 1.20275 loss)
I1211 13:59:45.222371  8328 solver.cpp:218] Iteration 100500 (12.5519 iter/s, 7.96692s/100 iters), loss = 0.438205
I1211 13:59:45.222371  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 13:59:45.223372  8328 solver.cpp:237]     Train net output #1: loss = 0.438205 (* 1 = 0.438205 loss)
I1211 13:59:45.223372  8328 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 13:59:51.555835  8328 solver.cpp:218] Iteration 100600 (15.7921 iter/s, 6.33228s/100 iters), loss = 0.464291
I1211 13:59:51.555835  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 13:59:51.555835  8328 solver.cpp:237]     Train net output #1: loss = 0.464291 (* 1 = 0.464291 loss)
I1211 13:59:51.555835  8328 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 13:59:57.887286  8328 solver.cpp:218] Iteration 100700 (15.7946 iter/s, 6.33128s/100 iters), loss = 0.352575
I1211 13:59:57.887286  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 13:59:57.887286  8328 solver.cpp:237]     Train net output #1: loss = 0.352574 (* 1 = 0.352574 loss)
I1211 13:59:57.887286  8328 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 14:00:04.360409  8328 solver.cpp:218] Iteration 100800 (15.4499 iter/s, 6.47253s/100 iters), loss = 0.38676
I1211 14:00:04.360409  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:00:04.360409  8328 solver.cpp:237]     Train net output #1: loss = 0.38676 (* 1 = 0.38676 loss)
I1211 14:00:04.360409  8328 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 14:00:10.729444  8328 solver.cpp:218] Iteration 100900 (15.7008 iter/s, 6.36912s/100 iters), loss = 0.383345
I1211 14:00:10.729444  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:00:10.729444  8328 solver.cpp:237]     Train net output #1: loss = 0.383345 (* 1 = 0.383345 loss)
I1211 14:00:10.729444  8328 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 14:00:16.763842 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:00:17.012854  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.caffemodel
I1211 14:00:17.027855  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.solverstate
I1211 14:00:17.032855  8328 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 14:00:17.032855  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:00:18.558971 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:00:18.618975  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 14:00:18.618975  8328 solver.cpp:397]     Test net output #1: loss = 1.20815 (* 1 = 1.20815 loss)
I1211 14:00:18.678978  8328 solver.cpp:218] Iteration 101000 (12.5804 iter/s, 7.94887s/100 iters), loss = 0.404247
I1211 14:00:18.678978  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:00:18.678978  8328 solver.cpp:237]     Train net output #1: loss = 0.404247 (* 1 = 0.404247 loss)
I1211 14:00:18.678978  8328 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 14:00:25.039463  8328 solver.cpp:218] Iteration 101100 (15.7233 iter/s, 6.35998s/100 iters), loss = 0.442148
I1211 14:00:25.039463  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:00:25.039463  8328 solver.cpp:237]     Train net output #1: loss = 0.442148 (* 1 = 0.442148 loss)
I1211 14:00:25.039463  8328 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 14:00:31.440907  8328 solver.cpp:218] Iteration 101200 (15.6215 iter/s, 6.40144s/100 iters), loss = 0.401156
I1211 14:00:31.441906  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:00:31.441906  8328 solver.cpp:237]     Train net output #1: loss = 0.401156 (* 1 = 0.401156 loss)
I1211 14:00:31.441906  8328 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 14:00:37.779419  8328 solver.cpp:218] Iteration 101300 (15.7795 iter/s, 6.33735s/100 iters), loss = 0.525265
I1211 14:00:37.779419  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 14:00:37.779419  8328 solver.cpp:237]     Train net output #1: loss = 0.525265 (* 1 = 0.525265 loss)
I1211 14:00:37.779419  8328 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 14:00:44.138897  8328 solver.cpp:218] Iteration 101400 (15.7261 iter/s, 6.35885s/100 iters), loss = 0.444328
I1211 14:00:44.138897  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:00:44.138897  8328 solver.cpp:237]     Train net output #1: loss = 0.444328 (* 1 = 0.444328 loss)
I1211 14:00:44.138897  8328 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 14:00:50.185312 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:00:50.436324  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.caffemodel
I1211 14:00:50.451323  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.solverstate
I1211 14:00:50.456323  8328 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 14:00:50.456323  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:00:51.973431 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:00:52.033437  8328 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 14:00:52.033437  8328 solver.cpp:397]     Test net output #1: loss = 1.21745 (* 1 = 1.21745 loss)
I1211 14:00:52.093438  8328 solver.cpp:218] Iteration 101500 (12.5712 iter/s, 7.95469s/100 iters), loss = 0.467109
I1211 14:00:52.093438  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 14:00:52.093438  8328 solver.cpp:237]     Train net output #1: loss = 0.467109 (* 1 = 0.467109 loss)
I1211 14:00:52.093438  8328 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 14:00:58.432909  8328 solver.cpp:218] Iteration 101600 (15.7751 iter/s, 6.33909s/100 iters), loss = 0.494277
I1211 14:00:58.432909  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 14:00:58.432909  8328 solver.cpp:237]     Train net output #1: loss = 0.494277 (* 1 = 0.494277 loss)
I1211 14:00:58.432909  8328 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 14:01:04.768848  8328 solver.cpp:218] Iteration 101700 (15.7859 iter/s, 6.33476s/100 iters), loss = 0.327216
I1211 14:01:04.768848  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:01:04.768848  8328 solver.cpp:237]     Train net output #1: loss = 0.327216 (* 1 = 0.327216 loss)
I1211 14:01:04.768848  8328 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 14:01:11.102779  8328 solver.cpp:218] Iteration 101800 (15.7869 iter/s, 6.33437s/100 iters), loss = 0.596826
I1211 14:01:11.103780  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:01:11.103780  8328 solver.cpp:237]     Train net output #1: loss = 0.596826 (* 1 = 0.596826 loss)
I1211 14:01:11.103780  8328 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 14:01:17.440253  8328 solver.cpp:218] Iteration 101900 (15.782 iter/s, 6.33633s/100 iters), loss = 0.487283
I1211 14:01:17.440253  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 14:01:17.440253  8328 solver.cpp:237]     Train net output #1: loss = 0.487283 (* 1 = 0.487283 loss)
I1211 14:01:17.440253  8328 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 14:01:23.483693 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:01:23.733703  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.caffemodel
I1211 14:01:23.750705  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.solverstate
I1211 14:01:23.755704  8328 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 14:01:23.755704  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:01:25.275851 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:01:25.335853  8328 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 14:01:25.335853  8328 solver.cpp:397]     Test net output #1: loss = 1.20984 (* 1 = 1.20984 loss)
I1211 14:01:25.396860  8328 solver.cpp:218] Iteration 102000 (12.5697 iter/s, 7.95565s/100 iters), loss = 0.355214
I1211 14:01:25.396860  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:01:25.396860  8328 solver.cpp:237]     Train net output #1: loss = 0.355214 (* 1 = 0.355214 loss)
I1211 14:01:25.396860  8328 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 14:01:31.759344  8328 solver.cpp:218] Iteration 102100 (15.7172 iter/s, 6.36247s/100 iters), loss = 0.417156
I1211 14:01:31.759344  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:01:31.759344  8328 solver.cpp:237]     Train net output #1: loss = 0.417156 (* 1 = 0.417156 loss)
I1211 14:01:31.759344  8328 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 14:01:38.104878  8328 solver.cpp:218] Iteration 102200 (15.76 iter/s, 6.34517s/100 iters), loss = 0.285514
I1211 14:01:38.104878  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 14:01:38.104878  8328 solver.cpp:237]     Train net output #1: loss = 0.285514 (* 1 = 0.285514 loss)
I1211 14:01:38.104878  8328 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 14:01:44.462286  8328 solver.cpp:218] Iteration 102300 (15.7313 iter/s, 6.35677s/100 iters), loss = 0.385791
I1211 14:01:44.462286  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:01:44.462286  8328 solver.cpp:237]     Train net output #1: loss = 0.385791 (* 1 = 0.385791 loss)
I1211 14:01:44.462286  8328 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 14:01:50.911486  8328 solver.cpp:218] Iteration 102400 (15.5077 iter/s, 6.44839s/100 iters), loss = 0.443702
I1211 14:01:50.911486  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 14:01:50.911486  8328 solver.cpp:237]     Train net output #1: loss = 0.443702 (* 1 = 0.443702 loss)
I1211 14:01:50.911486  8328 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 14:01:57.066279 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:01:57.319313  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.caffemodel
I1211 14:01:57.334309  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.solverstate
I1211 14:01:57.339320  8328 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 14:01:57.339320  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:01:58.903921 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:01:58.964920  8328 solver.cpp:397]     Test net output #0: accuracy = 0.678
I1211 14:01:58.965930  8328 solver.cpp:397]     Test net output #1: loss = 1.21109 (* 1 = 1.21109 loss)
I1211 14:01:59.027931  8328 solver.cpp:218] Iteration 102500 (12.3207 iter/s, 8.11641s/100 iters), loss = 0.385744
I1211 14:01:59.027931  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:01:59.027931  8328 solver.cpp:237]     Train net output #1: loss = 0.385744 (* 1 = 0.385744 loss)
I1211 14:01:59.027931  8328 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 14:02:05.458494  8328 solver.cpp:218] Iteration 102600 (15.5531 iter/s, 6.4296s/100 iters), loss = 0.448441
I1211 14:02:05.458494  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 14:02:05.458494  8328 solver.cpp:237]     Train net output #1: loss = 0.448441 (* 1 = 0.448441 loss)
I1211 14:02:05.458494  8328 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 14:02:11.926921  8328 solver.cpp:218] Iteration 102700 (15.4603 iter/s, 6.46818s/100 iters), loss = 0.303015
I1211 14:02:11.926921  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:02:11.926921  8328 solver.cpp:237]     Train net output #1: loss = 0.303015 (* 1 = 0.303015 loss)
I1211 14:02:11.926921  8328 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 14:02:18.326783  8328 solver.cpp:218] Iteration 102800 (15.6274 iter/s, 6.39901s/100 iters), loss = 0.449306
I1211 14:02:18.326783  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:02:18.326783  8328 solver.cpp:237]     Train net output #1: loss = 0.449306 (* 1 = 0.449306 loss)
I1211 14:02:18.326783  8328 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 14:02:24.667223  8328 solver.cpp:218] Iteration 102900 (15.7719 iter/s, 6.34041s/100 iters), loss = 0.406004
I1211 14:02:24.667223  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 14:02:24.667223  8328 solver.cpp:237]     Train net output #1: loss = 0.406004 (* 1 = 0.406004 loss)
I1211 14:02:24.667223  8328 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 14:02:30.707667 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:02:30.957680  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.caffemodel
I1211 14:02:30.972679  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.solverstate
I1211 14:02:30.978184  8328 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 14:02:30.978684  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:02:32.501833 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:02:32.563834  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6763
I1211 14:02:32.563834  8328 solver.cpp:397]     Test net output #1: loss = 1.21699 (* 1 = 1.21699 loss)
I1211 14:02:32.625838  8328 solver.cpp:218] Iteration 103000 (12.5654 iter/s, 7.95836s/100 iters), loss = 0.345424
I1211 14:02:32.625838  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:02:32.625838  8328 solver.cpp:237]     Train net output #1: loss = 0.345424 (* 1 = 0.345424 loss)
I1211 14:02:32.625838  8328 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 14:02:38.986433  8328 solver.cpp:218] Iteration 103100 (15.7248 iter/s, 6.35939s/100 iters), loss = 0.433137
I1211 14:02:38.986433  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:02:38.986433  8328 solver.cpp:237]     Train net output #1: loss = 0.433137 (* 1 = 0.433137 loss)
I1211 14:02:38.986433  8328 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 14:02:45.397766  8328 solver.cpp:218] Iteration 103200 (15.5967 iter/s, 6.4116s/100 iters), loss = 0.38043
I1211 14:02:45.397766  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:02:45.397766  8328 solver.cpp:237]     Train net output #1: loss = 0.38043 (* 1 = 0.38043 loss)
I1211 14:02:45.397766  8328 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 14:02:51.771690  8328 solver.cpp:218] Iteration 103300 (15.6908 iter/s, 6.37316s/100 iters), loss = 0.381522
I1211 14:02:51.771690  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:02:51.771690  8328 solver.cpp:237]     Train net output #1: loss = 0.381522 (* 1 = 0.381522 loss)
I1211 14:02:51.771690  8328 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 14:02:58.151448  8328 solver.cpp:218] Iteration 103400 (15.6751 iter/s, 6.37955s/100 iters), loss = 0.362701
I1211 14:02:58.151448  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:02:58.151448  8328 solver.cpp:237]     Train net output #1: loss = 0.362701 (* 1 = 0.362701 loss)
I1211 14:02:58.151448  8328 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 14:03:04.222020 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:03:04.474042  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.caffemodel
I1211 14:03:04.489550  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.solverstate
I1211 14:03:04.494552  8328 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 14:03:04.494552  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:03:06.009464 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:03:06.068462  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6787
I1211 14:03:06.068462  8328 solver.cpp:397]     Test net output #1: loss = 1.21321 (* 1 = 1.21321 loss)
I1211 14:03:06.128473  8328 solver.cpp:218] Iteration 103500 (12.5367 iter/s, 7.97661s/100 iters), loss = 0.473916
I1211 14:03:06.128473  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:03:06.128473  8328 solver.cpp:237]     Train net output #1: loss = 0.473916 (* 1 = 0.473916 loss)
I1211 14:03:06.128473  8328 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 14:03:12.469581  8328 solver.cpp:218] Iteration 103600 (15.7707 iter/s, 6.34085s/100 iters), loss = 0.404584
I1211 14:03:12.469581  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:03:12.470582  8328 solver.cpp:237]     Train net output #1: loss = 0.404584 (* 1 = 0.404584 loss)
I1211 14:03:12.470582  8328 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 14:03:18.795554  8328 solver.cpp:218] Iteration 103700 (15.8106 iter/s, 6.32488s/100 iters), loss = 0.315438
I1211 14:03:18.795554  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:03:18.795554  8328 solver.cpp:237]     Train net output #1: loss = 0.315438 (* 1 = 0.315438 loss)
I1211 14:03:18.795554  8328 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 14:03:25.205627  8328 solver.cpp:218] Iteration 103800 (15.6022 iter/s, 6.40936s/100 iters), loss = 0.470887
I1211 14:03:25.205627  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:03:25.205627  8328 solver.cpp:237]     Train net output #1: loss = 0.470887 (* 1 = 0.470887 loss)
I1211 14:03:25.205627  8328 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 14:03:31.597484  8328 solver.cpp:218] Iteration 103900 (15.6456 iter/s, 6.39157s/100 iters), loss = 0.438223
I1211 14:03:31.597484  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 14:03:31.597484  8328 solver.cpp:237]     Train net output #1: loss = 0.438223 (* 1 = 0.438223 loss)
I1211 14:03:31.597484  8328 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 14:03:37.729954 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:03:37.980526  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.caffemodel
I1211 14:03:37.996031  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.solverstate
I1211 14:03:38.000532  8328 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 14:03:38.000532  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:03:39.531687 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:03:39.593686  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 14:03:39.593686  8328 solver.cpp:397]     Test net output #1: loss = 1.21882 (* 1 = 1.21882 loss)
I1211 14:03:39.658305  8328 solver.cpp:218] Iteration 104000 (12.4068 iter/s, 8.06013s/100 iters), loss = 0.445955
I1211 14:03:39.658305  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:03:39.658305  8328 solver.cpp:237]     Train net output #1: loss = 0.445955 (* 1 = 0.445955 loss)
I1211 14:03:39.658305  8328 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 14:03:46.086714  8328 solver.cpp:218] Iteration 104100 (15.5556 iter/s, 6.42854s/100 iters), loss = 0.456569
I1211 14:03:46.086714  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:03:46.086714  8328 solver.cpp:237]     Train net output #1: loss = 0.456569 (* 1 = 0.456569 loss)
I1211 14:03:46.086714  8328 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 14:03:52.511847  8328 solver.cpp:218] Iteration 104200 (15.5657 iter/s, 6.42436s/100 iters), loss = 0.294556
I1211 14:03:52.511847  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:03:52.511847  8328 solver.cpp:237]     Train net output #1: loss = 0.294556 (* 1 = 0.294556 loss)
I1211 14:03:52.511847  8328 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 14:03:58.873714  8328 solver.cpp:218] Iteration 104300 (15.72 iter/s, 6.36133s/100 iters), loss = 0.386293
I1211 14:03:58.873714  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:03:58.873714  8328 solver.cpp:237]     Train net output #1: loss = 0.386293 (* 1 = 0.386293 loss)
I1211 14:03:58.873714  8328 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 14:04:05.238010  8328 solver.cpp:218] Iteration 104400 (15.7124 iter/s, 6.36439s/100 iters), loss = 0.347359
I1211 14:04:05.238010  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:04:05.238010  8328 solver.cpp:237]     Train net output #1: loss = 0.347359 (* 1 = 0.347359 loss)
I1211 14:04:05.238010  8328 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 14:04:11.282081 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:04:11.530102  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.caffemodel
I1211 14:04:11.545102  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.solverstate
I1211 14:04:11.550101  8328 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 14:04:11.550101  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:04:13.069545 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:04:13.129551  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6759
I1211 14:04:13.129551  8328 solver.cpp:397]     Test net output #1: loss = 1.2231 (* 1 = 1.2231 loss)
I1211 14:04:13.189558  8328 solver.cpp:218] Iteration 104500 (12.5767 iter/s, 7.95122s/100 iters), loss = 0.380624
I1211 14:04:13.189558  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:04:13.189558  8328 solver.cpp:237]     Train net output #1: loss = 0.380624 (* 1 = 0.380624 loss)
I1211 14:04:13.189558  8328 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 14:04:19.523649  8328 solver.cpp:218] Iteration 104600 (15.7903 iter/s, 6.333s/100 iters), loss = 0.432922
I1211 14:04:19.523649  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:04:19.523649  8328 solver.cpp:237]     Train net output #1: loss = 0.432922 (* 1 = 0.432922 loss)
I1211 14:04:19.523649  8328 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 14:04:25.896543  8328 solver.cpp:218] Iteration 104700 (15.6912 iter/s, 6.37298s/100 iters), loss = 0.320097
I1211 14:04:25.896543  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:04:25.896543  8328 solver.cpp:237]     Train net output #1: loss = 0.320097 (* 1 = 0.320097 loss)
I1211 14:04:25.896543  8328 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 14:04:32.261658  8328 solver.cpp:218] Iteration 104800 (15.7116 iter/s, 6.36473s/100 iters), loss = 0.367107
I1211 14:04:32.261658  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:04:32.261658  8328 solver.cpp:237]     Train net output #1: loss = 0.367107 (* 1 = 0.367107 loss)
I1211 14:04:32.261658  8328 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 14:04:38.593484  8328 solver.cpp:218] Iteration 104900 (15.7943 iter/s, 6.3314s/100 iters), loss = 0.355189
I1211 14:04:38.593484  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:04:38.594486  8328 solver.cpp:237]     Train net output #1: loss = 0.355189 (* 1 = 0.355189 loss)
I1211 14:04:38.594486  8328 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 14:04:44.617220 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:04:44.868268  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.caffemodel
I1211 14:04:44.883777  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.solverstate
I1211 14:04:44.888779  8328 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 14:04:44.888779  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:04:46.406656 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:04:46.466656  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6781
I1211 14:04:46.466656  8328 solver.cpp:397]     Test net output #1: loss = 1.22594 (* 1 = 1.22594 loss)
I1211 14:04:46.527664  8328 solver.cpp:218] Iteration 105000 (12.6057 iter/s, 7.93292s/100 iters), loss = 0.305859
I1211 14:04:46.527664  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 14:04:46.527664  8328 solver.cpp:237]     Train net output #1: loss = 0.305859 (* 1 = 0.305859 loss)
I1211 14:04:46.527664  8328 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 14:04:52.865403  8328 solver.cpp:218] Iteration 105100 (15.7781 iter/s, 6.33789s/100 iters), loss = 0.416912
I1211 14:04:52.865403  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:04:52.865403  8328 solver.cpp:237]     Train net output #1: loss = 0.416912 (* 1 = 0.416912 loss)
I1211 14:04:52.865403  8328 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 14:04:59.209090  8328 solver.cpp:218] Iteration 105200 (15.7645 iter/s, 6.34335s/100 iters), loss = 0.279879
I1211 14:04:59.209090  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:04:59.209090  8328 solver.cpp:237]     Train net output #1: loss = 0.279879 (* 1 = 0.279879 loss)
I1211 14:04:59.209090  8328 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 14:05:05.549953  8328 solver.cpp:218] Iteration 105300 (15.7724 iter/s, 6.34019s/100 iters), loss = 0.395868
I1211 14:05:05.549953  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:05:05.549953  8328 solver.cpp:237]     Train net output #1: loss = 0.395868 (* 1 = 0.395868 loss)
I1211 14:05:05.549953  8328 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 14:05:11.897756  8328 solver.cpp:218] Iteration 105400 (15.7538 iter/s, 6.34767s/100 iters), loss = 0.370124
I1211 14:05:11.897756  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:05:11.897756  8328 solver.cpp:237]     Train net output #1: loss = 0.370124 (* 1 = 0.370124 loss)
I1211 14:05:11.897756  8328 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 14:05:17.951529 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:05:18.206558  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.caffemodel
I1211 14:05:18.222558  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.solverstate
I1211 14:05:18.227560  8328 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 14:05:18.227560  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:05:19.767624 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:05:19.827636  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6752
I1211 14:05:19.827636  8328 solver.cpp:397]     Test net output #1: loss = 1.22938 (* 1 = 1.22938 loss)
I1211 14:05:19.889139  8328 solver.cpp:218] Iteration 105500 (12.5154 iter/s, 7.99014s/100 iters), loss = 0.30408
I1211 14:05:19.889139  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 14:05:19.889139  8328 solver.cpp:237]     Train net output #1: loss = 0.30408 (* 1 = 0.30408 loss)
I1211 14:05:19.889139  8328 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 14:05:26.329918  8328 solver.cpp:218] Iteration 105600 (15.5254 iter/s, 6.44106s/100 iters), loss = 0.411266
I1211 14:05:26.329918  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:05:26.329918  8328 solver.cpp:237]     Train net output #1: loss = 0.411266 (* 1 = 0.411266 loss)
I1211 14:05:26.329918  8328 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 14:05:32.732626  8328 solver.cpp:218] Iteration 105700 (15.6213 iter/s, 6.4015s/100 iters), loss = 0.300739
I1211 14:05:32.732626  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:05:32.732626  8328 solver.cpp:237]     Train net output #1: loss = 0.300739 (* 1 = 0.300739 loss)
I1211 14:05:32.732626  8328 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 14:05:39.111974  8328 solver.cpp:218] Iteration 105800 (15.6766 iter/s, 6.37892s/100 iters), loss = 0.425139
I1211 14:05:39.111974  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:05:39.111974  8328 solver.cpp:237]     Train net output #1: loss = 0.425139 (* 1 = 0.425139 loss)
I1211 14:05:39.111974  8328 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 14:05:45.433629  8328 solver.cpp:218] Iteration 105900 (15.819 iter/s, 6.32153s/100 iters), loss = 0.365029
I1211 14:05:45.433629  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:05:45.433629  8328 solver.cpp:237]     Train net output #1: loss = 0.365029 (* 1 = 0.365029 loss)
I1211 14:05:45.433629  8328 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 14:05:51.450295 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:05:51.699484  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.caffemodel
I1211 14:05:51.715005  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.solverstate
I1211 14:05:51.719988  8328 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 14:05:51.719988  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:05:53.236332 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:05:53.295840  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 14:05:53.295840  8328 solver.cpp:397]     Test net output #1: loss = 1.23859 (* 1 = 1.23859 loss)
I1211 14:05:53.356935  8328 solver.cpp:218] Iteration 106000 (12.6222 iter/s, 7.92254s/100 iters), loss = 0.298407
I1211 14:05:53.356935  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 14:05:53.356935  8328 solver.cpp:237]     Train net output #1: loss = 0.298407 (* 1 = 0.298407 loss)
I1211 14:05:53.356935  8328 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 14:05:59.679919  8328 solver.cpp:218] Iteration 106100 (15.8162 iter/s, 6.32261s/100 iters), loss = 0.423694
I1211 14:05:59.679919  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:05:59.679919  8328 solver.cpp:237]     Train net output #1: loss = 0.423694 (* 1 = 0.423694 loss)
I1211 14:05:59.679919  8328 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 14:06:06.007331  8328 solver.cpp:218] Iteration 106200 (15.8052 iter/s, 6.32705s/100 iters), loss = 0.266752
I1211 14:06:06.007331  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:06:06.007331  8328 solver.cpp:237]     Train net output #1: loss = 0.266752 (* 1 = 0.266752 loss)
I1211 14:06:06.007331  8328 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 14:06:12.331888  8328 solver.cpp:218] Iteration 106300 (15.8122 iter/s, 6.32422s/100 iters), loss = 0.403876
I1211 14:06:12.331888  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:06:12.331888  8328 solver.cpp:237]     Train net output #1: loss = 0.403876 (* 1 = 0.403876 loss)
I1211 14:06:12.331888  8328 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 14:06:18.691185  8328 solver.cpp:218] Iteration 106400 (15.7266 iter/s, 6.35866s/100 iters), loss = 0.383061
I1211 14:06:18.691185  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:06:18.691185  8328 solver.cpp:237]     Train net output #1: loss = 0.383061 (* 1 = 0.383061 loss)
I1211 14:06:18.691185  8328 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 14:06:24.714612 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:06:24.966625  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.caffemodel
I1211 14:06:24.981626  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.solverstate
I1211 14:06:24.987131  8328 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 14:06:24.987131  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:06:26.501754 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:06:26.561753  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6779
I1211 14:06:26.561753  8328 solver.cpp:397]     Test net output #1: loss = 1.23409 (* 1 = 1.23409 loss)
I1211 14:06:26.622257  8328 solver.cpp:218] Iteration 106500 (12.609 iter/s, 7.93083s/100 iters), loss = 0.31207
I1211 14:06:26.622257  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:06:26.622257  8328 solver.cpp:237]     Train net output #1: loss = 0.31207 (* 1 = 0.31207 loss)
I1211 14:06:26.622257  8328 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 14:06:32.961149  8328 solver.cpp:218] Iteration 106600 (15.7754 iter/s, 6.33897s/100 iters), loss = 0.412228
I1211 14:06:32.961149  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:06:32.962151  8328 solver.cpp:237]     Train net output #1: loss = 0.412228 (* 1 = 0.412228 loss)
I1211 14:06:32.962151  8328 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 14:06:39.298530  8328 solver.cpp:218] Iteration 106700 (15.7819 iter/s, 6.33636s/100 iters), loss = 0.276204
I1211 14:06:39.298530  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:06:39.298530  8328 solver.cpp:237]     Train net output #1: loss = 0.276204 (* 1 = 0.276204 loss)
I1211 14:06:39.298530  8328 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 14:06:45.691568  8328 solver.cpp:218] Iteration 106800 (15.6438 iter/s, 6.39231s/100 iters), loss = 0.340439
I1211 14:06:45.691568  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:06:45.691568  8328 solver.cpp:237]     Train net output #1: loss = 0.340439 (* 1 = 0.340439 loss)
I1211 14:06:45.691568  8328 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 14:06:52.032543  8328 solver.cpp:218] Iteration 106900 (15.7693 iter/s, 6.34144s/100 iters), loss = 0.422308
I1211 14:06:52.032543  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 14:06:52.033545  8328 solver.cpp:237]     Train net output #1: loss = 0.422308 (* 1 = 0.422308 loss)
I1211 14:06:52.033545  8328 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 14:06:58.058413 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:06:58.308912  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.caffemodel
I1211 14:06:58.324412  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.solverstate
I1211 14:06:58.329413  8328 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 14:06:58.329413  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:06:59.851927 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:06:59.910928  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6778
I1211 14:06:59.910928  8328 solver.cpp:397]     Test net output #1: loss = 1.23552 (* 1 = 1.23552 loss)
I1211 14:06:59.971931  8328 solver.cpp:218] Iteration 107000 (12.5969 iter/s, 7.93849s/100 iters), loss = 0.330418
I1211 14:06:59.971931  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:06:59.971931  8328 solver.cpp:237]     Train net output #1: loss = 0.330418 (* 1 = 0.330418 loss)
I1211 14:06:59.971931  8328 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 14:07:06.303414  8328 solver.cpp:218] Iteration 107100 (15.795 iter/s, 6.3311s/100 iters), loss = 0.442331
I1211 14:07:06.303414  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:07:06.303414  8328 solver.cpp:237]     Train net output #1: loss = 0.442331 (* 1 = 0.442331 loss)
I1211 14:07:06.303414  8328 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 14:07:12.634887  8328 solver.cpp:218] Iteration 107200 (15.7949 iter/s, 6.33114s/100 iters), loss = 0.401184
I1211 14:07:12.634887  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:07:12.634887  8328 solver.cpp:237]     Train net output #1: loss = 0.401184 (* 1 = 0.401184 loss)
I1211 14:07:12.634887  8328 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 14:07:18.971350  8328 solver.cpp:218] Iteration 107300 (15.7843 iter/s, 6.33541s/100 iters), loss = 0.407816
I1211 14:07:18.971350  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:07:18.971350  8328 solver.cpp:237]     Train net output #1: loss = 0.407816 (* 1 = 0.407816 loss)
I1211 14:07:18.971350  8328 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 14:07:25.297675  8328 solver.cpp:218] Iteration 107400 (15.8072 iter/s, 6.32625s/100 iters), loss = 0.407258
I1211 14:07:25.297675  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:07:25.297675  8328 solver.cpp:237]     Train net output #1: loss = 0.407258 (* 1 = 0.407258 loss)
I1211 14:07:25.297675  8328 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 14:07:31.320132 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:07:31.570158  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.caffemodel
I1211 14:07:31.585160  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.solverstate
I1211 14:07:31.589160  8328 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 14:07:31.589160  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:07:33.105295 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:07:33.165797  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6785
I1211 14:07:33.165797  8328 solver.cpp:397]     Test net output #1: loss = 1.23519 (* 1 = 1.23519 loss)
I1211 14:07:33.226299  8328 solver.cpp:218] Iteration 107500 (12.6125 iter/s, 7.92867s/100 iters), loss = 0.370534
I1211 14:07:33.226299  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:07:33.226299  8328 solver.cpp:237]     Train net output #1: loss = 0.370534 (* 1 = 0.370534 loss)
I1211 14:07:33.226299  8328 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 14:07:39.558720  8328 solver.cpp:218] Iteration 107600 (15.7949 iter/s, 6.33118s/100 iters), loss = 0.398534
I1211 14:07:39.558720  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:07:39.558720  8328 solver.cpp:237]     Train net output #1: loss = 0.398534 (* 1 = 0.398534 loss)
I1211 14:07:39.558720  8328 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 14:07:45.890223  8328 solver.cpp:218] Iteration 107700 (15.7949 iter/s, 6.33117s/100 iters), loss = 0.35374
I1211 14:07:45.890223  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:07:45.890223  8328 solver.cpp:237]     Train net output #1: loss = 0.35374 (* 1 = 0.35374 loss)
I1211 14:07:45.890223  8328 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 14:07:52.216718  8328 solver.cpp:218] Iteration 107800 (15.8062 iter/s, 6.32663s/100 iters), loss = 0.337411
I1211 14:07:52.216718  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:07:52.216718  8328 solver.cpp:237]     Train net output #1: loss = 0.337411 (* 1 = 0.337411 loss)
I1211 14:07:52.216718  8328 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 14:07:58.545233  8328 solver.cpp:218] Iteration 107900 (15.8025 iter/s, 6.32811s/100 iters), loss = 0.385172
I1211 14:07:58.545233  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:07:58.545233  8328 solver.cpp:237]     Train net output #1: loss = 0.385172 (* 1 = 0.385172 loss)
I1211 14:07:58.545233  8328 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 14:08:04.569131 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:08:04.818640  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.caffemodel
I1211 14:08:04.834640  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.solverstate
I1211 14:08:04.838641  8328 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 14:08:04.838641  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:08:06.353739 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:08:06.413744  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6784
I1211 14:08:06.413744  8328 solver.cpp:397]     Test net output #1: loss = 1.23519 (* 1 = 1.23519 loss)
I1211 14:08:06.474745  8328 solver.cpp:218] Iteration 108000 (12.6131 iter/s, 7.92826s/100 iters), loss = 0.359738
I1211 14:08:06.474745  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 14:08:06.474745  8328 solver.cpp:237]     Train net output #1: loss = 0.359738 (* 1 = 0.359738 loss)
I1211 14:08:06.474745  8328 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 14:08:12.827172  8328 solver.cpp:218] Iteration 108100 (15.7413 iter/s, 6.35273s/100 iters), loss = 0.426549
I1211 14:08:12.827172  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:08:12.827172  8328 solver.cpp:237]     Train net output #1: loss = 0.426549 (* 1 = 0.426549 loss)
I1211 14:08:12.827172  8328 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 14:08:19.168603  8328 solver.cpp:218] Iteration 108200 (15.772 iter/s, 6.34035s/100 iters), loss = 0.327798
I1211 14:08:19.168603  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:08:19.168603  8328 solver.cpp:237]     Train net output #1: loss = 0.327798 (* 1 = 0.327798 loss)
I1211 14:08:19.168603  8328 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 14:08:25.511085  8328 solver.cpp:218] Iteration 108300 (15.7675 iter/s, 6.34214s/100 iters), loss = 0.337093
I1211 14:08:25.511085  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 14:08:25.511085  8328 solver.cpp:237]     Train net output #1: loss = 0.337093 (* 1 = 0.337093 loss)
I1211 14:08:25.511085  8328 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 14:08:31.853536  8328 solver.cpp:218] Iteration 108400 (15.7668 iter/s, 6.34244s/100 iters), loss = 0.326168
I1211 14:08:31.853536  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:08:31.853536  8328 solver.cpp:237]     Train net output #1: loss = 0.326168 (* 1 = 0.326168 loss)
I1211 14:08:31.853536  8328 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 14:08:37.893038 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:08:38.142062  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.caffemodel
I1211 14:08:38.157063  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.solverstate
I1211 14:08:38.163064  8328 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 14:08:38.163064  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:08:39.676693 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:08:39.736196  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6755
I1211 14:08:39.736196  8328 solver.cpp:397]     Test net output #1: loss = 1.24116 (* 1 = 1.24116 loss)
I1211 14:08:39.797201  8328 solver.cpp:218] Iteration 108500 (12.5906 iter/s, 7.94243s/100 iters), loss = 0.34456
I1211 14:08:39.797201  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:08:39.797201  8328 solver.cpp:237]     Train net output #1: loss = 0.34456 (* 1 = 0.34456 loss)
I1211 14:08:39.797201  8328 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 14:08:46.130616  8328 solver.cpp:218] Iteration 108600 (15.7902 iter/s, 6.33306s/100 iters), loss = 0.446192
I1211 14:08:46.130616  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:08:46.130616  8328 solver.cpp:237]     Train net output #1: loss = 0.446192 (* 1 = 0.446192 loss)
I1211 14:08:46.130616  8328 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 14:08:52.470582  8328 solver.cpp:218] Iteration 108700 (15.7744 iter/s, 6.33938s/100 iters), loss = 0.329303
I1211 14:08:52.470582  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:08:52.470582  8328 solver.cpp:237]     Train net output #1: loss = 0.329303 (* 1 = 0.329303 loss)
I1211 14:08:52.470582  8328 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 14:08:58.800498  8328 solver.cpp:218] Iteration 108800 (15.7987 iter/s, 6.32965s/100 iters), loss = 0.400152
I1211 14:08:58.800498  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:08:58.800498  8328 solver.cpp:237]     Train net output #1: loss = 0.400152 (* 1 = 0.400152 loss)
I1211 14:08:58.800498  8328 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 14:09:05.135010  8328 solver.cpp:218] Iteration 108900 (15.7878 iter/s, 6.33401s/100 iters), loss = 0.388961
I1211 14:09:05.135010  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 14:09:05.135010  8328 solver.cpp:237]     Train net output #1: loss = 0.388961 (* 1 = 0.388961 loss)
I1211 14:09:05.135010  8328 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 14:09:11.160445 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:09:11.410468  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.caffemodel
I1211 14:09:11.425467  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.solverstate
I1211 14:09:11.430469  8328 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 14:09:11.430469  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:09:12.948597 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:09:13.008601  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6782
I1211 14:09:13.008601  8328 solver.cpp:397]     Test net output #1: loss = 1.23552 (* 1 = 1.23552 loss)
I1211 14:09:13.069103  8328 solver.cpp:218] Iteration 109000 (12.6043 iter/s, 7.93383s/100 iters), loss = 0.280517
I1211 14:09:13.069103  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:09:13.069103  8328 solver.cpp:237]     Train net output #1: loss = 0.280517 (* 1 = 0.280517 loss)
I1211 14:09:13.069103  8328 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 14:09:19.412077  8328 solver.cpp:218] Iteration 109100 (15.7662 iter/s, 6.34267s/100 iters), loss = 0.373576
I1211 14:09:19.412077  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:09:19.412077  8328 solver.cpp:237]     Train net output #1: loss = 0.373576 (* 1 = 0.373576 loss)
I1211 14:09:19.412077  8328 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 14:09:25.763626  8328 solver.cpp:218] Iteration 109200 (15.744 iter/s, 6.35164s/100 iters), loss = 0.273839
I1211 14:09:25.764627  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 14:09:25.764627  8328 solver.cpp:237]     Train net output #1: loss = 0.273839 (* 1 = 0.273839 loss)
I1211 14:09:25.764627  8328 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 14:09:32.108100  8328 solver.cpp:218] Iteration 109300 (15.7653 iter/s, 6.34303s/100 iters), loss = 0.350978
I1211 14:09:32.108100  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:09:32.108100  8328 solver.cpp:237]     Train net output #1: loss = 0.350978 (* 1 = 0.350978 loss)
I1211 14:09:32.108100  8328 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 14:09:38.449563  8328 solver.cpp:218] Iteration 109400 (15.7707 iter/s, 6.34088s/100 iters), loss = 0.338136
I1211 14:09:38.449563  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 14:09:38.449563  8328 solver.cpp:237]     Train net output #1: loss = 0.338136 (* 1 = 0.338136 loss)
I1211 14:09:38.449563  8328 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 14:09:44.479001 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:09:44.728022  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.caffemodel
I1211 14:09:44.744024  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.solverstate
I1211 14:09:44.749023  8328 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 14:09:44.749023  8328 net.cpp:676] Ignoring source layer accuracy_training
I1211 14:09:46.265148 12664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:09:46.325152  8328 solver.cpp:397]     Test net output #0: accuracy = 0.6754
I1211 14:09:46.325152  8328 solver.cpp:397]     Test net output #1: loss = 1.24972 (* 1 = 1.24972 loss)
I1211 14:09:46.386157  8328 solver.cpp:218] Iteration 109500 (12.6 iter/s, 7.93653s/100 iters), loss = 0.304841
I1211 14:09:46.386157  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:09:46.386157  8328 solver.cpp:237]     Train net output #1: loss = 0.304841 (* 1 = 0.304841 loss)
I1211 14:09:46.386157  8328 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 14:09:52.717633  8328 solver.cpp:218] Iteration 109600 (15.7953 iter/s, 6.331s/100 iters), loss = 0.42185
I1211 14:09:52.717633  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 14:09:52.717633  8328 solver.cpp:237]     Train net output #1: loss = 0.42185 (* 1 = 0.42185 loss)
I1211 14:09:52.717633  8328 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 14:09:59.049110  8328 solver.cpp:218] Iteration 109700 (15.7944 iter/s, 6.33134s/100 iters), loss = 0.276298
I1211 14:09:59.049110  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 14:09:59.049110  8328 solver.cpp:237]     Train net output #1: loss = 0.276298 (* 1 = 0.276298 loss)
I1211 14:09:59.050112  8328 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 14:10:05.415962  8328 solver.cpp:218] Iteration 109800 (15.7086 iter/s, 6.36594s/100 iters), loss = 0.366853
I1211 14:10:05.416463  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 14:10:05.416463  8328 solver.cpp:237]     Train net output #1: loss = 0.366853 (* 1 = 0.366853 loss)
I1211 14:10:05.416463  8328 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 14:10:11.772289  8328 solver.cpp:218] Iteration 109900 (15.7339 iter/s, 6.3557s/100 iters), loss = 0.409405
I1211 14:10:11.772289  8328 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 14:10:11.772289  8328 solver.cpp:237]     Train net output #1: loss = 0.409405 (* 1 = 0.409405 loss)
I1211 14:10:11.772289  8328 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 14:10:17.875255 14664 data_layer.cpp:73] Restarting data prefetching from start.
I1211 14:10:18.126832  8328 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.caffemodel
I1211 14:10:18.143352  8328 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar
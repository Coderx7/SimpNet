
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate 
I1211 09:37:15.746110  5076 caffe.cpp:219] Using GPUs 0
I1211 09:37:15.930639  5076 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 09:37:16.233804  5076 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:37:16.249799  5076 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 09:37:16.250799  5076 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:37:16.251799  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:37:16.251799  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added1
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added2
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 09:37:16.251799  5076 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 09:37:16.251799  5076 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 09:37:16.252802  5076 layer_factory.cpp:58] Creating layer cifar
I1211 09:37:16.258796  5076 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 09:37:16.258796  5076 net.cpp:84] Creating Layer cifar
I1211 09:37:16.258796  5076 net.cpp:380] cifar -> data
I1211 09:37:16.258796  5076 net.cpp:380] cifar -> label
I1211 09:37:16.259795  5076 data_layer.cpp:45] output data size: 100,3,32,32
I1211 09:37:16.265800  5076 net.cpp:122] Setting up cifar
I1211 09:37:16.265800  5076 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 09:37:16.265800  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.265800  5076 net.cpp:137] Memory required for data: 1229200
I1211 09:37:16.265800  5076 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 09:37:16.265800  5076 net.cpp:84] Creating Layer label_cifar_1_split
I1211 09:37:16.265800  5076 net.cpp:406] label_cifar_1_split <- label
I1211 09:37:16.265800  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 09:37:16.265800  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 09:37:16.265800  5076 net.cpp:122] Setting up label_cifar_1_split
I1211 09:37:16.265800  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.265800  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.265800  5076 net.cpp:137] Memory required for data: 1230000
I1211 09:37:16.265800  5076 layer_factory.cpp:58] Creating layer conv1
I1211 09:37:16.265800  5076 net.cpp:84] Creating Layer conv1
I1211 09:37:16.265800  5076 net.cpp:406] conv1 <- data
I1211 09:37:16.265800  5076 net.cpp:380] conv1 -> conv1
I1211 09:37:16.266794 10680 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:37:16.504084  5076 net.cpp:122] Setting up conv1
I1211 09:37:16.504084  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.504084  5076 net.cpp:137] Memory required for data: 13518000
I1211 09:37:16.504084  5076 layer_factory.cpp:58] Creating layer bn1
I1211 09:37:16.504084  5076 net.cpp:84] Creating Layer bn1
I1211 09:37:16.504084  5076 net.cpp:406] bn1 <- conv1
I1211 09:37:16.504084  5076 net.cpp:367] bn1 -> conv1 (in-place)
I1211 09:37:16.504585  5076 net.cpp:122] Setting up bn1
I1211 09:37:16.504585  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.504585  5076 net.cpp:137] Memory required for data: 25806000
I1211 09:37:16.504585  5076 layer_factory.cpp:58] Creating layer scale1
I1211 09:37:16.504585  5076 net.cpp:84] Creating Layer scale1
I1211 09:37:16.504585  5076 net.cpp:406] scale1 <- conv1
I1211 09:37:16.504585  5076 net.cpp:367] scale1 -> conv1 (in-place)
I1211 09:37:16.504585  5076 layer_factory.cpp:58] Creating layer scale1
I1211 09:37:16.504585  5076 net.cpp:122] Setting up scale1
I1211 09:37:16.504585  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.504585  5076 net.cpp:137] Memory required for data: 38094000
I1211 09:37:16.504585  5076 layer_factory.cpp:58] Creating layer relu1
I1211 09:37:16.504585  5076 net.cpp:84] Creating Layer relu1
I1211 09:37:16.504585  5076 net.cpp:406] relu1 <- conv1
I1211 09:37:16.504585  5076 net.cpp:367] relu1 -> conv1 (in-place)
I1211 09:37:16.505084  5076 net.cpp:122] Setting up relu1
I1211 09:37:16.505084  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.505084  5076 net.cpp:137] Memory required for data: 50382000
I1211 09:37:16.505084  5076 layer_factory.cpp:58] Creating layer conv1_0
I1211 09:37:16.505084  5076 net.cpp:84] Creating Layer conv1_0
I1211 09:37:16.505084  5076 net.cpp:406] conv1_0 <- conv1
I1211 09:37:16.505084  5076 net.cpp:380] conv1_0 -> conv1_0
I1211 09:37:16.506585  5076 net.cpp:122] Setting up conv1_0
I1211 09:37:16.506585  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.506585  5076 net.cpp:137] Memory required for data: 66766000
I1211 09:37:16.506585  5076 layer_factory.cpp:58] Creating layer bn1_0
I1211 09:37:16.506585  5076 net.cpp:84] Creating Layer bn1_0
I1211 09:37:16.506585  5076 net.cpp:406] bn1_0 <- conv1_0
I1211 09:37:16.506585  5076 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 09:37:16.507084  5076 net.cpp:122] Setting up bn1_0
I1211 09:37:16.507084  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.507084  5076 net.cpp:137] Memory required for data: 83150000
I1211 09:37:16.507084  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:37:16.507084  5076 net.cpp:84] Creating Layer scale1_0
I1211 09:37:16.507084  5076 net.cpp:406] scale1_0 <- conv1_0
I1211 09:37:16.507084  5076 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 09:37:16.507084  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:37:16.507084  5076 net.cpp:122] Setting up scale1_0
I1211 09:37:16.507084  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.507084  5076 net.cpp:137] Memory required for data: 99534000
I1211 09:37:16.507084  5076 layer_factory.cpp:58] Creating layer relu1_0
I1211 09:37:16.507084  5076 net.cpp:84] Creating Layer relu1_0
I1211 09:37:16.507084  5076 net.cpp:406] relu1_0 <- conv1_0
I1211 09:37:16.507084  5076 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 09:37:16.507084  5076 net.cpp:122] Setting up relu1_0
I1211 09:37:16.507084  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.507084  5076 net.cpp:137] Memory required for data: 115918000
I1211 09:37:16.507084  5076 layer_factory.cpp:58] Creating layer conv2
I1211 09:37:16.507084  5076 net.cpp:84] Creating Layer conv2
I1211 09:37:16.507084  5076 net.cpp:406] conv2 <- conv1_0
I1211 09:37:16.507084  5076 net.cpp:380] conv2 -> conv2
I1211 09:37:16.508584  5076 net.cpp:122] Setting up conv2
I1211 09:37:16.508584  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.508584  5076 net.cpp:137] Memory required for data: 132302000
I1211 09:37:16.508584  5076 layer_factory.cpp:58] Creating layer bn2
I1211 09:37:16.508584  5076 net.cpp:84] Creating Layer bn2
I1211 09:37:16.508584  5076 net.cpp:406] bn2 <- conv2
I1211 09:37:16.508584  5076 net.cpp:367] bn2 -> conv2 (in-place)
I1211 09:37:16.508584  5076 net.cpp:122] Setting up bn2
I1211 09:37:16.508584  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.508584  5076 net.cpp:137] Memory required for data: 148686000
I1211 09:37:16.508584  5076 layer_factory.cpp:58] Creating layer scale2
I1211 09:37:16.508584  5076 net.cpp:84] Creating Layer scale2
I1211 09:37:16.508584  5076 net.cpp:406] scale2 <- conv2
I1211 09:37:16.508584  5076 net.cpp:367] scale2 -> conv2 (in-place)
I1211 09:37:16.508584  5076 layer_factory.cpp:58] Creating layer scale2
I1211 09:37:16.508584  5076 net.cpp:122] Setting up scale2
I1211 09:37:16.508584  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.508584  5076 net.cpp:137] Memory required for data: 165070000
I1211 09:37:16.508584  5076 layer_factory.cpp:58] Creating layer relu2
I1211 09:37:16.508584  5076 net.cpp:84] Creating Layer relu2
I1211 09:37:16.508584  5076 net.cpp:406] relu2 <- conv2
I1211 09:37:16.508584  5076 net.cpp:367] relu2 -> conv2 (in-place)
I1211 09:37:16.509083  5076 net.cpp:122] Setting up relu2
I1211 09:37:16.509083  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.509083  5076 net.cpp:137] Memory required for data: 181454000
I1211 09:37:16.509083  5076 layer_factory.cpp:58] Creating layer conv2_1
I1211 09:37:16.509083  5076 net.cpp:84] Creating Layer conv2_1
I1211 09:37:16.509083  5076 net.cpp:406] conv2_1 <- conv2
I1211 09:37:16.509083  5076 net.cpp:380] conv2_1 -> conv2_1
I1211 09:37:16.510084  5076 net.cpp:122] Setting up conv2_1
I1211 09:37:16.510084  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.510084  5076 net.cpp:137] Memory required for data: 197838000
I1211 09:37:16.510084  5076 layer_factory.cpp:58] Creating layer bn2_1
I1211 09:37:16.510084  5076 net.cpp:84] Creating Layer bn2_1
I1211 09:37:16.510084  5076 net.cpp:406] bn2_1 <- conv2_1
I1211 09:37:16.510084  5076 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 09:37:16.510084  5076 net.cpp:122] Setting up bn2_1
I1211 09:37:16.510084  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.510084  5076 net.cpp:137] Memory required for data: 214222000
I1211 09:37:16.510084  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:37:16.510084  5076 net.cpp:84] Creating Layer scale2_1
I1211 09:37:16.510084  5076 net.cpp:406] scale2_1 <- conv2_1
I1211 09:37:16.510084  5076 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 09:37:16.510584  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:37:16.510584  5076 net.cpp:122] Setting up scale2_1
I1211 09:37:16.510584  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.510584  5076 net.cpp:137] Memory required for data: 230606000
I1211 09:37:16.510584  5076 layer_factory.cpp:58] Creating layer relu2_1
I1211 09:37:16.510584  5076 net.cpp:84] Creating Layer relu2_1
I1211 09:37:16.510584  5076 net.cpp:406] relu2_1 <- conv2_1
I1211 09:37:16.510584  5076 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 09:37:16.510584  5076 net.cpp:122] Setting up relu2_1
I1211 09:37:16.510584  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.510584  5076 net.cpp:137] Memory required for data: 246990000
I1211 09:37:16.510584  5076 layer_factory.cpp:58] Creating layer conv2_2
I1211 09:37:16.510584  5076 net.cpp:84] Creating Layer conv2_2
I1211 09:37:16.510584  5076 net.cpp:406] conv2_2 <- conv2_1
I1211 09:37:16.510584  5076 net.cpp:380] conv2_2 -> conv2_2
I1211 09:37:16.512102  5076 net.cpp:122] Setting up conv2_2
I1211 09:37:16.512102  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.512102  5076 net.cpp:137] Memory required for data: 267470000
I1211 09:37:16.512102  5076 layer_factory.cpp:58] Creating layer bn2_2
I1211 09:37:16.512102  5076 net.cpp:84] Creating Layer bn2_2
I1211 09:37:16.512102  5076 net.cpp:406] bn2_2 <- conv2_2
I1211 09:37:16.512603  5076 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 09:37:16.512603  5076 net.cpp:122] Setting up bn2_2
I1211 09:37:16.512603  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.512603  5076 net.cpp:137] Memory required for data: 287950000
I1211 09:37:16.512603  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:37:16.512603  5076 net.cpp:84] Creating Layer scale2_2
I1211 09:37:16.512603  5076 net.cpp:406] scale2_2 <- conv2_2
I1211 09:37:16.512603  5076 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 09:37:16.512603  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:37:16.512603  5076 net.cpp:122] Setting up scale2_2
I1211 09:37:16.512603  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.512603  5076 net.cpp:137] Memory required for data: 308430000
I1211 09:37:16.512603  5076 layer_factory.cpp:58] Creating layer relu2_2
I1211 09:37:16.512603  5076 net.cpp:84] Creating Layer relu2_2
I1211 09:37:16.512603  5076 net.cpp:406] relu2_2 <- conv2_2
I1211 09:37:16.512603  5076 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 09:37:16.513103  5076 net.cpp:122] Setting up relu2_2
I1211 09:37:16.513103  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.513103  5076 net.cpp:137] Memory required for data: 328910000
I1211 09:37:16.513103  5076 layer_factory.cpp:58] Creating layer newconv_added1
I1211 09:37:16.513103  5076 net.cpp:84] Creating Layer newconv_added1
I1211 09:37:16.513103  5076 net.cpp:406] newconv_added1 <- conv2_2
I1211 09:37:16.513103  5076 net.cpp:380] newconv_added1 -> newconv_added1
I1211 09:37:16.514102  5076 net.cpp:122] Setting up newconv_added1
I1211 09:37:16.514102  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.514102  5076 net.cpp:137] Memory required for data: 349390000
I1211 09:37:16.514102  5076 layer_factory.cpp:58] Creating layer bn_added1
I1211 09:37:16.514102  5076 net.cpp:84] Creating Layer bn_added1
I1211 09:37:16.514102  5076 net.cpp:406] bn_added1 <- newconv_added1
I1211 09:37:16.514102  5076 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 09:37:16.514602  5076 net.cpp:122] Setting up bn_added1
I1211 09:37:16.514602  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.514602  5076 net.cpp:137] Memory required for data: 369870000
I1211 09:37:16.514602  5076 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:37:16.514602  5076 net.cpp:84] Creating Layer scale_added1
I1211 09:37:16.514602  5076 net.cpp:406] scale_added1 <- newconv_added1
I1211 09:37:16.514602  5076 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 09:37:16.514602  5076 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:37:16.514602  5076 net.cpp:122] Setting up scale_added1
I1211 09:37:16.514602  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.514602  5076 net.cpp:137] Memory required for data: 390350000
I1211 09:37:16.514602  5076 layer_factory.cpp:58] Creating layer relu_added1
I1211 09:37:16.514602  5076 net.cpp:84] Creating Layer relu_added1
I1211 09:37:16.514602  5076 net.cpp:406] relu_added1 <- newconv_added1
I1211 09:37:16.514602  5076 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 09:37:16.515092  5076 net.cpp:122] Setting up relu_added1
I1211 09:37:16.515092  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.515092  5076 net.cpp:137] Memory required for data: 410830000
I1211 09:37:16.515092  5076 layer_factory.cpp:58] Creating layer pool2_1
I1211 09:37:16.515092  5076 net.cpp:84] Creating Layer pool2_1
I1211 09:37:16.515092  5076 net.cpp:406] pool2_1 <- newconv_added1
I1211 09:37:16.515092  5076 net.cpp:380] pool2_1 -> pool2_1
I1211 09:37:16.515092  5076 net.cpp:122] Setting up pool2_1
I1211 09:37:16.515092  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.515092  5076 net.cpp:137] Memory required for data: 415950000
I1211 09:37:16.515602  5076 layer_factory.cpp:58] Creating layer conv3
I1211 09:37:16.515602  5076 net.cpp:84] Creating Layer conv3
I1211 09:37:16.515602  5076 net.cpp:406] conv3 <- pool2_1
I1211 09:37:16.515602  5076 net.cpp:380] conv3 -> conv3
I1211 09:37:16.516602  5076 net.cpp:122] Setting up conv3
I1211 09:37:16.516602  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.516602  5076 net.cpp:137] Memory required for data: 421070000
I1211 09:37:16.516602  5076 layer_factory.cpp:58] Creating layer bn3
I1211 09:37:16.516602  5076 net.cpp:84] Creating Layer bn3
I1211 09:37:16.516602  5076 net.cpp:406] bn3 <- conv3
I1211 09:37:16.516602  5076 net.cpp:367] bn3 -> conv3 (in-place)
I1211 09:37:16.517102  5076 net.cpp:122] Setting up bn3
I1211 09:37:16.517102  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.517102  5076 net.cpp:137] Memory required for data: 426190000
I1211 09:37:16.517102  5076 layer_factory.cpp:58] Creating layer scale3
I1211 09:37:16.517102  5076 net.cpp:84] Creating Layer scale3
I1211 09:37:16.517102  5076 net.cpp:406] scale3 <- conv3
I1211 09:37:16.517102  5076 net.cpp:367] scale3 -> conv3 (in-place)
I1211 09:37:16.517102  5076 layer_factory.cpp:58] Creating layer scale3
I1211 09:37:16.517102  5076 net.cpp:122] Setting up scale3
I1211 09:37:16.517102  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.517102  5076 net.cpp:137] Memory required for data: 431310000
I1211 09:37:16.517102  5076 layer_factory.cpp:58] Creating layer relu3
I1211 09:37:16.517102  5076 net.cpp:84] Creating Layer relu3
I1211 09:37:16.517102  5076 net.cpp:406] relu3 <- conv3
I1211 09:37:16.517102  5076 net.cpp:367] relu3 -> conv3 (in-place)
I1211 09:37:16.517602  5076 net.cpp:122] Setting up relu3
I1211 09:37:16.517602  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.517602  5076 net.cpp:137] Memory required for data: 436430000
I1211 09:37:16.517602  5076 layer_factory.cpp:58] Creating layer conv3_1
I1211 09:37:16.517602  5076 net.cpp:84] Creating Layer conv3_1
I1211 09:37:16.517602  5076 net.cpp:406] conv3_1 <- conv3
I1211 09:37:16.517602  5076 net.cpp:380] conv3_1 -> conv3_1
I1211 09:37:16.518602  5076 net.cpp:122] Setting up conv3_1
I1211 09:37:16.518602  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.518602  5076 net.cpp:137] Memory required for data: 441550000
I1211 09:37:16.518602  5076 layer_factory.cpp:58] Creating layer bn3_1
I1211 09:37:16.518602  5076 net.cpp:84] Creating Layer bn3_1
I1211 09:37:16.518602  5076 net.cpp:406] bn3_1 <- conv3_1
I1211 09:37:16.518602  5076 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 09:37:16.519104  5076 net.cpp:122] Setting up bn3_1
I1211 09:37:16.519104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.519104  5076 net.cpp:137] Memory required for data: 446670000
I1211 09:37:16.519104  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:37:16.519104  5076 net.cpp:84] Creating Layer scale3_1
I1211 09:37:16.519104  5076 net.cpp:406] scale3_1 <- conv3_1
I1211 09:37:16.519104  5076 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 09:37:16.519104  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:37:16.519104  5076 net.cpp:122] Setting up scale3_1
I1211 09:37:16.519104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.519104  5076 net.cpp:137] Memory required for data: 451790000
I1211 09:37:16.519104  5076 layer_factory.cpp:58] Creating layer relu3_1
I1211 09:37:16.519104  5076 net.cpp:84] Creating Layer relu3_1
I1211 09:37:16.519104  5076 net.cpp:406] relu3_1 <- conv3_1
I1211 09:37:16.519104  5076 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 09:37:16.519104  5076 net.cpp:122] Setting up relu3_1
I1211 09:37:16.519104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.519104  5076 net.cpp:137] Memory required for data: 456910000
I1211 09:37:16.519104  5076 layer_factory.cpp:58] Creating layer conv4
I1211 09:37:16.519104  5076 net.cpp:84] Creating Layer conv4
I1211 09:37:16.519104  5076 net.cpp:406] conv4 <- conv3_1
I1211 09:37:16.519104  5076 net.cpp:380] conv4 -> conv4
I1211 09:37:16.520123  5076 net.cpp:122] Setting up conv4
I1211 09:37:16.520123  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.520123  5076 net.cpp:137] Memory required for data: 462030000
I1211 09:37:16.520123  5076 layer_factory.cpp:58] Creating layer bn4
I1211 09:37:16.520123  5076 net.cpp:84] Creating Layer bn4
I1211 09:37:16.520123  5076 net.cpp:406] bn4 <- conv4
I1211 09:37:16.520123  5076 net.cpp:367] bn4 -> conv4 (in-place)
I1211 09:37:16.520123  5076 net.cpp:122] Setting up bn4
I1211 09:37:16.520123  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.520123  5076 net.cpp:137] Memory required for data: 467150000
I1211 09:37:16.520123  5076 layer_factory.cpp:58] Creating layer scale4
I1211 09:37:16.520123  5076 net.cpp:84] Creating Layer scale4
I1211 09:37:16.520123  5076 net.cpp:406] scale4 <- conv4
I1211 09:37:16.520123  5076 net.cpp:367] scale4 -> conv4 (in-place)
I1211 09:37:16.520123  5076 layer_factory.cpp:58] Creating layer scale4
I1211 09:37:16.521122  5076 net.cpp:122] Setting up scale4
I1211 09:37:16.521122  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.521122  5076 net.cpp:137] Memory required for data: 472270000
I1211 09:37:16.521122  5076 layer_factory.cpp:58] Creating layer relu4
I1211 09:37:16.521122  5076 net.cpp:84] Creating Layer relu4
I1211 09:37:16.521122  5076 net.cpp:406] relu4 <- conv4
I1211 09:37:16.521122  5076 net.cpp:367] relu4 -> conv4 (in-place)
I1211 09:37:16.521122  5076 net.cpp:122] Setting up relu4
I1211 09:37:16.521122  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.521122  5076 net.cpp:137] Memory required for data: 477390000
I1211 09:37:16.521122  5076 layer_factory.cpp:58] Creating layer conv4_1
I1211 09:37:16.521122  5076 net.cpp:84] Creating Layer conv4_1
I1211 09:37:16.521122  5076 net.cpp:406] conv4_1 <- conv4
I1211 09:37:16.521122  5076 net.cpp:380] conv4_1 -> conv4_1
I1211 09:37:16.522122  5076 net.cpp:122] Setting up conv4_1
I1211 09:37:16.522122  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.522122  5076 net.cpp:137] Memory required for data: 482510000
I1211 09:37:16.522122  5076 layer_factory.cpp:58] Creating layer bn4_1
I1211 09:37:16.522122  5076 net.cpp:84] Creating Layer bn4_1
I1211 09:37:16.522122  5076 net.cpp:406] bn4_1 <- conv4_1
I1211 09:37:16.522122  5076 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 09:37:16.522122  5076 net.cpp:122] Setting up bn4_1
I1211 09:37:16.522122  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.522122  5076 net.cpp:137] Memory required for data: 487630000
I1211 09:37:16.522122  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:37:16.522122  5076 net.cpp:84] Creating Layer scale4_1
I1211 09:37:16.522122  5076 net.cpp:406] scale4_1 <- conv4_1
I1211 09:37:16.522122  5076 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 09:37:16.522122  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:37:16.523123  5076 net.cpp:122] Setting up scale4_1
I1211 09:37:16.523123  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.523123  5076 net.cpp:137] Memory required for data: 492750000
I1211 09:37:16.523123  5076 layer_factory.cpp:58] Creating layer relu4_1
I1211 09:37:16.523123  5076 net.cpp:84] Creating Layer relu4_1
I1211 09:37:16.523123  5076 net.cpp:406] relu4_1 <- conv4_1
I1211 09:37:16.523123  5076 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 09:37:16.523123  5076 net.cpp:122] Setting up relu4_1
I1211 09:37:16.523123  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.523123  5076 net.cpp:137] Memory required for data: 497870000
I1211 09:37:16.523123  5076 layer_factory.cpp:58] Creating layer conv4_2
I1211 09:37:16.523123  5076 net.cpp:84] Creating Layer conv4_2
I1211 09:37:16.523123  5076 net.cpp:406] conv4_2 <- conv4_1
I1211 09:37:16.523123  5076 net.cpp:380] conv4_2 -> conv4_2
I1211 09:37:16.524122  5076 net.cpp:122] Setting up conv4_2
I1211 09:37:16.524122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.524122  5076 net.cpp:137] Memory required for data: 503809200
I1211 09:37:16.524122  5076 layer_factory.cpp:58] Creating layer bn4_2
I1211 09:37:16.524122  5076 net.cpp:84] Creating Layer bn4_2
I1211 09:37:16.524122  5076 net.cpp:406] bn4_2 <- conv4_2
I1211 09:37:16.524122  5076 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 09:37:16.524122  5076 net.cpp:122] Setting up bn4_2
I1211 09:37:16.524122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.524122  5076 net.cpp:137] Memory required for data: 509748400
I1211 09:37:16.524122  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:37:16.524122  5076 net.cpp:84] Creating Layer scale4_2
I1211 09:37:16.524122  5076 net.cpp:406] scale4_2 <- conv4_2
I1211 09:37:16.524122  5076 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 09:37:16.524122  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:37:16.524122  5076 net.cpp:122] Setting up scale4_2
I1211 09:37:16.524122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.524122  5076 net.cpp:137] Memory required for data: 515687600
I1211 09:37:16.524122  5076 layer_factory.cpp:58] Creating layer relu4_2
I1211 09:37:16.524122  5076 net.cpp:84] Creating Layer relu4_2
I1211 09:37:16.524122  5076 net.cpp:406] relu4_2 <- conv4_2
I1211 09:37:16.524122  5076 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 09:37:16.525122  5076 net.cpp:122] Setting up relu4_2
I1211 09:37:16.525122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.525122  5076 net.cpp:137] Memory required for data: 521626800
I1211 09:37:16.525122  5076 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 09:37:16.525122  5076 net.cpp:84] Creating Layer added_new_conv2
I1211 09:37:16.525122  5076 net.cpp:406] added_new_conv2 <- conv4_2
I1211 09:37:16.525122  5076 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 09:37:16.526123  5076 net.cpp:122] Setting up added_new_conv2
I1211 09:37:16.526123  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.526123  5076 net.cpp:137] Memory required for data: 527566000
I1211 09:37:16.526123  5076 layer_factory.cpp:58] Creating layer bn_added2
I1211 09:37:16.526123  5076 net.cpp:84] Creating Layer bn_added2
I1211 09:37:16.526123  5076 net.cpp:406] bn_added2 <- added_new_conv2
I1211 09:37:16.526123  5076 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.526123  5076 net.cpp:122] Setting up bn_added2
I1211 09:37:16.526123  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.526123  5076 net.cpp:137] Memory required for data: 533505200
I1211 09:37:16.526123  5076 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:37:16.526123  5076 net.cpp:84] Creating Layer scale_added2
I1211 09:37:16.526123  5076 net.cpp:406] scale_added2 <- added_new_conv2
I1211 09:37:16.526123  5076 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.526123  5076 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:37:16.526123  5076 net.cpp:122] Setting up scale_added2
I1211 09:37:16.527122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.527122  5076 net.cpp:137] Memory required for data: 539444400
I1211 09:37:16.527122  5076 layer_factory.cpp:58] Creating layer relu_added2
I1211 09:37:16.527122  5076 net.cpp:84] Creating Layer relu_added2
I1211 09:37:16.527122  5076 net.cpp:406] relu_added2 <- added_new_conv2
I1211 09:37:16.527122  5076 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.527122  5076 net.cpp:122] Setting up relu_added2
I1211 09:37:16.527122  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.527122  5076 net.cpp:137] Memory required for data: 545383600
I1211 09:37:16.527122  5076 layer_factory.cpp:58] Creating layer pool4_2
I1211 09:37:16.527122  5076 net.cpp:84] Creating Layer pool4_2
I1211 09:37:16.527122  5076 net.cpp:406] pool4_2 <- added_new_conv2
I1211 09:37:16.527122  5076 net.cpp:380] pool4_2 -> pool4_2
I1211 09:37:16.527122  5076 net.cpp:122] Setting up pool4_2
I1211 09:37:16.527122  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.527122  5076 net.cpp:137] Memory required for data: 546868400
I1211 09:37:16.527122  5076 layer_factory.cpp:58] Creating layer conv4_0
I1211 09:37:16.527122  5076 net.cpp:84] Creating Layer conv4_0
I1211 09:37:16.527122  5076 net.cpp:406] conv4_0 <- pool4_2
I1211 09:37:16.527122  5076 net.cpp:380] conv4_0 -> conv4_0
I1211 09:37:16.528122  5076 net.cpp:122] Setting up conv4_0
I1211 09:37:16.528122  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.528122  5076 net.cpp:137] Memory required for data: 548353200
I1211 09:37:16.528122  5076 layer_factory.cpp:58] Creating layer bn4_0
I1211 09:37:16.528122  5076 net.cpp:84] Creating Layer bn4_0
I1211 09:37:16.528122  5076 net.cpp:406] bn4_0 <- conv4_0
I1211 09:37:16.528122  5076 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 09:37:16.529124  5076 net.cpp:122] Setting up bn4_0
I1211 09:37:16.529124  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.529124  5076 net.cpp:137] Memory required for data: 549838000
I1211 09:37:16.529124  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:37:16.529124  5076 net.cpp:84] Creating Layer scale4_0
I1211 09:37:16.529124  5076 net.cpp:406] scale4_0 <- conv4_0
I1211 09:37:16.529124  5076 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 09:37:16.529124  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:37:16.529124  5076 net.cpp:122] Setting up scale4_0
I1211 09:37:16.529124  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.529124  5076 net.cpp:137] Memory required for data: 551322800
I1211 09:37:16.529124  5076 layer_factory.cpp:58] Creating layer relu4_0
I1211 09:37:16.529124  5076 net.cpp:84] Creating Layer relu4_0
I1211 09:37:16.529124  5076 net.cpp:406] relu4_0 <- conv4_0
I1211 09:37:16.529124  5076 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 09:37:16.529124  5076 net.cpp:122] Setting up relu4_0
I1211 09:37:16.529124  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.529124  5076 net.cpp:137] Memory required for data: 552807600
I1211 09:37:16.529124  5076 layer_factory.cpp:58] Creating layer conv11
I1211 09:37:16.529124  5076 net.cpp:84] Creating Layer conv11
I1211 09:37:16.529124  5076 net.cpp:406] conv11 <- conv4_0
I1211 09:37:16.529124  5076 net.cpp:380] conv11 -> conv11
I1211 09:37:16.530122  5076 net.cpp:122] Setting up conv11
I1211 09:37:16.531122  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.531122  5076 net.cpp:137] Memory required for data: 554599600
I1211 09:37:16.531122  5076 layer_factory.cpp:58] Creating layer bn_conv11
I1211 09:37:16.531122  5076 net.cpp:84] Creating Layer bn_conv11
I1211 09:37:16.531122  5076 net.cpp:406] bn_conv11 <- conv11
I1211 09:37:16.531122  5076 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 09:37:16.531122  5076 net.cpp:122] Setting up bn_conv11
I1211 09:37:16.531122  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.531122  5076 net.cpp:137] Memory required for data: 556391600
I1211 09:37:16.531122  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:37:16.531122  5076 net.cpp:84] Creating Layer scale_conv11
I1211 09:37:16.531122  5076 net.cpp:406] scale_conv11 <- conv11
I1211 09:37:16.531122  5076 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 09:37:16.531122  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:37:16.531122  5076 net.cpp:122] Setting up scale_conv11
I1211 09:37:16.531122  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.531122  5076 net.cpp:137] Memory required for data: 558183600
I1211 09:37:16.531122  5076 layer_factory.cpp:58] Creating layer relu_conv11
I1211 09:37:16.531122  5076 net.cpp:84] Creating Layer relu_conv11
I1211 09:37:16.531122  5076 net.cpp:406] relu_conv11 <- conv11
I1211 09:37:16.531122  5076 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 09:37:16.532132  5076 net.cpp:122] Setting up relu_conv11
I1211 09:37:16.532132  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.532132  5076 net.cpp:137] Memory required for data: 559975600
I1211 09:37:16.532132  5076 layer_factory.cpp:58] Creating layer conv12
I1211 09:37:16.532132  5076 net.cpp:84] Creating Layer conv12
I1211 09:37:16.532132  5076 net.cpp:406] conv12 <- conv11
I1211 09:37:16.532132  5076 net.cpp:380] conv12 -> conv12
I1211 09:37:16.533107  5076 net.cpp:122] Setting up conv12
I1211 09:37:16.533107  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.533107  5076 net.cpp:137] Memory required for data: 562279600
I1211 09:37:16.533107  5076 layer_factory.cpp:58] Creating layer bn_conv12
I1211 09:37:16.533107  5076 net.cpp:84] Creating Layer bn_conv12
I1211 09:37:16.533107  5076 net.cpp:406] bn_conv12 <- conv12
I1211 09:37:16.533107  5076 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 09:37:16.534104  5076 net.cpp:122] Setting up bn_conv12
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 564583600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer scale_conv12
I1211 09:37:16.534104  5076 net.cpp:406] scale_conv12 <- conv12
I1211 09:37:16.534104  5076 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:37:16.534104  5076 net.cpp:122] Setting up scale_conv12
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 566887600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer relu_conv12
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer relu_conv12
I1211 09:37:16.534104  5076 net.cpp:406] relu_conv12 <- conv12
I1211 09:37:16.534104  5076 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 09:37:16.534104  5076 net.cpp:122] Setting up relu_conv12
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 569191600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer poolcp6
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer poolcp6
I1211 09:37:16.534104  5076 net.cpp:406] poolcp6 <- conv12
I1211 09:37:16.534104  5076 net.cpp:380] poolcp6 -> poolcp6
I1211 09:37:16.534104  5076 net.cpp:122] Setting up poolcp6
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 569227600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer ip1
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer ip1
I1211 09:37:16.534104  5076 net.cpp:406] ip1 <- poolcp6
I1211 09:37:16.534104  5076 net.cpp:380] ip1 -> ip1
I1211 09:37:16.534104  5076 net.cpp:122] Setting up ip1
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 569267600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 09:37:16.534104  5076 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 09:37:16.534104  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 09:37:16.534104  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 09:37:16.534104  5076 net.cpp:122] Setting up ip1_ip1_0_split
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 569347600
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer accuracy_training
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer accuracy_training
I1211 09:37:16.534104  5076 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 09:37:16.534104  5076 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 09:37:16.534104  5076 net.cpp:380] accuracy_training -> accuracy_training
I1211 09:37:16.534104  5076 net.cpp:122] Setting up accuracy_training
I1211 09:37:16.534104  5076 net.cpp:129] Top shape: (1)
I1211 09:37:16.534104  5076 net.cpp:137] Memory required for data: 569347604
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer loss
I1211 09:37:16.534104  5076 net.cpp:84] Creating Layer loss
I1211 09:37:16.534104  5076 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 09:37:16.534104  5076 net.cpp:406] loss <- label_cifar_1_split_1
I1211 09:37:16.534104  5076 net.cpp:380] loss -> loss
I1211 09:37:16.534104  5076 layer_factory.cpp:58] Creating layer loss
I1211 09:37:16.535104  5076 net.cpp:122] Setting up loss
I1211 09:37:16.535104  5076 net.cpp:129] Top shape: (1)
I1211 09:37:16.535104  5076 net.cpp:132]     with loss weight 1
I1211 09:37:16.535104  5076 net.cpp:137] Memory required for data: 569347608
I1211 09:37:16.535104  5076 net.cpp:198] loss needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:200] accuracy_training does not need backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] ip1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] poolcp6 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu_conv12 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale_conv12 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn_conv12 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv12 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu_conv11 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale_conv11 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn_conv11 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv11 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu4_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale4_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn4_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv4_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] pool4_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu_added2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale_added2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn_added2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] added_new_conv2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu4_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale4_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn4_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv4_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu4_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale4_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn4_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv4_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu4 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale4 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn4 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv4 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu3_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale3_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn3_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv3_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu3 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale3 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn3 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv3 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] pool2_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu_added1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale_added1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn_added1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] newconv_added1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu2_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale2_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn2_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv2_2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu2_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale2_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn2_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv2_1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv2 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu1_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale1_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn1_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv1_0 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] relu1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] scale1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] bn1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:198] conv1 needs backward computation.
I1211 09:37:16.535104  5076 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 09:37:16.536104  5076 net.cpp:200] cifar does not need backward computation.
I1211 09:37:16.536104  5076 net.cpp:242] This network produces output accuracy_training
I1211 09:37:16.536104  5076 net.cpp:242] This network produces output loss
I1211 09:37:16.536104  5076 net.cpp:255] Network initialization done.
I1211 09:37:16.536104  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:37:16.536104  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:37:16.536104  5076 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added1
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added2
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 09:37:16.537104  5076 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 09:37:16.537104  5076 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 09:37:16.537104  5076 layer_factory.cpp:58] Creating layer cifar
I1211 09:37:16.540122  5076 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 09:37:16.540122  5076 net.cpp:84] Creating Layer cifar
I1211 09:37:16.540122  5076 net.cpp:380] cifar -> data
I1211 09:37:16.540122  5076 net.cpp:380] cifar -> label
I1211 09:37:16.540122  5076 data_layer.cpp:45] output data size: 100,3,32,32
I1211 09:37:16.550123  5076 net.cpp:122] Setting up cifar
I1211 09:37:16.550123  5076 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 09:37:16.550123  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.550123  5076 net.cpp:137] Memory required for data: 1229200
I1211 09:37:16.551105  5076 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 09:37:16.551105  5076 net.cpp:84] Creating Layer label_cifar_1_split
I1211 09:37:16.551105  5076 net.cpp:406] label_cifar_1_split <- label
I1211 09:37:16.551105  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 09:37:16.551105  5076 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 09:37:16.551105  5076 net.cpp:122] Setting up label_cifar_1_split
I1211 09:37:16.551105  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.551105  5076 net.cpp:129] Top shape: 100 (100)
I1211 09:37:16.551105  5076 net.cpp:137] Memory required for data: 1230000
I1211 09:37:16.551105  5076 layer_factory.cpp:58] Creating layer conv1
I1211 09:37:16.551105  5076 net.cpp:84] Creating Layer conv1
I1211 09:37:16.551105  5076 net.cpp:406] conv1 <- data
I1211 09:37:16.551105  5076 net.cpp:380] conv1 -> conv1
I1211 09:37:16.552105  6380 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:37:16.552105  5076 net.cpp:122] Setting up conv1
I1211 09:37:16.552105  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.552105  5076 net.cpp:137] Memory required for data: 13518000
I1211 09:37:16.552105  5076 layer_factory.cpp:58] Creating layer bn1
I1211 09:37:16.552105  5076 net.cpp:84] Creating Layer bn1
I1211 09:37:16.552105  5076 net.cpp:406] bn1 <- conv1
I1211 09:37:16.552105  5076 net.cpp:367] bn1 -> conv1 (in-place)
I1211 09:37:16.553119  5076 net.cpp:122] Setting up bn1
I1211 09:37:16.553119  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.553119  5076 net.cpp:137] Memory required for data: 25806000
I1211 09:37:16.553119  5076 layer_factory.cpp:58] Creating layer scale1
I1211 09:37:16.553119  5076 net.cpp:84] Creating Layer scale1
I1211 09:37:16.553119  5076 net.cpp:406] scale1 <- conv1
I1211 09:37:16.553119  5076 net.cpp:367] scale1 -> conv1 (in-place)
I1211 09:37:16.553119  5076 layer_factory.cpp:58] Creating layer scale1
I1211 09:37:16.553119  5076 net.cpp:122] Setting up scale1
I1211 09:37:16.553119  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.553119  5076 net.cpp:137] Memory required for data: 38094000
I1211 09:37:16.553119  5076 layer_factory.cpp:58] Creating layer relu1
I1211 09:37:16.553119  5076 net.cpp:84] Creating Layer relu1
I1211 09:37:16.553119  5076 net.cpp:406] relu1 <- conv1
I1211 09:37:16.553119  5076 net.cpp:367] relu1 -> conv1 (in-place)
I1211 09:37:16.554107  5076 net.cpp:122] Setting up relu1
I1211 09:37:16.554107  5076 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:37:16.554107  5076 net.cpp:137] Memory required for data: 50382000
I1211 09:37:16.554107  5076 layer_factory.cpp:58] Creating layer conv1_0
I1211 09:37:16.554107  5076 net.cpp:84] Creating Layer conv1_0
I1211 09:37:16.554107  5076 net.cpp:406] conv1_0 <- conv1
I1211 09:37:16.554107  5076 net.cpp:380] conv1_0 -> conv1_0
I1211 09:37:16.555119  5076 net.cpp:122] Setting up conv1_0
I1211 09:37:16.555119  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.555119  5076 net.cpp:137] Memory required for data: 66766000
I1211 09:37:16.555119  5076 layer_factory.cpp:58] Creating layer bn1_0
I1211 09:37:16.555119  5076 net.cpp:84] Creating Layer bn1_0
I1211 09:37:16.555119  5076 net.cpp:406] bn1_0 <- conv1_0
I1211 09:37:16.555119  5076 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 09:37:16.555119  5076 net.cpp:122] Setting up bn1_0
I1211 09:37:16.555119  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.555119  5076 net.cpp:137] Memory required for data: 83150000
I1211 09:37:16.555119  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:37:16.555119  5076 net.cpp:84] Creating Layer scale1_0
I1211 09:37:16.555119  5076 net.cpp:406] scale1_0 <- conv1_0
I1211 09:37:16.555119  5076 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 09:37:16.555119  5076 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:37:16.555119  5076 net.cpp:122] Setting up scale1_0
I1211 09:37:16.555119  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.555119  5076 net.cpp:137] Memory required for data: 99534000
I1211 09:37:16.556118  5076 layer_factory.cpp:58] Creating layer relu1_0
I1211 09:37:16.556118  5076 net.cpp:84] Creating Layer relu1_0
I1211 09:37:16.556118  5076 net.cpp:406] relu1_0 <- conv1_0
I1211 09:37:16.556118  5076 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 09:37:16.556118  5076 net.cpp:122] Setting up relu1_0
I1211 09:37:16.556118  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.556118  5076 net.cpp:137] Memory required for data: 115918000
I1211 09:37:16.556118  5076 layer_factory.cpp:58] Creating layer conv2
I1211 09:37:16.556118  5076 net.cpp:84] Creating Layer conv2
I1211 09:37:16.556118  5076 net.cpp:406] conv2 <- conv1_0
I1211 09:37:16.556118  5076 net.cpp:380] conv2 -> conv2
I1211 09:37:16.557114  5076 net.cpp:122] Setting up conv2
I1211 09:37:16.557114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.557114  5076 net.cpp:137] Memory required for data: 132302000
I1211 09:37:16.557114  5076 layer_factory.cpp:58] Creating layer bn2
I1211 09:37:16.557114  5076 net.cpp:84] Creating Layer bn2
I1211 09:37:16.557114  5076 net.cpp:406] bn2 <- conv2
I1211 09:37:16.557114  5076 net.cpp:367] bn2 -> conv2 (in-place)
I1211 09:37:16.557114  5076 net.cpp:122] Setting up bn2
I1211 09:37:16.557114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.557114  5076 net.cpp:137] Memory required for data: 148686000
I1211 09:37:16.557114  5076 layer_factory.cpp:58] Creating layer scale2
I1211 09:37:16.557114  5076 net.cpp:84] Creating Layer scale2
I1211 09:37:16.557114  5076 net.cpp:406] scale2 <- conv2
I1211 09:37:16.557114  5076 net.cpp:367] scale2 -> conv2 (in-place)
I1211 09:37:16.558115  5076 layer_factory.cpp:58] Creating layer scale2
I1211 09:37:16.558115  5076 net.cpp:122] Setting up scale2
I1211 09:37:16.558115  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.558115  5076 net.cpp:137] Memory required for data: 165070000
I1211 09:37:16.558115  5076 layer_factory.cpp:58] Creating layer relu2
I1211 09:37:16.558115  5076 net.cpp:84] Creating Layer relu2
I1211 09:37:16.558115  5076 net.cpp:406] relu2 <- conv2
I1211 09:37:16.558115  5076 net.cpp:367] relu2 -> conv2 (in-place)
I1211 09:37:16.558115  5076 net.cpp:122] Setting up relu2
I1211 09:37:16.558115  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.558115  5076 net.cpp:137] Memory required for data: 181454000
I1211 09:37:16.558115  5076 layer_factory.cpp:58] Creating layer conv2_1
I1211 09:37:16.558115  5076 net.cpp:84] Creating Layer conv2_1
I1211 09:37:16.558115  5076 net.cpp:406] conv2_1 <- conv2
I1211 09:37:16.558115  5076 net.cpp:380] conv2_1 -> conv2_1
I1211 09:37:16.559114  5076 net.cpp:122] Setting up conv2_1
I1211 09:37:16.559114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.559114  5076 net.cpp:137] Memory required for data: 197838000
I1211 09:37:16.559114  5076 layer_factory.cpp:58] Creating layer bn2_1
I1211 09:37:16.559114  5076 net.cpp:84] Creating Layer bn2_1
I1211 09:37:16.559114  5076 net.cpp:406] bn2_1 <- conv2_1
I1211 09:37:16.559114  5076 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 09:37:16.560114  5076 net.cpp:122] Setting up bn2_1
I1211 09:37:16.560114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.560114  5076 net.cpp:137] Memory required for data: 214222000
I1211 09:37:16.560114  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:37:16.560114  5076 net.cpp:84] Creating Layer scale2_1
I1211 09:37:16.560114  5076 net.cpp:406] scale2_1 <- conv2_1
I1211 09:37:16.560114  5076 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 09:37:16.560114  5076 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:37:16.560114  5076 net.cpp:122] Setting up scale2_1
I1211 09:37:16.560114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.560114  5076 net.cpp:137] Memory required for data: 230606000
I1211 09:37:16.560114  5076 layer_factory.cpp:58] Creating layer relu2_1
I1211 09:37:16.560114  5076 net.cpp:84] Creating Layer relu2_1
I1211 09:37:16.560114  5076 net.cpp:406] relu2_1 <- conv2_1
I1211 09:37:16.560114  5076 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 09:37:16.560114  5076 net.cpp:122] Setting up relu2_1
I1211 09:37:16.560114  5076 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:37:16.560114  5076 net.cpp:137] Memory required for data: 246990000
I1211 09:37:16.560114  5076 layer_factory.cpp:58] Creating layer conv2_2
I1211 09:37:16.560114  5076 net.cpp:84] Creating Layer conv2_2
I1211 09:37:16.560114  5076 net.cpp:406] conv2_2 <- conv2_1
I1211 09:37:16.560114  5076 net.cpp:380] conv2_2 -> conv2_2
I1211 09:37:16.562104  5076 net.cpp:122] Setting up conv2_2
I1211 09:37:16.562104  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.562104  5076 net.cpp:137] Memory required for data: 267470000
I1211 09:37:16.562104  5076 layer_factory.cpp:58] Creating layer bn2_2
I1211 09:37:16.562104  5076 net.cpp:84] Creating Layer bn2_2
I1211 09:37:16.562104  5076 net.cpp:406] bn2_2 <- conv2_2
I1211 09:37:16.562104  5076 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 09:37:16.562104  5076 net.cpp:122] Setting up bn2_2
I1211 09:37:16.562104  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.562104  5076 net.cpp:137] Memory required for data: 287950000
I1211 09:37:16.562104  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:37:16.562104  5076 net.cpp:84] Creating Layer scale2_2
I1211 09:37:16.562104  5076 net.cpp:406] scale2_2 <- conv2_2
I1211 09:37:16.562104  5076 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 09:37:16.562104  5076 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:37:16.562104  5076 net.cpp:122] Setting up scale2_2
I1211 09:37:16.562104  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.562104  5076 net.cpp:137] Memory required for data: 308430000
I1211 09:37:16.562104  5076 layer_factory.cpp:58] Creating layer relu2_2
I1211 09:37:16.562104  5076 net.cpp:84] Creating Layer relu2_2
I1211 09:37:16.562104  5076 net.cpp:406] relu2_2 <- conv2_2
I1211 09:37:16.562104  5076 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 09:37:16.562104  5076 net.cpp:122] Setting up relu2_2
I1211 09:37:16.562104  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.562104  5076 net.cpp:137] Memory required for data: 328910000
I1211 09:37:16.562104  5076 layer_factory.cpp:58] Creating layer newconv_added1
I1211 09:37:16.562104  5076 net.cpp:84] Creating Layer newconv_added1
I1211 09:37:16.562104  5076 net.cpp:406] newconv_added1 <- conv2_2
I1211 09:37:16.562104  5076 net.cpp:380] newconv_added1 -> newconv_added1
I1211 09:37:16.564116  5076 net.cpp:122] Setting up newconv_added1
I1211 09:37:16.564116  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.564116  5076 net.cpp:137] Memory required for data: 349390000
I1211 09:37:16.564116  5076 layer_factory.cpp:58] Creating layer bn_added1
I1211 09:37:16.564116  5076 net.cpp:84] Creating Layer bn_added1
I1211 09:37:16.564116  5076 net.cpp:406] bn_added1 <- newconv_added1
I1211 09:37:16.564116  5076 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 09:37:16.564116  5076 net.cpp:122] Setting up bn_added1
I1211 09:37:16.564116  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.564116  5076 net.cpp:137] Memory required for data: 369870000
I1211 09:37:16.564116  5076 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:37:16.564116  5076 net.cpp:84] Creating Layer scale_added1
I1211 09:37:16.564116  5076 net.cpp:406] scale_added1 <- newconv_added1
I1211 09:37:16.564116  5076 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 09:37:16.564116  5076 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:37:16.564116  5076 net.cpp:122] Setting up scale_added1
I1211 09:37:16.564116  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.565114  5076 net.cpp:137] Memory required for data: 390350000
I1211 09:37:16.565114  5076 layer_factory.cpp:58] Creating layer relu_added1
I1211 09:37:16.565114  5076 net.cpp:84] Creating Layer relu_added1
I1211 09:37:16.565114  5076 net.cpp:406] relu_added1 <- newconv_added1
I1211 09:37:16.565114  5076 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 09:37:16.565114  5076 net.cpp:122] Setting up relu_added1
I1211 09:37:16.565114  5076 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:37:16.565114  5076 net.cpp:137] Memory required for data: 410830000
I1211 09:37:16.565114  5076 layer_factory.cpp:58] Creating layer pool2_1
I1211 09:37:16.565114  5076 net.cpp:84] Creating Layer pool2_1
I1211 09:37:16.565114  5076 net.cpp:406] pool2_1 <- newconv_added1
I1211 09:37:16.565114  5076 net.cpp:380] pool2_1 -> pool2_1
I1211 09:37:16.565114  5076 net.cpp:122] Setting up pool2_1
I1211 09:37:16.565114  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.565114  5076 net.cpp:137] Memory required for data: 415950000
I1211 09:37:16.565114  5076 layer_factory.cpp:58] Creating layer conv3
I1211 09:37:16.565114  5076 net.cpp:84] Creating Layer conv3
I1211 09:37:16.565114  5076 net.cpp:406] conv3 <- pool2_1
I1211 09:37:16.565114  5076 net.cpp:380] conv3 -> conv3
I1211 09:37:16.566114  5076 net.cpp:122] Setting up conv3
I1211 09:37:16.566114  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.566114  5076 net.cpp:137] Memory required for data: 421070000
I1211 09:37:16.566114  5076 layer_factory.cpp:58] Creating layer bn3
I1211 09:37:16.566114  5076 net.cpp:84] Creating Layer bn3
I1211 09:37:16.566114  5076 net.cpp:406] bn3 <- conv3
I1211 09:37:16.566114  5076 net.cpp:367] bn3 -> conv3 (in-place)
I1211 09:37:16.566114  5076 net.cpp:122] Setting up bn3
I1211 09:37:16.566114  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.566114  5076 net.cpp:137] Memory required for data: 426190000
I1211 09:37:16.566114  5076 layer_factory.cpp:58] Creating layer scale3
I1211 09:37:16.566114  5076 net.cpp:84] Creating Layer scale3
I1211 09:37:16.566114  5076 net.cpp:406] scale3 <- conv3
I1211 09:37:16.566114  5076 net.cpp:367] scale3 -> conv3 (in-place)
I1211 09:37:16.566114  5076 layer_factory.cpp:58] Creating layer scale3
I1211 09:37:16.567118  5076 net.cpp:122] Setting up scale3
I1211 09:37:16.567118  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.567118  5076 net.cpp:137] Memory required for data: 431310000
I1211 09:37:16.567118  5076 layer_factory.cpp:58] Creating layer relu3
I1211 09:37:16.567118  5076 net.cpp:84] Creating Layer relu3
I1211 09:37:16.567118  5076 net.cpp:406] relu3 <- conv3
I1211 09:37:16.567118  5076 net.cpp:367] relu3 -> conv3 (in-place)
I1211 09:37:16.567118  5076 net.cpp:122] Setting up relu3
I1211 09:37:16.567118  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.567118  5076 net.cpp:137] Memory required for data: 436430000
I1211 09:37:16.567118  5076 layer_factory.cpp:58] Creating layer conv3_1
I1211 09:37:16.567118  5076 net.cpp:84] Creating Layer conv3_1
I1211 09:37:16.567118  5076 net.cpp:406] conv3_1 <- conv3
I1211 09:37:16.567118  5076 net.cpp:380] conv3_1 -> conv3_1
I1211 09:37:16.569114  5076 net.cpp:122] Setting up conv3_1
I1211 09:37:16.569114  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.569114  5076 net.cpp:137] Memory required for data: 441550000
I1211 09:37:16.569114  5076 layer_factory.cpp:58] Creating layer bn3_1
I1211 09:37:16.569114  5076 net.cpp:84] Creating Layer bn3_1
I1211 09:37:16.569114  5076 net.cpp:406] bn3_1 <- conv3_1
I1211 09:37:16.569114  5076 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 09:37:16.569114  5076 net.cpp:122] Setting up bn3_1
I1211 09:37:16.569114  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.569114  5076 net.cpp:137] Memory required for data: 446670000
I1211 09:37:16.569114  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:37:16.569114  5076 net.cpp:84] Creating Layer scale3_1
I1211 09:37:16.569114  5076 net.cpp:406] scale3_1 <- conv3_1
I1211 09:37:16.569114  5076 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 09:37:16.569114  5076 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:37:16.570111  5076 net.cpp:122] Setting up scale3_1
I1211 09:37:16.570111  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.570111  5076 net.cpp:137] Memory required for data: 451790000
I1211 09:37:16.570111  5076 layer_factory.cpp:58] Creating layer relu3_1
I1211 09:37:16.570111  5076 net.cpp:84] Creating Layer relu3_1
I1211 09:37:16.570111  5076 net.cpp:406] relu3_1 <- conv3_1
I1211 09:37:16.570111  5076 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 09:37:16.570111  5076 net.cpp:122] Setting up relu3_1
I1211 09:37:16.570111  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.570111  5076 net.cpp:137] Memory required for data: 456910000
I1211 09:37:16.570111  5076 layer_factory.cpp:58] Creating layer conv4
I1211 09:37:16.570111  5076 net.cpp:84] Creating Layer conv4
I1211 09:37:16.570111  5076 net.cpp:406] conv4 <- conv3_1
I1211 09:37:16.570111  5076 net.cpp:380] conv4 -> conv4
I1211 09:37:16.571105  5076 net.cpp:122] Setting up conv4
I1211 09:37:16.571105  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.571105  5076 net.cpp:137] Memory required for data: 462030000
I1211 09:37:16.571105  5076 layer_factory.cpp:58] Creating layer bn4
I1211 09:37:16.571105  5076 net.cpp:84] Creating Layer bn4
I1211 09:37:16.571105  5076 net.cpp:406] bn4 <- conv4
I1211 09:37:16.571105  5076 net.cpp:367] bn4 -> conv4 (in-place)
I1211 09:37:16.572104  5076 net.cpp:122] Setting up bn4
I1211 09:37:16.572104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.572104  5076 net.cpp:137] Memory required for data: 467150000
I1211 09:37:16.572104  5076 layer_factory.cpp:58] Creating layer scale4
I1211 09:37:16.572104  5076 net.cpp:84] Creating Layer scale4
I1211 09:37:16.572104  5076 net.cpp:406] scale4 <- conv4
I1211 09:37:16.572104  5076 net.cpp:367] scale4 -> conv4 (in-place)
I1211 09:37:16.572104  5076 layer_factory.cpp:58] Creating layer scale4
I1211 09:37:16.572104  5076 net.cpp:122] Setting up scale4
I1211 09:37:16.572104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.572104  5076 net.cpp:137] Memory required for data: 472270000
I1211 09:37:16.572104  5076 layer_factory.cpp:58] Creating layer relu4
I1211 09:37:16.572104  5076 net.cpp:84] Creating Layer relu4
I1211 09:37:16.572104  5076 net.cpp:406] relu4 <- conv4
I1211 09:37:16.572104  5076 net.cpp:367] relu4 -> conv4 (in-place)
I1211 09:37:16.572104  5076 net.cpp:122] Setting up relu4
I1211 09:37:16.572104  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.572104  5076 net.cpp:137] Memory required for data: 477390000
I1211 09:37:16.572104  5076 layer_factory.cpp:58] Creating layer conv4_1
I1211 09:37:16.572104  5076 net.cpp:84] Creating Layer conv4_1
I1211 09:37:16.572104  5076 net.cpp:406] conv4_1 <- conv4
I1211 09:37:16.572104  5076 net.cpp:380] conv4_1 -> conv4_1
I1211 09:37:16.573123  5076 net.cpp:122] Setting up conv4_1
I1211 09:37:16.573123  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.573123  5076 net.cpp:137] Memory required for data: 482510000
I1211 09:37:16.573123  5076 layer_factory.cpp:58] Creating layer bn4_1
I1211 09:37:16.573123  5076 net.cpp:84] Creating Layer bn4_1
I1211 09:37:16.573123  5076 net.cpp:406] bn4_1 <- conv4_1
I1211 09:37:16.573123  5076 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 09:37:16.574118  5076 net.cpp:122] Setting up bn4_1
I1211 09:37:16.574118  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.574118  5076 net.cpp:137] Memory required for data: 487630000
I1211 09:37:16.574118  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:37:16.574118  5076 net.cpp:84] Creating Layer scale4_1
I1211 09:37:16.574118  5076 net.cpp:406] scale4_1 <- conv4_1
I1211 09:37:16.574118  5076 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 09:37:16.574118  5076 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:37:16.574118  5076 net.cpp:122] Setting up scale4_1
I1211 09:37:16.574118  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.574118  5076 net.cpp:137] Memory required for data: 492750000
I1211 09:37:16.574118  5076 layer_factory.cpp:58] Creating layer relu4_1
I1211 09:37:16.574118  5076 net.cpp:84] Creating Layer relu4_1
I1211 09:37:16.574118  5076 net.cpp:406] relu4_1 <- conv4_1
I1211 09:37:16.574118  5076 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 09:37:16.574118  5076 net.cpp:122] Setting up relu4_1
I1211 09:37:16.574118  5076 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:37:16.574118  5076 net.cpp:137] Memory required for data: 497870000
I1211 09:37:16.574118  5076 layer_factory.cpp:58] Creating layer conv4_2
I1211 09:37:16.574118  5076 net.cpp:84] Creating Layer conv4_2
I1211 09:37:16.574118  5076 net.cpp:406] conv4_2 <- conv4_1
I1211 09:37:16.574118  5076 net.cpp:380] conv4_2 -> conv4_2
I1211 09:37:16.575119  5076 net.cpp:122] Setting up conv4_2
I1211 09:37:16.575119  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.575119  5076 net.cpp:137] Memory required for data: 503809200
I1211 09:37:16.575119  5076 layer_factory.cpp:58] Creating layer bn4_2
I1211 09:37:16.576118  5076 net.cpp:84] Creating Layer bn4_2
I1211 09:37:16.576118  5076 net.cpp:406] bn4_2 <- conv4_2
I1211 09:37:16.576118  5076 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 09:37:16.576118  5076 net.cpp:122] Setting up bn4_2
I1211 09:37:16.576118  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.576118  5076 net.cpp:137] Memory required for data: 509748400
I1211 09:37:16.576118  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:37:16.576118  5076 net.cpp:84] Creating Layer scale4_2
I1211 09:37:16.576118  5076 net.cpp:406] scale4_2 <- conv4_2
I1211 09:37:16.576118  5076 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 09:37:16.576118  5076 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:37:16.576118  5076 net.cpp:122] Setting up scale4_2
I1211 09:37:16.576118  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.576118  5076 net.cpp:137] Memory required for data: 515687600
I1211 09:37:16.576118  5076 layer_factory.cpp:58] Creating layer relu4_2
I1211 09:37:16.576118  5076 net.cpp:84] Creating Layer relu4_2
I1211 09:37:16.576118  5076 net.cpp:406] relu4_2 <- conv4_2
I1211 09:37:16.576118  5076 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 09:37:16.576118  5076 net.cpp:122] Setting up relu4_2
I1211 09:37:16.576118  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.576118  5076 net.cpp:137] Memory required for data: 521626800
I1211 09:37:16.576118  5076 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 09:37:16.576118  5076 net.cpp:84] Creating Layer added_new_conv2
I1211 09:37:16.576118  5076 net.cpp:406] added_new_conv2 <- conv4_2
I1211 09:37:16.576118  5076 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 09:37:16.577118  5076 net.cpp:122] Setting up added_new_conv2
I1211 09:37:16.577118  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.577118  5076 net.cpp:137] Memory required for data: 527566000
I1211 09:37:16.577118  5076 layer_factory.cpp:58] Creating layer bn_added2
I1211 09:37:16.577118  5076 net.cpp:84] Creating Layer bn_added2
I1211 09:37:16.577118  5076 net.cpp:406] bn_added2 <- added_new_conv2
I1211 09:37:16.577118  5076 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.578119  5076 net.cpp:122] Setting up bn_added2
I1211 09:37:16.578119  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.578119  5076 net.cpp:137] Memory required for data: 533505200
I1211 09:37:16.578119  5076 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:37:16.578119  5076 net.cpp:84] Creating Layer scale_added2
I1211 09:37:16.578119  5076 net.cpp:406] scale_added2 <- added_new_conv2
I1211 09:37:16.578119  5076 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.578119  5076 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:37:16.578119  5076 net.cpp:122] Setting up scale_added2
I1211 09:37:16.578119  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.578119  5076 net.cpp:137] Memory required for data: 539444400
I1211 09:37:16.578119  5076 layer_factory.cpp:58] Creating layer relu_added2
I1211 09:37:16.578119  5076 net.cpp:84] Creating Layer relu_added2
I1211 09:37:16.578119  5076 net.cpp:406] relu_added2 <- added_new_conv2
I1211 09:37:16.578119  5076 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 09:37:16.579116  5076 net.cpp:122] Setting up relu_added2
I1211 09:37:16.579116  5076 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:37:16.579116  5076 net.cpp:137] Memory required for data: 545383600
I1211 09:37:16.579116  5076 layer_factory.cpp:58] Creating layer pool4_2
I1211 09:37:16.579116  5076 net.cpp:84] Creating Layer pool4_2
I1211 09:37:16.579116  5076 net.cpp:406] pool4_2 <- added_new_conv2
I1211 09:37:16.579116  5076 net.cpp:380] pool4_2 -> pool4_2
I1211 09:37:16.579116  5076 net.cpp:122] Setting up pool4_2
I1211 09:37:16.579116  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.579116  5076 net.cpp:137] Memory required for data: 546868400
I1211 09:37:16.579116  5076 layer_factory.cpp:58] Creating layer conv4_0
I1211 09:37:16.579116  5076 net.cpp:84] Creating Layer conv4_0
I1211 09:37:16.579116  5076 net.cpp:406] conv4_0 <- pool4_2
I1211 09:37:16.579116  5076 net.cpp:380] conv4_0 -> conv4_0
I1211 09:37:16.581120  5076 net.cpp:122] Setting up conv4_0
I1211 09:37:16.581120  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.581120  5076 net.cpp:137] Memory required for data: 548353200
I1211 09:37:16.581120  5076 layer_factory.cpp:58] Creating layer bn4_0
I1211 09:37:16.581120  5076 net.cpp:84] Creating Layer bn4_0
I1211 09:37:16.581120  5076 net.cpp:406] bn4_0 <- conv4_0
I1211 09:37:16.581120  5076 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 09:37:16.581120  5076 net.cpp:122] Setting up bn4_0
I1211 09:37:16.581120  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.581120  5076 net.cpp:137] Memory required for data: 549838000
I1211 09:37:16.581120  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:37:16.581120  5076 net.cpp:84] Creating Layer scale4_0
I1211 09:37:16.581120  5076 net.cpp:406] scale4_0 <- conv4_0
I1211 09:37:16.581120  5076 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 09:37:16.581120  5076 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:37:16.581120  5076 net.cpp:122] Setting up scale4_0
I1211 09:37:16.581120  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.581120  5076 net.cpp:137] Memory required for data: 551322800
I1211 09:37:16.581120  5076 layer_factory.cpp:58] Creating layer relu4_0
I1211 09:37:16.581120  5076 net.cpp:84] Creating Layer relu4_0
I1211 09:37:16.581120  5076 net.cpp:406] relu4_0 <- conv4_0
I1211 09:37:16.581120  5076 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 09:37:16.582108  5076 net.cpp:122] Setting up relu4_0
I1211 09:37:16.582108  5076 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:37:16.582108  5076 net.cpp:137] Memory required for data: 552807600
I1211 09:37:16.582108  5076 layer_factory.cpp:58] Creating layer conv11
I1211 09:37:16.582108  5076 net.cpp:84] Creating Layer conv11
I1211 09:37:16.582108  5076 net.cpp:406] conv11 <- conv4_0
I1211 09:37:16.582108  5076 net.cpp:380] conv11 -> conv11
I1211 09:37:16.584120  5076 net.cpp:122] Setting up conv11
I1211 09:37:16.584120  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.584120  5076 net.cpp:137] Memory required for data: 554599600
I1211 09:37:16.584120  5076 layer_factory.cpp:58] Creating layer bn_conv11
I1211 09:37:16.584120  5076 net.cpp:84] Creating Layer bn_conv11
I1211 09:37:16.584120  5076 net.cpp:406] bn_conv11 <- conv11
I1211 09:37:16.584120  5076 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 09:37:16.584120  5076 net.cpp:122] Setting up bn_conv11
I1211 09:37:16.584120  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.584120  5076 net.cpp:137] Memory required for data: 556391600
I1211 09:37:16.584120  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:37:16.584120  5076 net.cpp:84] Creating Layer scale_conv11
I1211 09:37:16.584120  5076 net.cpp:406] scale_conv11 <- conv11
I1211 09:37:16.584120  5076 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 09:37:16.584120  5076 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:37:16.584120  5076 net.cpp:122] Setting up scale_conv11
I1211 09:37:16.584120  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.584120  5076 net.cpp:137] Memory required for data: 558183600
I1211 09:37:16.584120  5076 layer_factory.cpp:58] Creating layer relu_conv11
I1211 09:37:16.584120  5076 net.cpp:84] Creating Layer relu_conv11
I1211 09:37:16.584120  5076 net.cpp:406] relu_conv11 <- conv11
I1211 09:37:16.584120  5076 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 09:37:16.584120  5076 net.cpp:122] Setting up relu_conv11
I1211 09:37:16.584120  5076 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:37:16.584120  5076 net.cpp:137] Memory required for data: 559975600
I1211 09:37:16.584120  5076 layer_factory.cpp:58] Creating layer conv12
I1211 09:37:16.584120  5076 net.cpp:84] Creating Layer conv12
I1211 09:37:16.584120  5076 net.cpp:406] conv12 <- conv11
I1211 09:37:16.584120  5076 net.cpp:380] conv12 -> conv12
I1211 09:37:16.586119  5076 net.cpp:122] Setting up conv12
I1211 09:37:16.586119  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.586119  5076 net.cpp:137] Memory required for data: 562279600
I1211 09:37:16.586119  5076 layer_factory.cpp:58] Creating layer bn_conv12
I1211 09:37:16.586119  5076 net.cpp:84] Creating Layer bn_conv12
I1211 09:37:16.586119  5076 net.cpp:406] bn_conv12 <- conv12
I1211 09:37:16.586119  5076 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 09:37:16.586119  5076 net.cpp:122] Setting up bn_conv12
I1211 09:37:16.586119  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.586119  5076 net.cpp:137] Memory required for data: 564583600
I1211 09:37:16.586119  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:37:16.586119  5076 net.cpp:84] Creating Layer scale_conv12
I1211 09:37:16.586119  5076 net.cpp:406] scale_conv12 <- conv12
I1211 09:37:16.586119  5076 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 09:37:16.586119  5076 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:37:16.586119  5076 net.cpp:122] Setting up scale_conv12
I1211 09:37:16.586119  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.586119  5076 net.cpp:137] Memory required for data: 566887600
I1211 09:37:16.586119  5076 layer_factory.cpp:58] Creating layer relu_conv12
I1211 09:37:16.586119  5076 net.cpp:84] Creating Layer relu_conv12
I1211 09:37:16.586119  5076 net.cpp:406] relu_conv12 <- conv12
I1211 09:37:16.586119  5076 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 09:37:16.587103  5076 net.cpp:122] Setting up relu_conv12
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:37:16.587103  5076 net.cpp:137] Memory required for data: 569191600
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer poolcp6
I1211 09:37:16.587103  5076 net.cpp:84] Creating Layer poolcp6
I1211 09:37:16.587103  5076 net.cpp:406] poolcp6 <- conv12
I1211 09:37:16.587103  5076 net.cpp:380] poolcp6 -> poolcp6
I1211 09:37:16.587103  5076 net.cpp:122] Setting up poolcp6
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 09:37:16.587103  5076 net.cpp:137] Memory required for data: 569227600
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer ip1
I1211 09:37:16.587103  5076 net.cpp:84] Creating Layer ip1
I1211 09:37:16.587103  5076 net.cpp:406] ip1 <- poolcp6
I1211 09:37:16.587103  5076 net.cpp:380] ip1 -> ip1
I1211 09:37:16.587103  5076 net.cpp:122] Setting up ip1
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.587103  5076 net.cpp:137] Memory required for data: 569267600
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 09:37:16.587103  5076 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 09:37:16.587103  5076 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 09:37:16.587103  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 09:37:16.587103  5076 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 09:37:16.587103  5076 net.cpp:122] Setting up ip1_ip1_0_split
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:37:16.587103  5076 net.cpp:137] Memory required for data: 569347600
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer accuracy
I1211 09:37:16.587103  5076 net.cpp:84] Creating Layer accuracy
I1211 09:37:16.587103  5076 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 09:37:16.587103  5076 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 09:37:16.587103  5076 net.cpp:380] accuracy -> accuracy
I1211 09:37:16.587103  5076 net.cpp:122] Setting up accuracy
I1211 09:37:16.587103  5076 net.cpp:129] Top shape: (1)
I1211 09:37:16.587103  5076 net.cpp:137] Memory required for data: 569347604
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer loss
I1211 09:37:16.587103  5076 net.cpp:84] Creating Layer loss
I1211 09:37:16.587103  5076 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 09:37:16.587103  5076 net.cpp:406] loss <- label_cifar_1_split_1
I1211 09:37:16.587103  5076 net.cpp:380] loss -> loss
I1211 09:37:16.587103  5076 layer_factory.cpp:58] Creating layer loss
I1211 09:37:16.588115  5076 net.cpp:122] Setting up loss
I1211 09:37:16.588115  5076 net.cpp:129] Top shape: (1)
I1211 09:37:16.588115  5076 net.cpp:132]     with loss weight 1
I1211 09:37:16.588115  5076 net.cpp:137] Memory required for data: 569347608
I1211 09:37:16.588115  5076 net.cpp:198] loss needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:200] accuracy does not need backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] ip1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] poolcp6 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu_conv12 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale_conv12 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn_conv12 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv12 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu_conv11 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale_conv11 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn_conv11 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv11 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu4_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale4_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn4_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv4_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] pool4_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu_added2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale_added2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn_added2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] added_new_conv2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu4_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale4_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn4_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv4_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu4_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale4_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn4_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv4_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu4 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale4 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn4 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv4 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu3_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale3_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn3_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv3_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu3 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale3 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn3 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv3 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] pool2_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu_added1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale_added1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn_added1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] newconv_added1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu2_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale2_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn2_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv2_2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu2_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale2_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn2_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv2_1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv2 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu1_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale1_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn1_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv1_0 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] relu1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] scale1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] bn1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:198] conv1 needs backward computation.
I1211 09:37:16.588115  5076 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 09:37:16.588115  5076 net.cpp:200] cifar does not need backward computation.
I1211 09:37:16.588115  5076 net.cpp:242] This network produces output accuracy
I1211 09:37:16.588115  5076 net.cpp:242] This network produces output loss
I1211 09:37:16.588115  5076 net.cpp:255] Network initialization done.
I1211 09:37:16.588115  5076 solver.cpp:56] Solver scaffolding done.
I1211 09:37:16.593114  5076 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate
I1211 09:37:16.596114  5076 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.caffemodel
I1211 09:37:16.596114  5076 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:37:16.597118  5076 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 09:37:16.601119  5076 caffe.cpp:249] Starting Optimization
I1211 09:37:16.601119  5076 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k
I1211 09:37:16.601119  5076 solver.cpp:273] Learning Rate Policy: multistep
I1211 09:37:16.604130  5076 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 09:37:16.606148  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:37:18.182171  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:37:18.241691  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5908
I1211 09:37:18.241691  5076 solver.cpp:397]     Test net output #1: loss = 1.61171 (* 1 = 1.61171 loss)
I1211 09:37:18.356446  5076 solver.cpp:218] Iteration 90000 (51300.9 iter/s, 1.75435s/100 iters), loss = 0.74886
I1211 09:37:18.356446  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:37:18.356446  5076 solver.cpp:237]     Train net output #1: loss = 0.74886 (* 1 = 0.74886 loss)
I1211 09:37:18.356446  5076 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 09:37:24.739164  5076 solver.cpp:218] Iteration 90100 (15.669 iter/s, 6.38205s/100 iters), loss = 0.682632
I1211 09:37:24.739164  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:37:24.739164  5076 solver.cpp:237]     Train net output #1: loss = 0.682632 (* 1 = 0.682632 loss)
I1211 09:37:24.739164  5076 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 09:37:31.104565  5076 solver.cpp:218] Iteration 90200 (15.7109 iter/s, 6.36501s/100 iters), loss = 0.562995
I1211 09:37:31.104565  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:37:31.104565  5076 solver.cpp:237]     Train net output #1: loss = 0.562995 (* 1 = 0.562995 loss)
I1211 09:37:31.104565  5076 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 09:37:37.495929  5076 solver.cpp:218] Iteration 90300 (15.6461 iter/s, 6.39139s/100 iters), loss = 0.766713
I1211 09:37:37.495929  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:37:37.495929  5076 solver.cpp:237]     Train net output #1: loss = 0.766713 (* 1 = 0.766713 loss)
I1211 09:37:37.495929  5076 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 09:37:43.843521  5076 solver.cpp:218] Iteration 90400 (15.7559 iter/s, 6.34684s/100 iters), loss = 0.816327
I1211 09:37:43.843521  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 09:37:43.843521  5076 solver.cpp:237]     Train net output #1: loss = 0.816327 (* 1 = 0.816327 loss)
I1211 09:37:43.843521  5076 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 09:37:49.906575 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:37:50.156112  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.caffemodel
I1211 09:37:50.171113  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.solverstate
I1211 09:37:50.176112  5076 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 09:37:50.176112  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:37:51.712555  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:37:51.775166  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5629
I1211 09:37:51.775166  5076 solver.cpp:397]     Test net output #1: loss = 1.78903 (* 1 = 1.78903 loss)
I1211 09:37:51.838320  5076 solver.cpp:218] Iteration 90500 (12.5081 iter/s, 7.9948s/100 iters), loss = 0.67461
I1211 09:37:51.839321  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:37:51.839321  5076 solver.cpp:237]     Train net output #1: loss = 0.67461 (* 1 = 0.67461 loss)
I1211 09:37:51.839321  5076 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 09:37:58.225239  5076 solver.cpp:218] Iteration 90600 (15.6604 iter/s, 6.38555s/100 iters), loss = 0.680435
I1211 09:37:58.225239  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:37:58.225239  5076 solver.cpp:237]     Train net output #1: loss = 0.680435 (* 1 = 0.680435 loss)
I1211 09:37:58.225239  5076 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 09:38:04.641860  5076 solver.cpp:218] Iteration 90700 (15.5848 iter/s, 6.4165s/100 iters), loss = 0.62471
I1211 09:38:04.642345  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:38:04.642345  5076 solver.cpp:237]     Train net output #1: loss = 0.62471 (* 1 = 0.62471 loss)
I1211 09:38:04.642345  5076 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 09:38:10.983954  5076 solver.cpp:218] Iteration 90800 (15.7689 iter/s, 6.34161s/100 iters), loss = 0.730053
I1211 09:38:10.983954  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 09:38:10.983954  5076 solver.cpp:237]     Train net output #1: loss = 0.730053 (* 1 = 0.730053 loss)
I1211 09:38:10.983954  5076 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 09:38:17.330090  5076 solver.cpp:218] Iteration 90900 (15.7592 iter/s, 6.3455s/100 iters), loss = 0.692629
I1211 09:38:17.330090  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:38:17.330090  5076 solver.cpp:237]     Train net output #1: loss = 0.692629 (* 1 = 0.692629 loss)
I1211 09:38:17.330090  5076 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 09:38:23.371260 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:38:23.624296  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.caffemodel
I1211 09:38:23.640296  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.solverstate
I1211 09:38:23.645298  5076 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 09:38:23.645298  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:38:25.164438  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:38:25.224442  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5856
I1211 09:38:25.224442  5076 solver.cpp:397]     Test net output #1: loss = 1.67441 (* 1 = 1.67441 loss)
I1211 09:38:25.285442  5076 solver.cpp:218] Iteration 91000 (12.5706 iter/s, 7.95504s/100 iters), loss = 0.62983
I1211 09:38:25.285442  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:38:25.285442  5076 solver.cpp:237]     Train net output #1: loss = 0.62983 (* 1 = 0.62983 loss)
I1211 09:38:25.285442  5076 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 09:38:31.612888  5076 solver.cpp:218] Iteration 91100 (15.8047 iter/s, 6.32724s/100 iters), loss = 0.683559
I1211 09:38:31.612888  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:38:31.612888  5076 solver.cpp:237]     Train net output #1: loss = 0.683559 (* 1 = 0.683559 loss)
I1211 09:38:31.612888  5076 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 09:38:37.951344  5076 solver.cpp:218] Iteration 91200 (15.7778 iter/s, 6.33803s/100 iters), loss = 0.623467
I1211 09:38:37.951344  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:38:37.951344  5076 solver.cpp:237]     Train net output #1: loss = 0.623467 (* 1 = 0.623467 loss)
I1211 09:38:37.951344  5076 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 09:38:44.292330  5076 solver.cpp:218] Iteration 91300 (15.7722 iter/s, 6.34026s/100 iters), loss = 0.61372
I1211 09:38:44.292831  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:38:44.292831  5076 solver.cpp:237]     Train net output #1: loss = 0.61372 (* 1 = 0.61372 loss)
I1211 09:38:44.292831  5076 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 09:38:50.624308  5076 solver.cpp:218] Iteration 91400 (15.7941 iter/s, 6.33148s/100 iters), loss = 0.78888
I1211 09:38:50.624308  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 09:38:50.624308  5076 solver.cpp:237]     Train net output #1: loss = 0.78888 (* 1 = 0.78888 loss)
I1211 09:38:50.624308  5076 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 09:38:56.649706 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:38:56.898717  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.caffemodel
I1211 09:38:56.914721  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.solverstate
I1211 09:38:56.919721  5076 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 09:38:56.919721  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:38:58.437845  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:38:58.497359  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5903
I1211 09:38:58.497859  5076 solver.cpp:397]     Test net output #1: loss = 1.65412 (* 1 = 1.65412 loss)
I1211 09:38:58.557862  5076 solver.cpp:218] Iteration 91500 (12.6047 iter/s, 7.93356s/100 iters), loss = 0.58745
I1211 09:38:58.557862  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:38:58.557862  5076 solver.cpp:237]     Train net output #1: loss = 0.58745 (* 1 = 0.58745 loss)
I1211 09:38:58.557862  5076 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 09:39:04.900283  5076 solver.cpp:218] Iteration 91600 (15.7697 iter/s, 6.34126s/100 iters), loss = 0.649182
I1211 09:39:04.900283  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:39:04.900283  5076 solver.cpp:237]     Train net output #1: loss = 0.649182 (* 1 = 0.649182 loss)
I1211 09:39:04.900283  5076 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 09:39:11.229686  5076 solver.cpp:218] Iteration 91700 (15.7994 iter/s, 6.32936s/100 iters), loss = 0.567265
I1211 09:39:11.229686  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 09:39:11.229686  5076 solver.cpp:237]     Train net output #1: loss = 0.567265 (* 1 = 0.567265 loss)
I1211 09:39:11.229686  5076 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 09:39:17.562117  5076 solver.cpp:218] Iteration 91800 (15.7921 iter/s, 6.33227s/100 iters), loss = 0.763634
I1211 09:39:17.562117  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:39:17.562117  5076 solver.cpp:237]     Train net output #1: loss = 0.763634 (* 1 = 0.763634 loss)
I1211 09:39:17.562117  5076 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 09:39:23.900566  5076 solver.cpp:218] Iteration 91900 (15.7786 iter/s, 6.3377s/100 iters), loss = 0.69141
I1211 09:39:23.900566  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:39:23.900566  5076 solver.cpp:237]     Train net output #1: loss = 0.69141 (* 1 = 0.69141 loss)
I1211 09:39:23.900566  5076 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 09:39:29.924060 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:39:30.175071  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.caffemodel
I1211 09:39:30.192575  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.solverstate
I1211 09:39:30.197075  5076 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 09:39:30.197075  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:39:31.714155  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:39:31.774154  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6058
I1211 09:39:31.775156  5076 solver.cpp:397]     Test net output #1: loss = 1.53413 (* 1 = 1.53413 loss)
I1211 09:39:31.835160  5076 solver.cpp:218] Iteration 92000 (12.6032 iter/s, 7.93446s/100 iters), loss = 0.615464
I1211 09:39:31.835160  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:39:31.835160  5076 solver.cpp:237]     Train net output #1: loss = 0.615464 (* 1 = 0.615464 loss)
I1211 09:39:31.835160  5076 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 09:39:38.179657  5076 solver.cpp:218] Iteration 92100 (15.7634 iter/s, 6.34381s/100 iters), loss = 0.752277
I1211 09:39:38.179657  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:39:38.179657  5076 solver.cpp:237]     Train net output #1: loss = 0.752277 (* 1 = 0.752277 loss)
I1211 09:39:38.179657  5076 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 09:39:44.511858  5076 solver.cpp:218] Iteration 92200 (15.7932 iter/s, 6.33184s/100 iters), loss = 0.619502
I1211 09:39:44.511858  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:39:44.511858  5076 solver.cpp:237]     Train net output #1: loss = 0.619502 (* 1 = 0.619502 loss)
I1211 09:39:44.511858  5076 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 09:39:50.849663  5076 solver.cpp:218] Iteration 92300 (15.7801 iter/s, 6.33708s/100 iters), loss = 0.8447
I1211 09:39:50.849663  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.67
I1211 09:39:50.849663  5076 solver.cpp:237]     Train net output #1: loss = 0.8447 (* 1 = 0.8447 loss)
I1211 09:39:50.849663  5076 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 09:39:57.187397  5076 solver.cpp:218] Iteration 92400 (15.7795 iter/s, 6.33736s/100 iters), loss = 0.742961
I1211 09:39:57.187397  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:39:57.187397  5076 solver.cpp:237]     Train net output #1: loss = 0.742961 (* 1 = 0.742961 loss)
I1211 09:39:57.187397  5076 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 09:40:03.236383 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:40:03.488400  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.caffemodel
I1211 09:40:03.502400  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.solverstate
I1211 09:40:03.507400  5076 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 09:40:03.507400  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:40:05.028517  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:40:05.088521  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5833
I1211 09:40:05.088521  5076 solver.cpp:397]     Test net output #1: loss = 1.6224 (* 1 = 1.6224 loss)
I1211 09:40:05.149519  5076 solver.cpp:218] Iteration 92500 (12.5597 iter/s, 7.962s/100 iters), loss = 0.655313
I1211 09:40:05.149519  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:40:05.149519  5076 solver.cpp:237]     Train net output #1: loss = 0.655313 (* 1 = 0.655313 loss)
I1211 09:40:05.149519  5076 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 09:40:11.489020  5076 solver.cpp:218] Iteration 92600 (15.7765 iter/s, 6.33854s/100 iters), loss = 0.790746
I1211 09:40:11.489020  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:40:11.489020  5076 solver.cpp:237]     Train net output #1: loss = 0.790746 (* 1 = 0.790746 loss)
I1211 09:40:11.489020  5076 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 09:40:17.813550  5076 solver.cpp:218] Iteration 92700 (15.8118 iter/s, 6.32439s/100 iters), loss = 0.69386
I1211 09:40:17.813550  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:40:17.813550  5076 solver.cpp:237]     Train net output #1: loss = 0.69386 (* 1 = 0.69386 loss)
I1211 09:40:17.813550  5076 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 09:40:24.148988  5076 solver.cpp:218] Iteration 92800 (15.7865 iter/s, 6.33454s/100 iters), loss = 0.847625
I1211 09:40:24.148988  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 09:40:24.148988  5076 solver.cpp:237]     Train net output #1: loss = 0.847625 (* 1 = 0.847625 loss)
I1211 09:40:24.148988  5076 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 09:40:30.477978  5076 solver.cpp:218] Iteration 92900 (15.8012 iter/s, 6.32861s/100 iters), loss = 0.747196
I1211 09:40:30.477978  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:40:30.477978  5076 solver.cpp:237]     Train net output #1: loss = 0.747196 (* 1 = 0.747196 loss)
I1211 09:40:30.477978  5076 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 09:40:36.499915 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:40:36.747928  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.caffemodel
I1211 09:40:36.762928  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.solverstate
I1211 09:40:36.767928  5076 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 09:40:36.767928  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:40:38.285550  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:40:38.346053  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5623
I1211 09:40:38.346053  5076 solver.cpp:397]     Test net output #1: loss = 1.83227 (* 1 = 1.83227 loss)
I1211 09:40:38.406057  5076 solver.cpp:218] Iteration 93000 (12.6129 iter/s, 7.92839s/100 iters), loss = 0.538011
I1211 09:40:38.406057  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:40:38.406057  5076 solver.cpp:237]     Train net output #1: loss = 0.538011 (* 1 = 0.538011 loss)
I1211 09:40:38.406057  5076 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 09:40:44.751531  5076 solver.cpp:218] Iteration 93100 (15.7616 iter/s, 6.34454s/100 iters), loss = 0.668017
I1211 09:40:44.751531  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:40:44.751531  5076 solver.cpp:237]     Train net output #1: loss = 0.668017 (* 1 = 0.668017 loss)
I1211 09:40:44.751531  5076 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 09:40:51.104005  5076 solver.cpp:218] Iteration 93200 (15.7426 iter/s, 6.35217s/100 iters), loss = 0.696363
I1211 09:40:51.104005  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:40:51.104005  5076 solver.cpp:237]     Train net output #1: loss = 0.696363 (* 1 = 0.696363 loss)
I1211 09:40:51.104005  5076 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 09:40:57.453476  5076 solver.cpp:218] Iteration 93300 (15.7496 iter/s, 6.34937s/100 iters), loss = 0.754788
I1211 09:40:57.454478  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 09:40:57.454478  5076 solver.cpp:237]     Train net output #1: loss = 0.754788 (* 1 = 0.754788 loss)
I1211 09:40:57.454478  5076 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 09:41:03.803958  5076 solver.cpp:218] Iteration 93400 (15.749 iter/s, 6.34962s/100 iters), loss = 0.768888
I1211 09:41:03.803958  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:41:03.803958  5076 solver.cpp:237]     Train net output #1: loss = 0.768888 (* 1 = 0.768888 loss)
I1211 09:41:03.803958  5076 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 09:41:09.837438 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:41:10.087460  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.caffemodel
I1211 09:41:10.103461  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.solverstate
I1211 09:41:10.108461  5076 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 09:41:10.108461  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:41:11.629782  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:41:11.689781  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5925
I1211 09:41:11.689781  5076 solver.cpp:397]     Test net output #1: loss = 1.59579 (* 1 = 1.59579 loss)
I1211 09:41:11.749786  5076 solver.cpp:218] Iteration 93500 (12.5855 iter/s, 7.94566s/100 iters), loss = 0.60468
I1211 09:41:11.749786  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:41:11.749786  5076 solver.cpp:237]     Train net output #1: loss = 0.60468 (* 1 = 0.60468 loss)
I1211 09:41:11.749786  5076 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 09:41:18.101251  5076 solver.cpp:218] Iteration 93600 (15.7476 iter/s, 6.35016s/100 iters), loss = 0.654179
I1211 09:41:18.101251  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 09:41:18.101251  5076 solver.cpp:237]     Train net output #1: loss = 0.654179 (* 1 = 0.654179 loss)
I1211 09:41:18.101251  5076 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 09:41:24.450744  5076 solver.cpp:218] Iteration 93700 (15.7499 iter/s, 6.34926s/100 iters), loss = 0.525548
I1211 09:41:24.450744  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:41:24.450744  5076 solver.cpp:237]     Train net output #1: loss = 0.525548 (* 1 = 0.525548 loss)
I1211 09:41:24.450744  5076 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 09:41:30.797044  5076 solver.cpp:218] Iteration 93800 (15.7584 iter/s, 6.34581s/100 iters), loss = 0.653243
I1211 09:41:30.797044  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 09:41:30.797044  5076 solver.cpp:237]     Train net output #1: loss = 0.653243 (* 1 = 0.653243 loss)
I1211 09:41:30.797044  5076 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 09:41:37.147852  5076 solver.cpp:218] Iteration 93900 (15.7471 iter/s, 6.35037s/100 iters), loss = 0.699942
I1211 09:41:37.147852  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:41:37.147852  5076 solver.cpp:237]     Train net output #1: loss = 0.699942 (* 1 = 0.699942 loss)
I1211 09:41:37.147852  5076 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 09:41:43.178314 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:41:43.428824  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.caffemodel
I1211 09:41:43.443328  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.solverstate
I1211 09:41:43.448329  5076 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 09:41:43.448329  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:41:44.965431  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:41:45.025933  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5948
I1211 09:41:45.025933  5076 solver.cpp:397]     Test net output #1: loss = 1.59526 (* 1 = 1.59526 loss)
I1211 09:41:45.086434  5076 solver.cpp:218] Iteration 94000 (12.5971 iter/s, 7.93836s/100 iters), loss = 0.776403
I1211 09:41:45.086434  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:41:45.086434  5076 solver.cpp:237]     Train net output #1: loss = 0.776403 (* 1 = 0.776403 loss)
I1211 09:41:45.086434  5076 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 09:41:51.426893  5076 solver.cpp:218] Iteration 94100 (15.7733 iter/s, 6.33983s/100 iters), loss = 0.707245
I1211 09:41:51.426893  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:41:51.426893  5076 solver.cpp:237]     Train net output #1: loss = 0.707245 (* 1 = 0.707245 loss)
I1211 09:41:51.426893  5076 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 09:41:57.837430  5076 solver.cpp:218] Iteration 94200 (15.5993 iter/s, 6.41053s/100 iters), loss = 0.666381
I1211 09:41:57.837430  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:41:57.837430  5076 solver.cpp:237]     Train net output #1: loss = 0.666381 (* 1 = 0.666381 loss)
I1211 09:41:57.837430  5076 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 09:42:04.183042  5076 solver.cpp:218] Iteration 94300 (15.7608 iter/s, 6.34484s/100 iters), loss = 0.660024
I1211 09:42:04.183042  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:42:04.183042  5076 solver.cpp:237]     Train net output #1: loss = 0.660024 (* 1 = 0.660024 loss)
I1211 09:42:04.183042  5076 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 09:42:10.596861  5076 solver.cpp:218] Iteration 94400 (15.5925 iter/s, 6.41334s/100 iters), loss = 0.809594
I1211 09:42:10.596861  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:42:10.596861  5076 solver.cpp:237]     Train net output #1: loss = 0.809594 (* 1 = 0.809594 loss)
I1211 09:42:10.596861  5076 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 09:42:16.795473 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:42:17.052587  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.caffemodel
I1211 09:42:17.074606  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.solverstate
I1211 09:42:17.080605  5076 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 09:42:17.080605  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:42:18.642488  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:42:18.705502  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5922
I1211 09:42:18.705502  5076 solver.cpp:397]     Test net output #1: loss = 1.59547 (* 1 = 1.59547 loss)
I1211 09:42:18.768525  5076 solver.cpp:218] Iteration 94500 (12.2388 iter/s, 8.17075s/100 iters), loss = 0.677559
I1211 09:42:18.768525  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 09:42:18.768525  5076 solver.cpp:237]     Train net output #1: loss = 0.677559 (* 1 = 0.677559 loss)
I1211 09:42:18.768525  5076 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 09:42:25.296066  5076 solver.cpp:218] Iteration 94600 (15.319 iter/s, 6.52785s/100 iters), loss = 0.646372
I1211 09:42:25.297072  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:42:25.297072  5076 solver.cpp:237]     Train net output #1: loss = 0.646372 (* 1 = 0.646372 loss)
I1211 09:42:25.297072  5076 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 09:42:31.759286  5076 solver.cpp:218] Iteration 94700 (15.4739 iter/s, 6.4625s/100 iters), loss = 0.514611
I1211 09:42:31.759286  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 09:42:31.759286  5076 solver.cpp:237]     Train net output #1: loss = 0.514611 (* 1 = 0.514611 loss)
I1211 09:42:31.759286  5076 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 09:42:38.137210  5076 solver.cpp:218] Iteration 94800 (15.6822 iter/s, 6.37667s/100 iters), loss = 0.692424
I1211 09:42:38.137210  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:42:38.137210  5076 solver.cpp:237]     Train net output #1: loss = 0.692424 (* 1 = 0.692424 loss)
I1211 09:42:38.137210  5076 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 09:42:44.512846  5076 solver.cpp:218] Iteration 94900 (15.6855 iter/s, 6.3753s/100 iters), loss = 0.732049
I1211 09:42:44.512846  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:42:44.512846  5076 solver.cpp:237]     Train net output #1: loss = 0.732049 (* 1 = 0.732049 loss)
I1211 09:42:44.512846  5076 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 09:42:50.588757 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:42:50.841270  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.caffemodel
I1211 09:42:50.856775  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.solverstate
I1211 09:42:50.861778  5076 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 09:42:50.861778  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:42:52.393892  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:42:52.454897  5076 solver.cpp:397]     Test net output #0: accuracy = 0.5714
I1211 09:42:52.454897  5076 solver.cpp:397]     Test net output #1: loss = 1.76185 (* 1 = 1.76185 loss)
I1211 09:42:52.516901  5076 solver.cpp:218] Iteration 95000 (12.4949 iter/s, 8.00329s/100 iters), loss = 0.66951
I1211 09:42:52.516901  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 09:42:52.516901  5076 solver.cpp:237]     Train net output #1: loss = 0.66951 (* 1 = 0.66951 loss)
I1211 09:42:52.516901  5076 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 09:42:52.516901  5076 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 09:42:58.995491  5076 solver.cpp:218] Iteration 95100 (15.4359 iter/s, 6.4784s/100 iters), loss = 0.679516
I1211 09:42:58.995491  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:42:58.995491  5076 solver.cpp:237]     Train net output #1: loss = 0.679516 (* 1 = 0.679516 loss)
I1211 09:42:58.995491  5076 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 09:43:05.417032  5076 solver.cpp:218] Iteration 95200 (15.5739 iter/s, 6.42098s/100 iters), loss = 0.508673
I1211 09:43:05.417032  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:43:05.417032  5076 solver.cpp:237]     Train net output #1: loss = 0.508673 (* 1 = 0.508673 loss)
I1211 09:43:05.417032  5076 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 09:43:11.826503  5076 solver.cpp:218] Iteration 95300 (15.6034 iter/s, 6.40886s/100 iters), loss = 0.476329
I1211 09:43:11.826503  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:43:11.826503  5076 solver.cpp:237]     Train net output #1: loss = 0.476329 (* 1 = 0.476329 loss)
I1211 09:43:11.826503  5076 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 09:43:18.229985  5076 solver.cpp:218] Iteration 95400 (15.6157 iter/s, 6.40381s/100 iters), loss = 0.511068
I1211 09:43:18.229985  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:43:18.229985  5076 solver.cpp:237]     Train net output #1: loss = 0.511068 (* 1 = 0.511068 loss)
I1211 09:43:18.229985  5076 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 09:43:24.320473 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:43:24.571490  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.caffemodel
I1211 09:43:24.585490  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.solverstate
I1211 09:43:24.590490  5076 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 09:43:24.590490  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:43:26.124593  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:43:26.184597  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6724
I1211 09:43:26.184597  5076 solver.cpp:397]     Test net output #1: loss = 1.19746 (* 1 = 1.19746 loss)
I1211 09:43:26.245599  5076 solver.cpp:218] Iteration 95500 (12.4771 iter/s, 8.01471s/100 iters), loss = 0.489466
I1211 09:43:26.246098  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:43:26.246098  5076 solver.cpp:237]     Train net output #1: loss = 0.489466 (* 1 = 0.489466 loss)
I1211 09:43:26.246098  5076 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 09:43:32.650583  5076 solver.cpp:218] Iteration 95600 (15.6143 iter/s, 6.4044s/100 iters), loss = 0.54427
I1211 09:43:32.650583  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:43:32.650583  5076 solver.cpp:237]     Train net output #1: loss = 0.54427 (* 1 = 0.54427 loss)
I1211 09:43:32.650583  5076 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 09:43:39.076716  5076 solver.cpp:218] Iteration 95700 (15.5622 iter/s, 6.42582s/100 iters), loss = 0.391753
I1211 09:43:39.076716  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:43:39.076716  5076 solver.cpp:237]     Train net output #1: loss = 0.391753 (* 1 = 0.391753 loss)
I1211 09:43:39.076716  5076 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 09:43:45.471169  5076 solver.cpp:218] Iteration 95800 (15.6408 iter/s, 6.39355s/100 iters), loss = 0.499719
I1211 09:43:45.471169  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:43:45.471169  5076 solver.cpp:237]     Train net output #1: loss = 0.499719 (* 1 = 0.499719 loss)
I1211 09:43:45.471169  5076 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 09:43:51.839633  5076 solver.cpp:218] Iteration 95900 (15.7032 iter/s, 6.36812s/100 iters), loss = 0.503133
I1211 09:43:51.839633  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:43:51.839633  5076 solver.cpp:237]     Train net output #1: loss = 0.503133 (* 1 = 0.503133 loss)
I1211 09:43:51.839633  5076 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 09:43:57.894361 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:43:58.146376  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.caffemodel
I1211 09:43:58.161376  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.solverstate
I1211 09:43:58.166380  5076 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 09:43:58.166380  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:43:59.691491  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:43:59.752493  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1211 09:43:59.752493  5076 solver.cpp:397]     Test net output #1: loss = 1.18948 (* 1 = 1.18948 loss)
I1211 09:43:59.812494  5076 solver.cpp:218] Iteration 96000 (12.5425 iter/s, 7.9729s/100 iters), loss = 0.432123
I1211 09:43:59.812494  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:43:59.812494  5076 solver.cpp:237]     Train net output #1: loss = 0.432123 (* 1 = 0.432123 loss)
I1211 09:43:59.812494  5076 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 09:44:06.209982  5076 solver.cpp:218] Iteration 96100 (15.6332 iter/s, 6.39665s/100 iters), loss = 0.52665
I1211 09:44:06.209982  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:44:06.209982  5076 solver.cpp:237]     Train net output #1: loss = 0.52665 (* 1 = 0.52665 loss)
I1211 09:44:06.209982  5076 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 09:44:12.614486  5076 solver.cpp:218] Iteration 96200 (15.6139 iter/s, 6.40455s/100 iters), loss = 0.37133
I1211 09:44:12.614486  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:44:12.614486  5076 solver.cpp:237]     Train net output #1: loss = 0.37133 (* 1 = 0.37133 loss)
I1211 09:44:12.614486  5076 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 09:44:19.054466  5076 solver.cpp:218] Iteration 96300 (15.5308 iter/s, 6.43882s/100 iters), loss = 0.485709
I1211 09:44:19.054966  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:44:19.054966  5076 solver.cpp:237]     Train net output #1: loss = 0.485709 (* 1 = 0.485709 loss)
I1211 09:44:19.054966  5076 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 09:44:25.394500  5076 solver.cpp:218] Iteration 96400 (15.7741 iter/s, 6.33952s/100 iters), loss = 0.369812
I1211 09:44:25.394500  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:44:25.394500  5076 solver.cpp:237]     Train net output #1: loss = 0.369812 (* 1 = 0.369812 loss)
I1211 09:44:25.394500  5076 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 09:44:31.470082 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:44:31.725095  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.caffemodel
I1211 09:44:31.746095  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.solverstate
I1211 09:44:31.751600  5076 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 09:44:31.751600  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:44:33.290261  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:44:33.351279  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6754
I1211 09:44:33.351279  5076 solver.cpp:397]     Test net output #1: loss = 1.19582 (* 1 = 1.19582 loss)
I1211 09:44:33.412300  5076 solver.cpp:218] Iteration 96500 (12.4735 iter/s, 8.017s/100 iters), loss = 0.458218
I1211 09:44:33.412300  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:44:33.412300  5076 solver.cpp:237]     Train net output #1: loss = 0.458218 (* 1 = 0.458218 loss)
I1211 09:44:33.412300  5076 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 09:44:39.802845  5076 solver.cpp:218] Iteration 96600 (15.6476 iter/s, 6.39074s/100 iters), loss = 0.433565
I1211 09:44:39.802845  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:44:39.802845  5076 solver.cpp:237]     Train net output #1: loss = 0.433565 (* 1 = 0.433565 loss)
I1211 09:44:39.802845  5076 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 09:44:46.227339  5076 solver.cpp:218] Iteration 96700 (15.568 iter/s, 6.42341s/100 iters), loss = 0.355316
I1211 09:44:46.227339  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:44:46.227339  5076 solver.cpp:237]     Train net output #1: loss = 0.355316 (* 1 = 0.355316 loss)
I1211 09:44:46.227339  5076 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 09:44:52.743044  5076 solver.cpp:218] Iteration 96800 (15.3485 iter/s, 6.51531s/100 iters), loss = 0.53043
I1211 09:44:52.743044  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:44:52.743044  5076 solver.cpp:237]     Train net output #1: loss = 0.53043 (* 1 = 0.53043 loss)
I1211 09:44:52.743044  5076 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 09:44:59.296910  5076 solver.cpp:218] Iteration 96900 (15.2595 iter/s, 6.5533s/100 iters), loss = 0.445602
I1211 09:44:59.296910  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:44:59.296910  5076 solver.cpp:237]     Train net output #1: loss = 0.445602 (* 1 = 0.445602 loss)
I1211 09:44:59.296910  5076 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 09:45:05.450426 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:45:05.704426  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.caffemodel
I1211 09:45:05.720425  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.solverstate
I1211 09:45:05.724926  5076 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 09:45:05.725426  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:45:07.281925  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:45:07.342926  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6766
I1211 09:45:07.343426  5076 solver.cpp:397]     Test net output #1: loss = 1.18951 (* 1 = 1.18951 loss)
I1211 09:45:07.404925  5076 solver.cpp:218] Iteration 97000 (12.3337 iter/s, 8.10786s/100 iters), loss = 0.447407
I1211 09:45:07.405426  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:45:07.405426  5076 solver.cpp:237]     Train net output #1: loss = 0.447407 (* 1 = 0.447407 loss)
I1211 09:45:07.405426  5076 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 09:45:13.793424  5076 solver.cpp:218] Iteration 97100 (15.6544 iter/s, 6.38797s/100 iters), loss = 0.488413
I1211 09:45:13.793926  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:45:13.793926  5076 solver.cpp:237]     Train net output #1: loss = 0.488413 (* 1 = 0.488413 loss)
I1211 09:45:13.793926  5076 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 09:45:20.173924  5076 solver.cpp:218] Iteration 97200 (15.6749 iter/s, 6.37961s/100 iters), loss = 0.338073
I1211 09:45:20.173924  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:45:20.173924  5076 solver.cpp:237]     Train net output #1: loss = 0.338073 (* 1 = 0.338073 loss)
I1211 09:45:20.173924  5076 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 09:45:26.541522  5076 solver.cpp:218] Iteration 97300 (15.7048 iter/s, 6.36747s/100 iters), loss = 0.311613
I1211 09:45:26.541522  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:45:26.541522  5076 solver.cpp:237]     Train net output #1: loss = 0.311613 (* 1 = 0.311613 loss)
I1211 09:45:26.541522  5076 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 09:45:32.899850  5076 solver.cpp:218] Iteration 97400 (15.7285 iter/s, 6.3579s/100 iters), loss = 0.414347
I1211 09:45:32.899850  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:45:32.899850  5076 solver.cpp:237]     Train net output #1: loss = 0.414347 (* 1 = 0.414347 loss)
I1211 09:45:32.899850  5076 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 09:45:38.947854 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:45:39.198351  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.caffemodel
I1211 09:45:39.213851  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.solverstate
I1211 09:45:39.218852  5076 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 09:45:39.218852  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:45:40.741852  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:45:40.802354  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6788
I1211 09:45:40.802354  5076 solver.cpp:397]     Test net output #1: loss = 1.19089 (* 1 = 1.19089 loss)
I1211 09:45:40.864354  5076 solver.cpp:218] Iteration 97500 (12.5569 iter/s, 7.96374s/100 iters), loss = 0.444709
I1211 09:45:40.864354  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:45:40.864354  5076 solver.cpp:237]     Train net output #1: loss = 0.444709 (* 1 = 0.444709 loss)
I1211 09:45:40.864354  5076 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 09:45:47.218791  5076 solver.cpp:218] Iteration 97600 (15.7382 iter/s, 6.35395s/100 iters), loss = 0.44422
I1211 09:45:47.218791  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:45:47.218791  5076 solver.cpp:237]     Train net output #1: loss = 0.44422 (* 1 = 0.44422 loss)
I1211 09:45:47.218791  5076 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 09:45:53.588799  5076 solver.cpp:218] Iteration 97700 (15.6993 iter/s, 6.36973s/100 iters), loss = 0.360053
I1211 09:45:53.589299  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:45:53.589299  5076 solver.cpp:237]     Train net output #1: loss = 0.360053 (* 1 = 0.360053 loss)
I1211 09:45:53.589299  5076 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 09:45:59.949470  5076 solver.cpp:218] Iteration 97800 (15.7229 iter/s, 6.36016s/100 iters), loss = 0.409356
I1211 09:45:59.949470  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:45:59.949470  5076 solver.cpp:237]     Train net output #1: loss = 0.409356 (* 1 = 0.409356 loss)
I1211 09:45:59.949470  5076 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 09:46:06.312697  5076 solver.cpp:218] Iteration 97900 (15.7173 iter/s, 6.36243s/100 iters), loss = 0.400705
I1211 09:46:06.312697  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:46:06.312697  5076 solver.cpp:237]     Train net output #1: loss = 0.400705 (* 1 = 0.400705 loss)
I1211 09:46:06.312697  5076 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 09:46:12.365187 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:46:12.615686  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.caffemodel
I1211 09:46:12.631193  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.solverstate
I1211 09:46:12.636205  5076 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 09:46:12.636205  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:46:14.163699  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:46:14.224186  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6796
I1211 09:46:14.224186  5076 solver.cpp:397]     Test net output #1: loss = 1.19756 (* 1 = 1.19756 loss)
I1211 09:46:14.284685  5076 solver.cpp:218] Iteration 98000 (12.5446 iter/s, 7.97156s/100 iters), loss = 0.466292
I1211 09:46:14.284685  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:46:14.284685  5076 solver.cpp:237]     Train net output #1: loss = 0.466292 (* 1 = 0.466292 loss)
I1211 09:46:14.284685  5076 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 09:46:20.652092  5076 solver.cpp:218] Iteration 98100 (15.7066 iter/s, 6.36676s/100 iters), loss = 0.500463
I1211 09:46:20.652092  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:46:20.652092  5076 solver.cpp:237]     Train net output #1: loss = 0.500463 (* 1 = 0.500463 loss)
I1211 09:46:20.652092  5076 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 09:46:27.043006  5076 solver.cpp:218] Iteration 98200 (15.6475 iter/s, 6.39081s/100 iters), loss = 0.344298
I1211 09:46:27.043006  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:46:27.043006  5076 solver.cpp:237]     Train net output #1: loss = 0.344298 (* 1 = 0.344298 loss)
I1211 09:46:27.043006  5076 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 09:46:33.417850  5076 solver.cpp:218] Iteration 98300 (15.688 iter/s, 6.37429s/100 iters), loss = 0.362091
I1211 09:46:33.417850  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:46:33.417850  5076 solver.cpp:237]     Train net output #1: loss = 0.362091 (* 1 = 0.362091 loss)
I1211 09:46:33.417850  5076 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 09:46:39.793725  5076 solver.cpp:218] Iteration 98400 (15.6854 iter/s, 6.37536s/100 iters), loss = 0.529031
I1211 09:46:39.793725  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:46:39.793725  5076 solver.cpp:237]     Train net output #1: loss = 0.529031 (* 1 = 0.529031 loss)
I1211 09:46:39.793725  5076 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 09:46:45.844506 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:46:46.095018  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.caffemodel
I1211 09:46:46.111023  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.solverstate
I1211 09:46:46.116019  5076 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 09:46:46.116019  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:46:47.643003  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:46:47.704005  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6781
I1211 09:46:47.704005  5076 solver.cpp:397]     Test net output #1: loss = 1.19604 (* 1 = 1.19604 loss)
I1211 09:46:47.764506  5076 solver.cpp:218] Iteration 98500 (12.5464 iter/s, 7.9704s/100 iters), loss = 0.448277
I1211 09:46:47.765007  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:46:47.765007  5076 solver.cpp:237]     Train net output #1: loss = 0.448277 (* 1 = 0.448277 loss)
I1211 09:46:47.765007  5076 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 09:46:54.140445  5076 solver.cpp:218] Iteration 98600 (15.6854 iter/s, 6.37534s/100 iters), loss = 0.389377
I1211 09:46:54.140445  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:46:54.140445  5076 solver.cpp:237]     Train net output #1: loss = 0.389377 (* 1 = 0.389377 loss)
I1211 09:46:54.140445  5076 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 09:47:00.519245  5076 solver.cpp:218] Iteration 98700 (15.6788 iter/s, 6.37802s/100 iters), loss = 0.422746
I1211 09:47:00.519245  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:47:00.519245  5076 solver.cpp:237]     Train net output #1: loss = 0.422746 (* 1 = 0.422746 loss)
I1211 09:47:00.519245  5076 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 09:47:06.894388  5076 solver.cpp:218] Iteration 98800 (15.6865 iter/s, 6.37492s/100 iters), loss = 0.590047
I1211 09:47:06.894888  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 09:47:06.894888  5076 solver.cpp:237]     Train net output #1: loss = 0.590047 (* 1 = 0.590047 loss)
I1211 09:47:06.894888  5076 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 09:47:13.377770  5076 solver.cpp:218] Iteration 98900 (15.4261 iter/s, 6.48251s/100 iters), loss = 0.375774
I1211 09:47:13.377770  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:47:13.377770  5076 solver.cpp:237]     Train net output #1: loss = 0.375774 (* 1 = 0.375774 loss)
I1211 09:47:13.377770  5076 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 09:47:19.499794 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:47:19.753352  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.caffemodel
I1211 09:47:19.768353  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.solverstate
I1211 09:47:19.773353  5076 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 09:47:19.773353  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:47:21.319398  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:47:21.380381  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6817
I1211 09:47:21.380381  5076 solver.cpp:397]     Test net output #1: loss = 1.19963 (* 1 = 1.19963 loss)
I1211 09:47:21.442392  5076 solver.cpp:218] Iteration 99000 (12.4007 iter/s, 8.06406s/100 iters), loss = 0.408767
I1211 09:47:21.442392  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:47:21.442392  5076 solver.cpp:237]     Train net output #1: loss = 0.408767 (* 1 = 0.408767 loss)
I1211 09:47:21.442392  5076 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 09:47:27.893292  5076 solver.cpp:218] Iteration 99100 (15.5026 iter/s, 6.45055s/100 iters), loss = 0.404848
I1211 09:47:27.893292  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:47:27.893292  5076 solver.cpp:237]     Train net output #1: loss = 0.404848 (* 1 = 0.404848 loss)
I1211 09:47:27.893292  5076 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 09:47:34.345417  5076 solver.cpp:218] Iteration 99200 (15.4998 iter/s, 6.45169s/100 iters), loss = 0.286888
I1211 09:47:34.345417  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:47:34.345417  5076 solver.cpp:237]     Train net output #1: loss = 0.286888 (* 1 = 0.286888 loss)
I1211 09:47:34.345417  5076 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 09:47:40.788034  5076 solver.cpp:218] Iteration 99300 (15.5225 iter/s, 6.44225s/100 iters), loss = 0.311533
I1211 09:47:40.788034  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 09:47:40.788034  5076 solver.cpp:237]     Train net output #1: loss = 0.311533 (* 1 = 0.311533 loss)
I1211 09:47:40.788034  5076 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 09:47:47.310838  5076 solver.cpp:218] Iteration 99400 (15.3326 iter/s, 6.52205s/100 iters), loss = 0.447418
I1211 09:47:47.310838  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:47:47.310838  5076 solver.cpp:237]     Train net output #1: loss = 0.447418 (* 1 = 0.447418 loss)
I1211 09:47:47.310838  5076 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 09:47:53.461189 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:47:53.716188  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.caffemodel
I1211 09:47:53.732188  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.solverstate
I1211 09:47:53.738189  5076 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 09:47:53.738189  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:47:55.285688  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:47:55.347188  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6785
I1211 09:47:55.347188  5076 solver.cpp:397]     Test net output #1: loss = 1.20388 (* 1 = 1.20388 loss)
I1211 09:47:55.409188  5076 solver.cpp:218] Iteration 99500 (12.349 iter/s, 8.09782s/100 iters), loss = 0.436116
I1211 09:47:55.409188  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:47:55.409188  5076 solver.cpp:237]     Train net output #1: loss = 0.436116 (* 1 = 0.436116 loss)
I1211 09:47:55.409188  5076 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 09:48:01.873639  5076 solver.cpp:218] Iteration 99600 (15.4698 iter/s, 6.46419s/100 iters), loss = 0.407284
I1211 09:48:01.873639  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:48:01.873639  5076 solver.cpp:237]     Train net output #1: loss = 0.407284 (* 1 = 0.407284 loss)
I1211 09:48:01.873639  5076 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 09:48:08.349526  5076 solver.cpp:218] Iteration 99700 (15.4439 iter/s, 6.47507s/100 iters), loss = 0.38998
I1211 09:48:08.349526  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:48:08.349526  5076 solver.cpp:237]     Train net output #1: loss = 0.38998 (* 1 = 0.38998 loss)
I1211 09:48:08.349526  5076 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 09:48:14.825794  5076 solver.cpp:218] Iteration 99800 (15.442 iter/s, 6.47584s/100 iters), loss = 0.337164
I1211 09:48:14.825794  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:48:14.825794  5076 solver.cpp:237]     Train net output #1: loss = 0.337164 (* 1 = 0.337164 loss)
I1211 09:48:14.825794  5076 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 09:48:21.291833  5076 solver.cpp:218] Iteration 99900 (15.4659 iter/s, 6.46584s/100 iters), loss = 0.403605
I1211 09:48:21.291833  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:48:21.292333  5076 solver.cpp:237]     Train net output #1: loss = 0.403605 (* 1 = 0.403605 loss)
I1211 09:48:21.292333  5076 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 09:48:27.448895 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:48:27.706027  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.caffemodel
I1211 09:48:27.725528  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.solverstate
I1211 09:48:27.730528  5076 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 09:48:27.730528  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:48:29.288758  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:48:29.351256  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6823
I1211 09:48:29.351256  5076 solver.cpp:397]     Test net output #1: loss = 1.20125 (* 1 = 1.20125 loss)
I1211 09:48:29.414257  5076 solver.cpp:218] Iteration 100000 (12.3132 iter/s, 8.12139s/100 iters), loss = 0.387077
I1211 09:48:29.414257  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:48:29.414257  5076 solver.cpp:237]     Train net output #1: loss = 0.387077 (* 1 = 0.387077 loss)
I1211 09:48:29.414257  5076 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 09:48:35.906975  5076 solver.cpp:218] Iteration 100100 (15.403 iter/s, 6.49224s/100 iters), loss = 0.388543
I1211 09:48:35.906975  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:48:35.906975  5076 solver.cpp:237]     Train net output #1: loss = 0.388543 (* 1 = 0.388543 loss)
I1211 09:48:35.906975  5076 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 09:48:42.360633  5076 solver.cpp:218] Iteration 100200 (15.4958 iter/s, 6.45334s/100 iters), loss = 0.378947
I1211 09:48:42.360633  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:48:42.360633  5076 solver.cpp:237]     Train net output #1: loss = 0.378947 (* 1 = 0.378947 loss)
I1211 09:48:42.360633  5076 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 09:48:48.804616  5076 solver.cpp:218] Iteration 100300 (15.5194 iter/s, 6.44356s/100 iters), loss = 0.366151
I1211 09:48:48.804616  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:48:48.804616  5076 solver.cpp:237]     Train net output #1: loss = 0.366151 (* 1 = 0.366151 loss)
I1211 09:48:48.804616  5076 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 09:48:55.310303  5076 solver.cpp:218] Iteration 100400 (15.3733 iter/s, 6.50479s/100 iters), loss = 0.390497
I1211 09:48:55.310303  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:48:55.310303  5076 solver.cpp:237]     Train net output #1: loss = 0.390497 (* 1 = 0.390497 loss)
I1211 09:48:55.310303  5076 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 09:49:01.502441 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:49:01.757938  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.caffemodel
I1211 09:49:01.774438  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.solverstate
I1211 09:49:01.779439  5076 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 09:49:01.779939  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:49:03.340441  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:49:03.400964  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6803
I1211 09:49:03.400964  5076 solver.cpp:397]     Test net output #1: loss = 1.21746 (* 1 = 1.21746 loss)
I1211 09:49:03.463454  5076 solver.cpp:218] Iteration 100500 (12.2659 iter/s, 8.15267s/100 iters), loss = 0.423059
I1211 09:49:03.463454  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:49:03.463454  5076 solver.cpp:237]     Train net output #1: loss = 0.423059 (* 1 = 0.423059 loss)
I1211 09:49:03.463454  5076 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 09:49:09.905365  5076 solver.cpp:218] Iteration 100600 (15.5247 iter/s, 6.44133s/100 iters), loss = 0.492812
I1211 09:49:09.905365  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:49:09.905365  5076 solver.cpp:237]     Train net output #1: loss = 0.492812 (* 1 = 0.492812 loss)
I1211 09:49:09.905365  5076 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 09:49:16.390931  5076 solver.cpp:218] Iteration 100700 (15.4195 iter/s, 6.48529s/100 iters), loss = 0.382846
I1211 09:49:16.390931  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:49:16.390931  5076 solver.cpp:237]     Train net output #1: loss = 0.382846 (* 1 = 0.382846 loss)
I1211 09:49:16.390931  5076 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 09:49:22.916030  5076 solver.cpp:218] Iteration 100800 (15.3268 iter/s, 6.52452s/100 iters), loss = 0.382101
I1211 09:49:22.916030  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:49:22.916030  5076 solver.cpp:237]     Train net output #1: loss = 0.382101 (* 1 = 0.382101 loss)
I1211 09:49:22.916030  5076 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 09:49:29.402737  5076 solver.cpp:218] Iteration 100900 (15.417 iter/s, 6.48635s/100 iters), loss = 0.370393
I1211 09:49:29.403236  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:49:29.403236  5076 solver.cpp:237]     Train net output #1: loss = 0.370393 (* 1 = 0.370393 loss)
I1211 09:49:29.403236  5076 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 09:49:35.570701 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:49:35.825201  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.caffemodel
I1211 09:49:35.841202  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.solverstate
I1211 09:49:35.846204  5076 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 09:49:35.846204  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:49:37.402253  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:49:37.463253  5076 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 09:49:37.463253  5076 solver.cpp:397]     Test net output #1: loss = 1.21866 (* 1 = 1.21866 loss)
I1211 09:49:37.526252  5076 solver.cpp:218] Iteration 101000 (12.3109 iter/s, 8.12285s/100 iters), loss = 0.370335
I1211 09:49:37.526252  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 09:49:37.526252  5076 solver.cpp:237]     Train net output #1: loss = 0.370335 (* 1 = 0.370335 loss)
I1211 09:49:37.526252  5076 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 09:49:44.035848  5076 solver.cpp:218] Iteration 101100 (15.3634 iter/s, 6.50899s/100 iters), loss = 0.331381
I1211 09:49:44.035848  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:49:44.035848  5076 solver.cpp:237]     Train net output #1: loss = 0.331381 (* 1 = 0.331381 loss)
I1211 09:49:44.035848  5076 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 09:49:50.580664  5076 solver.cpp:218] Iteration 101200 (15.2804 iter/s, 6.54434s/100 iters), loss = 0.293754
I1211 09:49:50.580664  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:49:50.580664  5076 solver.cpp:237]     Train net output #1: loss = 0.293754 (* 1 = 0.293754 loss)
I1211 09:49:50.580664  5076 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 09:49:57.062119  5076 solver.cpp:218] Iteration 101300 (15.4306 iter/s, 6.48061s/100 iters), loss = 0.443838
I1211 09:49:57.062119  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:49:57.062119  5076 solver.cpp:237]     Train net output #1: loss = 0.443838 (* 1 = 0.443838 loss)
I1211 09:49:57.062119  5076 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 09:50:03.552501  5076 solver.cpp:218] Iteration 101400 (15.4084 iter/s, 6.48996s/100 iters), loss = 0.357211
I1211 09:50:03.552501  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:50:03.552501  5076 solver.cpp:237]     Train net output #1: loss = 0.357211 (* 1 = 0.357211 loss)
I1211 09:50:03.552501  5076 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 09:50:09.708474 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:50:09.963973  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.caffemodel
I1211 09:50:09.979473  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.solverstate
I1211 09:50:09.984474  5076 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 09:50:09.984474  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:50:11.532974  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:50:11.593977  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6786
I1211 09:50:11.593977  5076 solver.cpp:397]     Test net output #1: loss = 1.21398 (* 1 = 1.21398 loss)
I1211 09:50:11.655505  5076 solver.cpp:218] Iteration 101500 (12.3415 iter/s, 8.10274s/100 iters), loss = 0.434366
I1211 09:50:11.655505  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:50:11.655505  5076 solver.cpp:237]     Train net output #1: loss = 0.434366 (* 1 = 0.434366 loss)
I1211 09:50:11.655505  5076 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 09:50:18.123036  5076 solver.cpp:218] Iteration 101600 (15.4634 iter/s, 6.46687s/100 iters), loss = 0.414648
I1211 09:50:18.123036  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:50:18.123036  5076 solver.cpp:237]     Train net output #1: loss = 0.414648 (* 1 = 0.414648 loss)
I1211 09:50:18.123036  5076 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 09:50:24.631556  5076 solver.cpp:218] Iteration 101700 (15.3657 iter/s, 6.508s/100 iters), loss = 0.371057
I1211 09:50:24.631556  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:50:24.631556  5076 solver.cpp:237]     Train net output #1: loss = 0.371057 (* 1 = 0.371057 loss)
I1211 09:50:24.631556  5076 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 09:50:31.125569  5076 solver.cpp:218] Iteration 101800 (15.4005 iter/s, 6.49329s/100 iters), loss = 0.36232
I1211 09:50:31.125569  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:50:31.125569  5076 solver.cpp:237]     Train net output #1: loss = 0.36232 (* 1 = 0.36232 loss)
I1211 09:50:31.125569  5076 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 09:50:37.575834  5076 solver.cpp:218] Iteration 101900 (15.5042 iter/s, 6.44986s/100 iters), loss = 0.446124
I1211 09:50:37.575834  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:50:37.575834  5076 solver.cpp:237]     Train net output #1: loss = 0.446124 (* 1 = 0.446124 loss)
I1211 09:50:37.575834  5076 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 09:50:43.854132 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:50:44.109632  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.caffemodel
I1211 09:50:44.126632  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.solverstate
I1211 09:50:44.131633  5076 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 09:50:44.131633  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:50:45.695646  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:50:45.758643  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 09:50:45.758643  5076 solver.cpp:397]     Test net output #1: loss = 1.21655 (* 1 = 1.21655 loss)
I1211 09:50:45.822142  5076 solver.cpp:218] Iteration 102000 (12.1278 iter/s, 8.2455s/100 iters), loss = 0.366883
I1211 09:50:45.822142  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:50:45.822142  5076 solver.cpp:237]     Train net output #1: loss = 0.366883 (* 1 = 0.366883 loss)
I1211 09:50:45.822142  5076 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 09:50:52.220175  5076 solver.cpp:218] Iteration 102100 (15.6307 iter/s, 6.39768s/100 iters), loss = 0.442861
I1211 09:50:52.220175  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:50:52.220175  5076 solver.cpp:237]     Train net output #1: loss = 0.442861 (* 1 = 0.442861 loss)
I1211 09:50:52.220175  5076 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 09:50:58.610952  5076 solver.cpp:218] Iteration 102200 (15.6484 iter/s, 6.39042s/100 iters), loss = 0.326367
I1211 09:50:58.610952  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:50:58.611452  5076 solver.cpp:237]     Train net output #1: loss = 0.326367 (* 1 = 0.326367 loss)
I1211 09:50:58.611452  5076 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 09:51:05.070173  5076 solver.cpp:218] Iteration 102300 (15.4835 iter/s, 6.4585s/100 iters), loss = 0.40486
I1211 09:51:05.070173  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:51:05.070173  5076 solver.cpp:237]     Train net output #1: loss = 0.404861 (* 1 = 0.404861 loss)
I1211 09:51:05.070173  5076 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 09:51:11.538286  5076 solver.cpp:218] Iteration 102400 (15.4615 iter/s, 6.46767s/100 iters), loss = 0.416761
I1211 09:51:11.538286  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:51:11.538286  5076 solver.cpp:237]     Train net output #1: loss = 0.416761 (* 1 = 0.416761 loss)
I1211 09:51:11.538286  5076 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 09:51:17.708588 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:51:17.966147  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.caffemodel
I1211 09:51:17.981648  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.solverstate
I1211 09:51:17.987149  5076 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 09:51:17.987149  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:51:19.543145  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:51:19.605146  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1211 09:51:19.605146  5076 solver.cpp:397]     Test net output #1: loss = 1.21678 (* 1 = 1.21678 loss)
I1211 09:51:19.666656  5076 solver.cpp:218] Iteration 102500 (12.3032 iter/s, 8.12794s/100 iters), loss = 0.386397
I1211 09:51:19.667160  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:51:19.667160  5076 solver.cpp:237]     Train net output #1: loss = 0.386397 (* 1 = 0.386397 loss)
I1211 09:51:19.667160  5076 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 09:51:26.172302  5076 solver.cpp:218] Iteration 102600 (15.3735 iter/s, 6.50469s/100 iters), loss = 0.444472
I1211 09:51:26.172302  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:51:26.172302  5076 solver.cpp:237]     Train net output #1: loss = 0.444473 (* 1 = 0.444473 loss)
I1211 09:51:26.172302  5076 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 09:51:32.703600  5076 solver.cpp:218] Iteration 102700 (15.3114 iter/s, 6.53106s/100 iters), loss = 0.324761
I1211 09:51:32.704100  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:51:32.704100  5076 solver.cpp:237]     Train net output #1: loss = 0.324761 (* 1 = 0.324761 loss)
I1211 09:51:32.704100  5076 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 09:51:39.179692  5076 solver.cpp:218] Iteration 102800 (15.4433 iter/s, 6.47529s/100 iters), loss = 0.365428
I1211 09:51:39.179692  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:51:39.179692  5076 solver.cpp:237]     Train net output #1: loss = 0.365428 (* 1 = 0.365428 loss)
I1211 09:51:39.179692  5076 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 09:51:45.659885  5076 solver.cpp:218] Iteration 102900 (15.4325 iter/s, 6.47981s/100 iters), loss = 0.43248
I1211 09:51:45.659885  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:51:45.659885  5076 solver.cpp:237]     Train net output #1: loss = 0.43248 (* 1 = 0.43248 loss)
I1211 09:51:45.659885  5076 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 09:51:51.840358 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:51:52.095858  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.caffemodel
I1211 09:51:52.110859  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.solverstate
I1211 09:51:52.115857  5076 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 09:51:52.115857  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:51:53.671857  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:51:53.733357  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6793
I1211 09:51:53.733357  5076 solver.cpp:397]     Test net output #1: loss = 1.22389 (* 1 = 1.22389 loss)
I1211 09:51:53.794858  5076 solver.cpp:218] Iteration 103000 (12.2934 iter/s, 8.13444s/100 iters), loss = 0.41508
I1211 09:51:53.794858  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:51:53.794858  5076 solver.cpp:237]     Train net output #1: loss = 0.41508 (* 1 = 0.41508 loss)
I1211 09:51:53.794858  5076 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 09:52:00.305003  5076 solver.cpp:218] Iteration 103100 (15.362 iter/s, 6.50956s/100 iters), loss = 0.486723
I1211 09:52:00.305003  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:52:00.305003  5076 solver.cpp:237]     Train net output #1: loss = 0.486724 (* 1 = 0.486724 loss)
I1211 09:52:00.305003  5076 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 09:52:06.790494  5076 solver.cpp:218] Iteration 103200 (15.4195 iter/s, 6.4853s/100 iters), loss = 0.293823
I1211 09:52:06.790994  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 09:52:06.790994  5076 solver.cpp:237]     Train net output #1: loss = 0.293823 (* 1 = 0.293823 loss)
I1211 09:52:06.790994  5076 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 09:52:13.285869  5076 solver.cpp:218] Iteration 103300 (15.3974 iter/s, 6.49461s/100 iters), loss = 0.335064
I1211 09:52:13.285869  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:52:13.285869  5076 solver.cpp:237]     Train net output #1: loss = 0.335065 (* 1 = 0.335065 loss)
I1211 09:52:13.285869  5076 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 09:52:19.784440  5076 solver.cpp:218] Iteration 103400 (15.3885 iter/s, 6.49837s/100 iters), loss = 0.39785
I1211 09:52:19.784948  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:52:19.784948  5076 solver.cpp:237]     Train net output #1: loss = 0.39785 (* 1 = 0.39785 loss)
I1211 09:52:19.784948  5076 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 09:52:25.923570 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:52:26.179082  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.caffemodel
I1211 09:52:26.195571  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.solverstate
I1211 09:52:26.200582  5076 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 09:52:26.200582  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:52:27.749567  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:52:27.810587  5076 solver.cpp:397]     Test net output #0: accuracy = 0.676
I1211 09:52:27.810587  5076 solver.cpp:397]     Test net output #1: loss = 1.22894 (* 1 = 1.22894 loss)
I1211 09:52:27.872068  5076 solver.cpp:218] Iteration 103500 (12.3659 iter/s, 8.08672s/100 iters), loss = 0.44019
I1211 09:52:27.872068  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:52:27.872068  5076 solver.cpp:237]     Train net output #1: loss = 0.440191 (* 1 = 0.440191 loss)
I1211 09:52:27.872068  5076 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 09:52:34.338982  5076 solver.cpp:218] Iteration 103600 (15.4642 iter/s, 6.46655s/100 iters), loss = 0.408298
I1211 09:52:34.338982  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:52:34.338982  5076 solver.cpp:237]     Train net output #1: loss = 0.408298 (* 1 = 0.408298 loss)
I1211 09:52:34.338982  5076 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 09:52:40.799043  5076 solver.cpp:218] Iteration 103700 (15.4808 iter/s, 6.4596s/100 iters), loss = 0.335871
I1211 09:52:40.799043  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:52:40.799043  5076 solver.cpp:237]     Train net output #1: loss = 0.335871 (* 1 = 0.335871 loss)
I1211 09:52:40.799043  5076 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 09:52:47.262339  5076 solver.cpp:218] Iteration 103800 (15.4735 iter/s, 6.46266s/100 iters), loss = 0.346346
I1211 09:52:47.262339  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:52:47.262339  5076 solver.cpp:237]     Train net output #1: loss = 0.346346 (* 1 = 0.346346 loss)
I1211 09:52:47.262339  5076 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 09:52:53.723776  5076 solver.cpp:218] Iteration 103900 (15.4781 iter/s, 6.46076s/100 iters), loss = 0.394841
I1211 09:52:53.723776  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:52:53.723776  5076 solver.cpp:237]     Train net output #1: loss = 0.394841 (* 1 = 0.394841 loss)
I1211 09:52:53.723776  5076 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 09:52:59.870775 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:53:00.123273  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.caffemodel
I1211 09:53:00.138273  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.solverstate
I1211 09:53:00.142774  5076 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 09:53:00.142774  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:53:01.697773  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:53:01.758774  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 09:53:01.758774  5076 solver.cpp:397]     Test net output #1: loss = 1.22853 (* 1 = 1.22853 loss)
I1211 09:53:01.821291  5076 solver.cpp:218] Iteration 104000 (12.3503 iter/s, 8.09698s/100 iters), loss = 0.458509
I1211 09:53:01.821291  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:53:01.821291  5076 solver.cpp:237]     Train net output #1: loss = 0.458509 (* 1 = 0.458509 loss)
I1211 09:53:01.821291  5076 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 09:53:08.278178  5076 solver.cpp:218] Iteration 104100 (15.4885 iter/s, 6.45641s/100 iters), loss = 0.379425
I1211 09:53:08.278178  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:53:08.278178  5076 solver.cpp:237]     Train net output #1: loss = 0.379425 (* 1 = 0.379425 loss)
I1211 09:53:08.278178  5076 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 09:53:14.729734  5076 solver.cpp:218] Iteration 104200 (15.5008 iter/s, 6.45127s/100 iters), loss = 0.259018
I1211 09:53:14.729734  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 09:53:14.730242  5076 solver.cpp:237]     Train net output #1: loss = 0.259018 (* 1 = 0.259018 loss)
I1211 09:53:14.730242  5076 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 09:53:21.192190  5076 solver.cpp:218] Iteration 104300 (15.4759 iter/s, 6.46166s/100 iters), loss = 0.301742
I1211 09:53:21.192190  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:53:21.192190  5076 solver.cpp:237]     Train net output #1: loss = 0.301742 (* 1 = 0.301742 loss)
I1211 09:53:21.192190  5076 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 09:53:27.643241  5076 solver.cpp:218] Iteration 104400 (15.5024 iter/s, 6.45059s/100 iters), loss = 0.390862
I1211 09:53:27.643241  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:53:27.643241  5076 solver.cpp:237]     Train net output #1: loss = 0.390862 (* 1 = 0.390862 loss)
I1211 09:53:27.643241  5076 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 09:53:33.813127 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:53:34.068648  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.caffemodel
I1211 09:53:34.085651  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.solverstate
I1211 09:53:34.090653  5076 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 09:53:34.090653  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:53:35.640187  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:53:35.702184  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 09:53:35.702694  5076 solver.cpp:397]     Test net output #1: loss = 1.23543 (* 1 = 1.23543 loss)
I1211 09:53:35.765184  5076 solver.cpp:218] Iteration 104500 (12.313 iter/s, 8.12152s/100 iters), loss = 0.360248
I1211 09:53:35.765687  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:53:35.765687  5076 solver.cpp:237]     Train net output #1: loss = 0.360248 (* 1 = 0.360248 loss)
I1211 09:53:35.765687  5076 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 09:53:42.283046  5076 solver.cpp:218] Iteration 104600 (15.3442 iter/s, 6.51711s/100 iters), loss = 0.464639
I1211 09:53:42.283046  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:53:42.283046  5076 solver.cpp:237]     Train net output #1: loss = 0.464639 (* 1 = 0.464639 loss)
I1211 09:53:42.283046  5076 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 09:53:48.764047  5076 solver.cpp:218] Iteration 104700 (15.4308 iter/s, 6.48055s/100 iters), loss = 0.307585
I1211 09:53:48.764549  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:53:48.764549  5076 solver.cpp:237]     Train net output #1: loss = 0.307585 (* 1 = 0.307585 loss)
I1211 09:53:48.764549  5076 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 09:53:55.232537  5076 solver.cpp:218] Iteration 104800 (15.461 iter/s, 6.46788s/100 iters), loss = 0.387238
I1211 09:53:55.233037  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 09:53:55.233037  5076 solver.cpp:237]     Train net output #1: loss = 0.387238 (* 1 = 0.387238 loss)
I1211 09:53:55.233037  5076 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 09:54:01.713493  5076 solver.cpp:218] Iteration 104900 (15.4313 iter/s, 6.48035s/100 iters), loss = 0.323894
I1211 09:54:01.713493  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:54:01.713493  5076 solver.cpp:237]     Train net output #1: loss = 0.323895 (* 1 = 0.323895 loss)
I1211 09:54:01.713493  5076 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 09:54:07.876933 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:54:08.134359  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.caffemodel
I1211 09:54:08.151861  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.solverstate
I1211 09:54:08.156862  5076 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 09:54:08.156862  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:54:09.712359  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:54:09.773358  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 09:54:09.773859  5076 solver.cpp:397]     Test net output #1: loss = 1.23133 (* 1 = 1.23133 loss)
I1211 09:54:09.835358  5076 solver.cpp:218] Iteration 105000 (12.3135 iter/s, 8.12114s/100 iters), loss = 0.334767
I1211 09:54:09.835358  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 09:54:09.835358  5076 solver.cpp:237]     Train net output #1: loss = 0.334768 (* 1 = 0.334768 loss)
I1211 09:54:09.835358  5076 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 09:54:16.342294  5076 solver.cpp:218] Iteration 105100 (15.3702 iter/s, 6.5061s/100 iters), loss = 0.285855
I1211 09:54:16.342294  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:54:16.342294  5076 solver.cpp:237]     Train net output #1: loss = 0.285855 (* 1 = 0.285855 loss)
I1211 09:54:16.342294  5076 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 09:54:22.819197  5076 solver.cpp:218] Iteration 105200 (15.4405 iter/s, 6.47648s/100 iters), loss = 0.293899
I1211 09:54:22.819197  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 09:54:22.819197  5076 solver.cpp:237]     Train net output #1: loss = 0.293899 (* 1 = 0.293899 loss)
I1211 09:54:22.819197  5076 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 09:54:29.311205  5076 solver.cpp:218] Iteration 105300 (15.4043 iter/s, 6.49169s/100 iters), loss = 0.313085
I1211 09:54:29.311705  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:54:29.311705  5076 solver.cpp:237]     Train net output #1: loss = 0.313085 (* 1 = 0.313085 loss)
I1211 09:54:29.311705  5076 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 09:54:35.825863  5076 solver.cpp:218] Iteration 105400 (15.352 iter/s, 6.51383s/100 iters), loss = 0.343421
I1211 09:54:35.825863  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:54:35.825863  5076 solver.cpp:237]     Train net output #1: loss = 0.343421 (* 1 = 0.343421 loss)
I1211 09:54:35.825863  5076 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 09:54:42.022052 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:54:42.272052  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.caffemodel
I1211 09:54:42.287551  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.solverstate
I1211 09:54:42.292549  5076 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 09:54:42.292549  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:54:43.822574  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:54:43.885074  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6756
I1211 09:54:43.885074  5076 solver.cpp:397]     Test net output #1: loss = 1.23728 (* 1 = 1.23728 loss)
I1211 09:54:43.946089  5076 solver.cpp:218] Iteration 105500 (12.3156 iter/s, 8.11981s/100 iters), loss = 0.438442
I1211 09:54:43.946089  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:54:43.946089  5076 solver.cpp:237]     Train net output #1: loss = 0.438442 (* 1 = 0.438442 loss)
I1211 09:54:43.946089  5076 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 09:54:50.502223  5076 solver.cpp:218] Iteration 105600 (15.2545 iter/s, 6.55545s/100 iters), loss = 0.363608
I1211 09:54:50.502223  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 09:54:50.502223  5076 solver.cpp:237]     Train net output #1: loss = 0.363608 (* 1 = 0.363608 loss)
I1211 09:54:50.502223  5076 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 09:54:57.024365  5076 solver.cpp:218] Iteration 105700 (15.3329 iter/s, 6.52194s/100 iters), loss = 0.3005
I1211 09:54:57.024365  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:54:57.024365  5076 solver.cpp:237]     Train net output #1: loss = 0.3005 (* 1 = 0.3005 loss)
I1211 09:54:57.024365  5076 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 09:55:03.619196  5076 solver.cpp:218] Iteration 105800 (15.1651 iter/s, 6.59407s/100 iters), loss = 0.372375
I1211 09:55:03.619196  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 09:55:03.619196  5076 solver.cpp:237]     Train net output #1: loss = 0.372375 (* 1 = 0.372375 loss)
I1211 09:55:03.619196  5076 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 09:55:10.064899  5076 solver.cpp:218] Iteration 105900 (15.5151 iter/s, 6.44533s/100 iters), loss = 0.353411
I1211 09:55:10.064899  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 09:55:10.064899  5076 solver.cpp:237]     Train net output #1: loss = 0.353411 (* 1 = 0.353411 loss)
I1211 09:55:10.064899  5076 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 09:55:16.281599 10680 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:55:16.540621  5076 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.caffemodel
I1211 09:55:16.556620  5076 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.solverstate
I1211 09:55:16.561622  5076 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 09:55:16.561622  5076 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:55:18.139147  6380 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:55:18.200146  5076 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 09:55:18.200146  5076 solver.cpp:397]     Test net output #1: loss = 1.24813 (* 1 = 1.24813 loss)
I1211 09:55:18.261656  5076 solver.cpp:218] Iteration 106000 (12.2007 iter/s, 8.19625s/100 iters), loss = 0.429953
I1211 09:55:18.261656  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:55:18.261656  5076 solver.cpp:237]     Train net output #1: loss = 0.429953 (* 1 = 0.429953 loss)
I1211 09:55:18.261656  5076 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 09:55:24.708700  5076 solver.cpp:218] Iteration 106100 (15.5129 iter/s, 6.44627s/100 iters), loss = 0.375896
I1211 09:55:24.708700  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:55:24.708700  5076 solver.cpp:237]     Train net output #1: loss = 0.375896 (* 1 = 0.375896 loss)
I1211 09:55:24.708700  5076 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 09:55:31.219477  5076 solver.cpp:218] Iteration 106200 (15.3601 iter/s, 6.51036s/100 iters), loss = 0.263644
I1211 09:55:31.219477  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 09:55:31.219477  5076 solver.cpp:237]     Train net output #1: loss = 0.263644 (* 1 = 0.263644 loss)
I1211 09:55:31.219477  5076 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 09:55:37.750018  5076 solver.cpp:218] Iteration 106300 (15.3136 iter/s, 6.53014s/100 iters), loss = 0.397262
I1211 09:55:37.750018  5076 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 09:55:37.750018  5076 solver.cpp:237]     Train net output #1: loss = 0.397262 (* 1 = 0.397262 loss)
I1211 09:55:37.750528  5076 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 09:55:44.234499  5076 solver.cpp:218] Iteration 106400 (15.4236 iter/s, 6.48356s/100 iters), loss = 0.409019
I1211 09:55:44.234499  5076 solver.cpp:237]     Train net output #0: accuracy_tr
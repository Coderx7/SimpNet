
G:\Caffe\examples\cifar100>REM go to the caffe root 

G:\Caffe\examples\cifar100>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/cifar100/fcifar100_full_relu_solver_bn.prototxt --snapshot=examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate 
I1211 09:56:03.109985  6644 caffe.cpp:219] Using GPUs 0
I1211 09:56:03.290989  6644 caffe.cpp:224] GPU 0: GeForce GTX 1080
I1211 09:56:03.578976  6644 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:56:03.595475  6644 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 500
snapshot_prefix: "examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin"
solver_mode: GPU
device_id: 0
random_seed: 786
net: "examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 50000
stepvalue: 95000
stepvalue: 153000
stepvalue: 198000
stepvalue: 223000
stepvalue: 270000
type: "AdaDelta"
I1211 09:56:03.596475  6644 solver.cpp:87] Creating training net from net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:56:03.596976  6644 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:56:03.596976  6644 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added1
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3_1
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_added2
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv11
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn_conv12
I1211 09:56:03.596976  6644 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1211 09:56:03.597476  6644 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_train_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 09:56:03.625016  6644 layer_factory.cpp:58] Creating layer cifar
I1211 09:56:03.627506  6644 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_train_leveldb_padding
I1211 09:56:03.628005  6644 net.cpp:84] Creating Layer cifar
I1211 09:56:03.628005  6644 net.cpp:380] cifar -> data
I1211 09:56:03.628005  6644 net.cpp:380] cifar -> label
I1211 09:56:03.628505  6644 data_layer.cpp:45] output data size: 100,3,32,32
I1211 09:56:03.634521  6644 net.cpp:122] Setting up cifar
I1211 09:56:03.634521  6644 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 09:56:03.634521  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.634521  6644 net.cpp:137] Memory required for data: 1229200
I1211 09:56:03.635006  6644 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 09:56:03.635006  6644 net.cpp:84] Creating Layer label_cifar_1_split
I1211 09:56:03.635006  6644 net.cpp:406] label_cifar_1_split <- label
I1211 09:56:03.635006  6644 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 09:56:03.635006  6644 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 09:56:03.635006  6644 net.cpp:122] Setting up label_cifar_1_split
I1211 09:56:03.635006  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.635006  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.635006  6644 net.cpp:137] Memory required for data: 1230000
I1211 09:56:03.635006  6644 layer_factory.cpp:58] Creating layer conv1
I1211 09:56:03.635006  6644 net.cpp:84] Creating Layer conv1
I1211 09:56:03.635006  6644 net.cpp:406] conv1 <- data
I1211 09:56:03.635006  6644 net.cpp:380] conv1 -> conv1
I1211 09:56:03.637014  2860 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:56:03.892006  6644 net.cpp:122] Setting up conv1
I1211 09:56:03.892006  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.892006  6644 net.cpp:137] Memory required for data: 13518000
I1211 09:56:03.892006  6644 layer_factory.cpp:58] Creating layer bn1
I1211 09:56:03.892006  6644 net.cpp:84] Creating Layer bn1
I1211 09:56:03.892006  6644 net.cpp:406] bn1 <- conv1
I1211 09:56:03.892006  6644 net.cpp:367] bn1 -> conv1 (in-place)
I1211 09:56:03.892006  6644 net.cpp:122] Setting up bn1
I1211 09:56:03.892006  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.892006  6644 net.cpp:137] Memory required for data: 25806000
I1211 09:56:03.892006  6644 layer_factory.cpp:58] Creating layer scale1
I1211 09:56:03.892006  6644 net.cpp:84] Creating Layer scale1
I1211 09:56:03.892006  6644 net.cpp:406] scale1 <- conv1
I1211 09:56:03.892006  6644 net.cpp:367] scale1 -> conv1 (in-place)
I1211 09:56:03.892506  6644 layer_factory.cpp:58] Creating layer scale1
I1211 09:56:03.892506  6644 net.cpp:122] Setting up scale1
I1211 09:56:03.892506  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.892506  6644 net.cpp:137] Memory required for data: 38094000
I1211 09:56:03.892506  6644 layer_factory.cpp:58] Creating layer relu1
I1211 09:56:03.892506  6644 net.cpp:84] Creating Layer relu1
I1211 09:56:03.892506  6644 net.cpp:406] relu1 <- conv1
I1211 09:56:03.892506  6644 net.cpp:367] relu1 -> conv1 (in-place)
I1211 09:56:03.892506  6644 net.cpp:122] Setting up relu1
I1211 09:56:03.893007  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.893007  6644 net.cpp:137] Memory required for data: 50382000
I1211 09:56:03.893007  6644 layer_factory.cpp:58] Creating layer conv1_0
I1211 09:56:03.893007  6644 net.cpp:84] Creating Layer conv1_0
I1211 09:56:03.893007  6644 net.cpp:406] conv1_0 <- conv1
I1211 09:56:03.893007  6644 net.cpp:380] conv1_0 -> conv1_0
I1211 09:56:03.894506  6644 net.cpp:122] Setting up conv1_0
I1211 09:56:03.894506  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.894506  6644 net.cpp:137] Memory required for data: 66766000
I1211 09:56:03.894506  6644 layer_factory.cpp:58] Creating layer bn1_0
I1211 09:56:03.894506  6644 net.cpp:84] Creating Layer bn1_0
I1211 09:56:03.894506  6644 net.cpp:406] bn1_0 <- conv1_0
I1211 09:56:03.894506  6644 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 09:56:03.894506  6644 net.cpp:122] Setting up bn1_0
I1211 09:56:03.894506  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.894506  6644 net.cpp:137] Memory required for data: 83150000
I1211 09:56:03.894506  6644 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:56:03.894506  6644 net.cpp:84] Creating Layer scale1_0
I1211 09:56:03.894506  6644 net.cpp:406] scale1_0 <- conv1_0
I1211 09:56:03.894506  6644 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 09:56:03.894506  6644 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:56:03.895006  6644 net.cpp:122] Setting up scale1_0
I1211 09:56:03.895006  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.895006  6644 net.cpp:137] Memory required for data: 99534000
I1211 09:56:03.895006  6644 layer_factory.cpp:58] Creating layer relu1_0
I1211 09:56:03.895006  6644 net.cpp:84] Creating Layer relu1_0
I1211 09:56:03.895006  6644 net.cpp:406] relu1_0 <- conv1_0
I1211 09:56:03.895006  6644 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 09:56:03.895006  6644 net.cpp:122] Setting up relu1_0
I1211 09:56:03.895006  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.895006  6644 net.cpp:137] Memory required for data: 115918000
I1211 09:56:03.895006  6644 layer_factory.cpp:58] Creating layer conv2
I1211 09:56:03.895006  6644 net.cpp:84] Creating Layer conv2
I1211 09:56:03.895006  6644 net.cpp:406] conv2 <- conv1_0
I1211 09:56:03.895006  6644 net.cpp:380] conv2 -> conv2
I1211 09:56:03.896006  6644 net.cpp:122] Setting up conv2
I1211 09:56:03.896006  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.896006  6644 net.cpp:137] Memory required for data: 132302000
I1211 09:56:03.896006  6644 layer_factory.cpp:58] Creating layer bn2
I1211 09:56:03.896006  6644 net.cpp:84] Creating Layer bn2
I1211 09:56:03.896006  6644 net.cpp:406] bn2 <- conv2
I1211 09:56:03.896006  6644 net.cpp:367] bn2 -> conv2 (in-place)
I1211 09:56:03.896507  6644 net.cpp:122] Setting up bn2
I1211 09:56:03.896507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.896507  6644 net.cpp:137] Memory required for data: 148686000
I1211 09:56:03.896507  6644 layer_factory.cpp:58] Creating layer scale2
I1211 09:56:03.896507  6644 net.cpp:84] Creating Layer scale2
I1211 09:56:03.896507  6644 net.cpp:406] scale2 <- conv2
I1211 09:56:03.896507  6644 net.cpp:367] scale2 -> conv2 (in-place)
I1211 09:56:03.896507  6644 layer_factory.cpp:58] Creating layer scale2
I1211 09:56:03.896507  6644 net.cpp:122] Setting up scale2
I1211 09:56:03.896507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.896507  6644 net.cpp:137] Memory required for data: 165070000
I1211 09:56:03.896507  6644 layer_factory.cpp:58] Creating layer relu2
I1211 09:56:03.896507  6644 net.cpp:84] Creating Layer relu2
I1211 09:56:03.896507  6644 net.cpp:406] relu2 <- conv2
I1211 09:56:03.896507  6644 net.cpp:367] relu2 -> conv2 (in-place)
I1211 09:56:03.896507  6644 net.cpp:122] Setting up relu2
I1211 09:56:03.896507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.896507  6644 net.cpp:137] Memory required for data: 181454000
I1211 09:56:03.896507  6644 layer_factory.cpp:58] Creating layer conv2_1
I1211 09:56:03.897006  6644 net.cpp:84] Creating Layer conv2_1
I1211 09:56:03.897006  6644 net.cpp:406] conv2_1 <- conv2
I1211 09:56:03.897006  6644 net.cpp:380] conv2_1 -> conv2_1
I1211 09:56:03.898007  6644 net.cpp:122] Setting up conv2_1
I1211 09:56:03.898007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.898007  6644 net.cpp:137] Memory required for data: 197838000
I1211 09:56:03.898007  6644 layer_factory.cpp:58] Creating layer bn2_1
I1211 09:56:03.898007  6644 net.cpp:84] Creating Layer bn2_1
I1211 09:56:03.898007  6644 net.cpp:406] bn2_1 <- conv2_1
I1211 09:56:03.898007  6644 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 09:56:03.898007  6644 net.cpp:122] Setting up bn2_1
I1211 09:56:03.898007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.898007  6644 net.cpp:137] Memory required for data: 214222000
I1211 09:56:03.898007  6644 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:56:03.898007  6644 net.cpp:84] Creating Layer scale2_1
I1211 09:56:03.898007  6644 net.cpp:406] scale2_1 <- conv2_1
I1211 09:56:03.898007  6644 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 09:56:03.898007  6644 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:56:03.898007  6644 net.cpp:122] Setting up scale2_1
I1211 09:56:03.898007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.898506  6644 net.cpp:137] Memory required for data: 230606000
I1211 09:56:03.898506  6644 layer_factory.cpp:58] Creating layer relu2_1
I1211 09:56:03.898506  6644 net.cpp:84] Creating Layer relu2_1
I1211 09:56:03.898506  6644 net.cpp:406] relu2_1 <- conv2_1
I1211 09:56:03.898506  6644 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 09:56:03.898506  6644 net.cpp:122] Setting up relu2_1
I1211 09:56:03.898506  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.898506  6644 net.cpp:137] Memory required for data: 246990000
I1211 09:56:03.898506  6644 layer_factory.cpp:58] Creating layer conv2_2
I1211 09:56:03.898506  6644 net.cpp:84] Creating Layer conv2_2
I1211 09:56:03.898506  6644 net.cpp:406] conv2_2 <- conv2_1
I1211 09:56:03.898506  6644 net.cpp:380] conv2_2 -> conv2_2
I1211 09:56:03.901006  6644 net.cpp:122] Setting up conv2_2
I1211 09:56:03.901006  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.901006  6644 net.cpp:137] Memory required for data: 267470000
I1211 09:56:03.901006  6644 layer_factory.cpp:58] Creating layer bn2_2
I1211 09:56:03.901006  6644 net.cpp:84] Creating Layer bn2_2
I1211 09:56:03.901006  6644 net.cpp:406] bn2_2 <- conv2_2
I1211 09:56:03.901006  6644 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 09:56:03.901506  6644 net.cpp:122] Setting up bn2_2
I1211 09:56:03.901506  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.901506  6644 net.cpp:137] Memory required for data: 287950000
I1211 09:56:03.901506  6644 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:56:03.901506  6644 net.cpp:84] Creating Layer scale2_2
I1211 09:56:03.901506  6644 net.cpp:406] scale2_2 <- conv2_2
I1211 09:56:03.901506  6644 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 09:56:03.901506  6644 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:56:03.901506  6644 net.cpp:122] Setting up scale2_2
I1211 09:56:03.901506  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.901506  6644 net.cpp:137] Memory required for data: 308430000
I1211 09:56:03.901506  6644 layer_factory.cpp:58] Creating layer relu2_2
I1211 09:56:03.901506  6644 net.cpp:84] Creating Layer relu2_2
I1211 09:56:03.901506  6644 net.cpp:406] relu2_2 <- conv2_2
I1211 09:56:03.901506  6644 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 09:56:03.902513  6644 net.cpp:122] Setting up relu2_2
I1211 09:56:03.902513  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.902513  6644 net.cpp:137] Memory required for data: 328910000
I1211 09:56:03.902513  6644 layer_factory.cpp:58] Creating layer newconv_added1
I1211 09:56:03.902513  6644 net.cpp:84] Creating Layer newconv_added1
I1211 09:56:03.902513  6644 net.cpp:406] newconv_added1 <- conv2_2
I1211 09:56:03.902513  6644 net.cpp:380] newconv_added1 -> newconv_added1
I1211 09:56:03.903520  6644 net.cpp:122] Setting up newconv_added1
I1211 09:56:03.903520  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.903520  6644 net.cpp:137] Memory required for data: 349390000
I1211 09:56:03.903520  6644 layer_factory.cpp:58] Creating layer bn_added1
I1211 09:56:03.903520  6644 net.cpp:84] Creating Layer bn_added1
I1211 09:56:03.903520  6644 net.cpp:406] bn_added1 <- newconv_added1
I1211 09:56:03.903520  6644 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 09:56:03.904021  6644 net.cpp:122] Setting up bn_added1
I1211 09:56:03.904021  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.904021  6644 net.cpp:137] Memory required for data: 369870000
I1211 09:56:03.904021  6644 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:56:03.904021  6644 net.cpp:84] Creating Layer scale_added1
I1211 09:56:03.904021  6644 net.cpp:406] scale_added1 <- newconv_added1
I1211 09:56:03.904021  6644 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 09:56:03.904021  6644 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:56:03.904021  6644 net.cpp:122] Setting up scale_added1
I1211 09:56:03.904021  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.904021  6644 net.cpp:137] Memory required for data: 390350000
I1211 09:56:03.904021  6644 layer_factory.cpp:58] Creating layer relu_added1
I1211 09:56:03.904021  6644 net.cpp:84] Creating Layer relu_added1
I1211 09:56:03.904021  6644 net.cpp:406] relu_added1 <- newconv_added1
I1211 09:56:03.904021  6644 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 09:56:03.904525  6644 net.cpp:122] Setting up relu_added1
I1211 09:56:03.904525  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.904525  6644 net.cpp:137] Memory required for data: 410830000
I1211 09:56:03.904525  6644 layer_factory.cpp:58] Creating layer pool2_1
I1211 09:56:03.904525  6644 net.cpp:84] Creating Layer pool2_1
I1211 09:56:03.905009  6644 net.cpp:406] pool2_1 <- newconv_added1
I1211 09:56:03.905009  6644 net.cpp:380] pool2_1 -> pool2_1
I1211 09:56:03.905009  6644 net.cpp:122] Setting up pool2_1
I1211 09:56:03.905009  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.905009  6644 net.cpp:137] Memory required for data: 415950000
I1211 09:56:03.905009  6644 layer_factory.cpp:58] Creating layer conv3
I1211 09:56:03.905009  6644 net.cpp:84] Creating Layer conv3
I1211 09:56:03.905009  6644 net.cpp:406] conv3 <- pool2_1
I1211 09:56:03.905009  6644 net.cpp:380] conv3 -> conv3
I1211 09:56:03.906508  6644 net.cpp:122] Setting up conv3
I1211 09:56:03.906508  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.906508  6644 net.cpp:137] Memory required for data: 421070000
I1211 09:56:03.906508  6644 layer_factory.cpp:58] Creating layer bn3
I1211 09:56:03.906508  6644 net.cpp:84] Creating Layer bn3
I1211 09:56:03.906508  6644 net.cpp:406] bn3 <- conv3
I1211 09:56:03.906508  6644 net.cpp:367] bn3 -> conv3 (in-place)
I1211 09:56:03.906508  6644 net.cpp:122] Setting up bn3
I1211 09:56:03.906508  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.906508  6644 net.cpp:137] Memory required for data: 426190000
I1211 09:56:03.906508  6644 layer_factory.cpp:58] Creating layer scale3
I1211 09:56:03.906508  6644 net.cpp:84] Creating Layer scale3
I1211 09:56:03.906508  6644 net.cpp:406] scale3 <- conv3
I1211 09:56:03.906508  6644 net.cpp:367] scale3 -> conv3 (in-place)
I1211 09:56:03.906508  6644 layer_factory.cpp:58] Creating layer scale3
I1211 09:56:03.907006  6644 net.cpp:122] Setting up scale3
I1211 09:56:03.907006  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.907006  6644 net.cpp:137] Memory required for data: 431310000
I1211 09:56:03.907006  6644 layer_factory.cpp:58] Creating layer relu3
I1211 09:56:03.907006  6644 net.cpp:84] Creating Layer relu3
I1211 09:56:03.907006  6644 net.cpp:406] relu3 <- conv3
I1211 09:56:03.907006  6644 net.cpp:367] relu3 -> conv3 (in-place)
I1211 09:56:03.907006  6644 net.cpp:122] Setting up relu3
I1211 09:56:03.907006  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.907006  6644 net.cpp:137] Memory required for data: 436430000
I1211 09:56:03.907006  6644 layer_factory.cpp:58] Creating layer conv3_1
I1211 09:56:03.907006  6644 net.cpp:84] Creating Layer conv3_1
I1211 09:56:03.907006  6644 net.cpp:406] conv3_1 <- conv3
I1211 09:56:03.907006  6644 net.cpp:380] conv3_1 -> conv3_1
I1211 09:56:03.908507  6644 net.cpp:122] Setting up conv3_1
I1211 09:56:03.908507  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.908507  6644 net.cpp:137] Memory required for data: 441550000
I1211 09:56:03.908507  6644 layer_factory.cpp:58] Creating layer bn3_1
I1211 09:56:03.908507  6644 net.cpp:84] Creating Layer bn3_1
I1211 09:56:03.908507  6644 net.cpp:406] bn3_1 <- conv3_1
I1211 09:56:03.908507  6644 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 09:56:03.908507  6644 net.cpp:122] Setting up bn3_1
I1211 09:56:03.908507  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.908507  6644 net.cpp:137] Memory required for data: 446670000
I1211 09:56:03.908507  6644 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:56:03.908507  6644 net.cpp:84] Creating Layer scale3_1
I1211 09:56:03.908507  6644 net.cpp:406] scale3_1 <- conv3_1
I1211 09:56:03.908507  6644 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 09:56:03.908507  6644 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:56:03.909006  6644 net.cpp:122] Setting up scale3_1
I1211 09:56:03.909006  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.909006  6644 net.cpp:137] Memory required for data: 451790000
I1211 09:56:03.909006  6644 layer_factory.cpp:58] Creating layer relu3_1
I1211 09:56:03.909006  6644 net.cpp:84] Creating Layer relu3_1
I1211 09:56:03.909006  6644 net.cpp:406] relu3_1 <- conv3_1
I1211 09:56:03.909006  6644 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 09:56:03.909006  6644 net.cpp:122] Setting up relu3_1
I1211 09:56:03.909006  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.909006  6644 net.cpp:137] Memory required for data: 456910000
I1211 09:56:03.909006  6644 layer_factory.cpp:58] Creating layer conv4
I1211 09:56:03.909006  6644 net.cpp:84] Creating Layer conv4
I1211 09:56:03.909006  6644 net.cpp:406] conv4 <- conv3_1
I1211 09:56:03.909006  6644 net.cpp:380] conv4 -> conv4
I1211 09:56:03.910006  6644 net.cpp:122] Setting up conv4
I1211 09:56:03.910006  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.910006  6644 net.cpp:137] Memory required for data: 462030000
I1211 09:56:03.910006  6644 layer_factory.cpp:58] Creating layer bn4
I1211 09:56:03.910006  6644 net.cpp:84] Creating Layer bn4
I1211 09:56:03.910006  6644 net.cpp:406] bn4 <- conv4
I1211 09:56:03.910506  6644 net.cpp:367] bn4 -> conv4 (in-place)
I1211 09:56:03.910506  6644 net.cpp:122] Setting up bn4
I1211 09:56:03.910506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.910506  6644 net.cpp:137] Memory required for data: 467150000
I1211 09:56:03.910506  6644 layer_factory.cpp:58] Creating layer scale4
I1211 09:56:03.910506  6644 net.cpp:84] Creating Layer scale4
I1211 09:56:03.910506  6644 net.cpp:406] scale4 <- conv4
I1211 09:56:03.910506  6644 net.cpp:367] scale4 -> conv4 (in-place)
I1211 09:56:03.910506  6644 layer_factory.cpp:58] Creating layer scale4
I1211 09:56:03.910506  6644 net.cpp:122] Setting up scale4
I1211 09:56:03.910506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.910506  6644 net.cpp:137] Memory required for data: 472270000
I1211 09:56:03.910506  6644 layer_factory.cpp:58] Creating layer relu4
I1211 09:56:03.910506  6644 net.cpp:84] Creating Layer relu4
I1211 09:56:03.910506  6644 net.cpp:406] relu4 <- conv4
I1211 09:56:03.910506  6644 net.cpp:367] relu4 -> conv4 (in-place)
I1211 09:56:03.911005  6644 net.cpp:122] Setting up relu4
I1211 09:56:03.911005  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.911005  6644 net.cpp:137] Memory required for data: 477390000
I1211 09:56:03.911005  6644 layer_factory.cpp:58] Creating layer conv4_1
I1211 09:56:03.911005  6644 net.cpp:84] Creating Layer conv4_1
I1211 09:56:03.911005  6644 net.cpp:406] conv4_1 <- conv4
I1211 09:56:03.911005  6644 net.cpp:380] conv4_1 -> conv4_1
I1211 09:56:03.912505  6644 net.cpp:122] Setting up conv4_1
I1211 09:56:03.912505  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.912505  6644 net.cpp:137] Memory required for data: 482510000
I1211 09:56:03.912505  6644 layer_factory.cpp:58] Creating layer bn4_1
I1211 09:56:03.912505  6644 net.cpp:84] Creating Layer bn4_1
I1211 09:56:03.912505  6644 net.cpp:406] bn4_1 <- conv4_1
I1211 09:56:03.912505  6644 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 09:56:03.912505  6644 net.cpp:122] Setting up bn4_1
I1211 09:56:03.912505  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.912505  6644 net.cpp:137] Memory required for data: 487630000
I1211 09:56:03.912505  6644 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:56:03.912505  6644 net.cpp:84] Creating Layer scale4_1
I1211 09:56:03.912505  6644 net.cpp:406] scale4_1 <- conv4_1
I1211 09:56:03.912505  6644 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 09:56:03.912505  6644 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:56:03.912505  6644 net.cpp:122] Setting up scale4_1
I1211 09:56:03.913007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.913007  6644 net.cpp:137] Memory required for data: 492750000
I1211 09:56:03.913007  6644 layer_factory.cpp:58] Creating layer relu4_1
I1211 09:56:03.913007  6644 net.cpp:84] Creating Layer relu4_1
I1211 09:56:03.913007  6644 net.cpp:406] relu4_1 <- conv4_1
I1211 09:56:03.913007  6644 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 09:56:03.913007  6644 net.cpp:122] Setting up relu4_1
I1211 09:56:03.913007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.913007  6644 net.cpp:137] Memory required for data: 497870000
I1211 09:56:03.913007  6644 layer_factory.cpp:58] Creating layer conv4_2
I1211 09:56:03.913007  6644 net.cpp:84] Creating Layer conv4_2
I1211 09:56:03.913007  6644 net.cpp:406] conv4_2 <- conv4_1
I1211 09:56:03.913007  6644 net.cpp:380] conv4_2 -> conv4_2
I1211 09:56:03.914006  6644 net.cpp:122] Setting up conv4_2
I1211 09:56:03.914006  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.914006  6644 net.cpp:137] Memory required for data: 503809200
I1211 09:56:03.914006  6644 layer_factory.cpp:58] Creating layer bn4_2
I1211 09:56:03.914006  6644 net.cpp:84] Creating Layer bn4_2
I1211 09:56:03.914006  6644 net.cpp:406] bn4_2 <- conv4_2
I1211 09:56:03.914006  6644 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 09:56:03.914506  6644 net.cpp:122] Setting up bn4_2
I1211 09:56:03.914506  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.914506  6644 net.cpp:137] Memory required for data: 509748400
I1211 09:56:03.914506  6644 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:56:03.914506  6644 net.cpp:84] Creating Layer scale4_2
I1211 09:56:03.914506  6644 net.cpp:406] scale4_2 <- conv4_2
I1211 09:56:03.914506  6644 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 09:56:03.914506  6644 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:56:03.914506  6644 net.cpp:122] Setting up scale4_2
I1211 09:56:03.914506  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.914506  6644 net.cpp:137] Memory required for data: 515687600
I1211 09:56:03.914506  6644 layer_factory.cpp:58] Creating layer relu4_2
I1211 09:56:03.914506  6644 net.cpp:84] Creating Layer relu4_2
I1211 09:56:03.914506  6644 net.cpp:406] relu4_2 <- conv4_2
I1211 09:56:03.914506  6644 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 09:56:03.915005  6644 net.cpp:122] Setting up relu4_2
I1211 09:56:03.915005  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.915005  6644 net.cpp:137] Memory required for data: 521626800
I1211 09:56:03.915005  6644 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 09:56:03.915005  6644 net.cpp:84] Creating Layer added_new_conv2
I1211 09:56:03.915005  6644 net.cpp:406] added_new_conv2 <- conv4_2
I1211 09:56:03.915005  6644 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 09:56:03.916507  6644 net.cpp:122] Setting up added_new_conv2
I1211 09:56:03.916507  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.916507  6644 net.cpp:137] Memory required for data: 527566000
I1211 09:56:03.916507  6644 layer_factory.cpp:58] Creating layer bn_added2
I1211 09:56:03.916507  6644 net.cpp:84] Creating Layer bn_added2
I1211 09:56:03.916507  6644 net.cpp:406] bn_added2 <- added_new_conv2
I1211 09:56:03.916507  6644 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.916507  6644 net.cpp:122] Setting up bn_added2
I1211 09:56:03.916507  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.916507  6644 net.cpp:137] Memory required for data: 533505200
I1211 09:56:03.916507  6644 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:56:03.916507  6644 net.cpp:84] Creating Layer scale_added2
I1211 09:56:03.916507  6644 net.cpp:406] scale_added2 <- added_new_conv2
I1211 09:56:03.916507  6644 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.917012  6644 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:56:03.917012  6644 net.cpp:122] Setting up scale_added2
I1211 09:56:03.917012  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.917012  6644 net.cpp:137] Memory required for data: 539444400
I1211 09:56:03.917012  6644 layer_factory.cpp:58] Creating layer relu_added2
I1211 09:56:03.917012  6644 net.cpp:84] Creating Layer relu_added2
I1211 09:56:03.917012  6644 net.cpp:406] relu_added2 <- added_new_conv2
I1211 09:56:03.917012  6644 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.917507  6644 net.cpp:122] Setting up relu_added2
I1211 09:56:03.917507  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.917507  6644 net.cpp:137] Memory required for data: 545383600
I1211 09:56:03.917507  6644 layer_factory.cpp:58] Creating layer pool4_2
I1211 09:56:03.917507  6644 net.cpp:84] Creating Layer pool4_2
I1211 09:56:03.917507  6644 net.cpp:406] pool4_2 <- added_new_conv2
I1211 09:56:03.917507  6644 net.cpp:380] pool4_2 -> pool4_2
I1211 09:56:03.917507  6644 net.cpp:122] Setting up pool4_2
I1211 09:56:03.917507  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.917507  6644 net.cpp:137] Memory required for data: 546868400
I1211 09:56:03.917507  6644 layer_factory.cpp:58] Creating layer conv4_0
I1211 09:56:03.917507  6644 net.cpp:84] Creating Layer conv4_0
I1211 09:56:03.917507  6644 net.cpp:406] conv4_0 <- pool4_2
I1211 09:56:03.917507  6644 net.cpp:380] conv4_0 -> conv4_0
I1211 09:56:03.919006  6644 net.cpp:122] Setting up conv4_0
I1211 09:56:03.919006  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.919006  6644 net.cpp:137] Memory required for data: 548353200
I1211 09:56:03.919006  6644 layer_factory.cpp:58] Creating layer bn4_0
I1211 09:56:03.919006  6644 net.cpp:84] Creating Layer bn4_0
I1211 09:56:03.919006  6644 net.cpp:406] bn4_0 <- conv4_0
I1211 09:56:03.919006  6644 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 09:56:03.919006  6644 net.cpp:122] Setting up bn4_0
I1211 09:56:03.919006  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.919006  6644 net.cpp:137] Memory required for data: 549838000
I1211 09:56:03.919006  6644 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:56:03.919006  6644 net.cpp:84] Creating Layer scale4_0
I1211 09:56:03.919006  6644 net.cpp:406] scale4_0 <- conv4_0
I1211 09:56:03.919006  6644 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 09:56:03.919006  6644 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:56:03.919507  6644 net.cpp:122] Setting up scale4_0
I1211 09:56:03.919507  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.919507  6644 net.cpp:137] Memory required for data: 551322800
I1211 09:56:03.919507  6644 layer_factory.cpp:58] Creating layer relu4_0
I1211 09:56:03.919507  6644 net.cpp:84] Creating Layer relu4_0
I1211 09:56:03.919507  6644 net.cpp:406] relu4_0 <- conv4_0
I1211 09:56:03.919507  6644 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 09:56:03.919507  6644 net.cpp:122] Setting up relu4_0
I1211 09:56:03.920007  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.920007  6644 net.cpp:137] Memory required for data: 552807600
I1211 09:56:03.920007  6644 layer_factory.cpp:58] Creating layer conv11
I1211 09:56:03.920007  6644 net.cpp:84] Creating Layer conv11
I1211 09:56:03.920007  6644 net.cpp:406] conv11 <- conv4_0
I1211 09:56:03.920007  6644 net.cpp:380] conv11 -> conv11
I1211 09:56:03.921507  6644 net.cpp:122] Setting up conv11
I1211 09:56:03.921507  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.921507  6644 net.cpp:137] Memory required for data: 554599600
I1211 09:56:03.921507  6644 layer_factory.cpp:58] Creating layer bn_conv11
I1211 09:56:03.921507  6644 net.cpp:84] Creating Layer bn_conv11
I1211 09:56:03.921507  6644 net.cpp:406] bn_conv11 <- conv11
I1211 09:56:03.921507  6644 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 09:56:03.921507  6644 net.cpp:122] Setting up bn_conv11
I1211 09:56:03.921507  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.921507  6644 net.cpp:137] Memory required for data: 556391600
I1211 09:56:03.921507  6644 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:56:03.921507  6644 net.cpp:84] Creating Layer scale_conv11
I1211 09:56:03.921507  6644 net.cpp:406] scale_conv11 <- conv11
I1211 09:56:03.921507  6644 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 09:56:03.921507  6644 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:56:03.922006  6644 net.cpp:122] Setting up scale_conv11
I1211 09:56:03.922006  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.922006  6644 net.cpp:137] Memory required for data: 558183600
I1211 09:56:03.922006  6644 layer_factory.cpp:58] Creating layer relu_conv11
I1211 09:56:03.922006  6644 net.cpp:84] Creating Layer relu_conv11
I1211 09:56:03.922006  6644 net.cpp:406] relu_conv11 <- conv11
I1211 09:56:03.922006  6644 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 09:56:03.922508  6644 net.cpp:122] Setting up relu_conv11
I1211 09:56:03.922508  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.922508  6644 net.cpp:137] Memory required for data: 559975600
I1211 09:56:03.922508  6644 layer_factory.cpp:58] Creating layer conv12
I1211 09:56:03.922508  6644 net.cpp:84] Creating Layer conv12
I1211 09:56:03.922508  6644 net.cpp:406] conv12 <- conv11
I1211 09:56:03.922508  6644 net.cpp:380] conv12 -> conv12
I1211 09:56:03.924007  6644 net.cpp:122] Setting up conv12
I1211 09:56:03.924007  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.924007  6644 net.cpp:137] Memory required for data: 562279600
I1211 09:56:03.924007  6644 layer_factory.cpp:58] Creating layer bn_conv12
I1211 09:56:03.924007  6644 net.cpp:84] Creating Layer bn_conv12
I1211 09:56:03.924007  6644 net.cpp:406] bn_conv12 <- conv12
I1211 09:56:03.924007  6644 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 09:56:03.924507  6644 net.cpp:122] Setting up bn_conv12
I1211 09:56:03.924507  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.924507  6644 net.cpp:137] Memory required for data: 564583600
I1211 09:56:03.924507  6644 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:56:03.924507  6644 net.cpp:84] Creating Layer scale_conv12
I1211 09:56:03.924507  6644 net.cpp:406] scale_conv12 <- conv12
I1211 09:56:03.924507  6644 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 09:56:03.924507  6644 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:56:03.924507  6644 net.cpp:122] Setting up scale_conv12
I1211 09:56:03.924507  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.924507  6644 net.cpp:137] Memory required for data: 566887600
I1211 09:56:03.924507  6644 layer_factory.cpp:58] Creating layer relu_conv12
I1211 09:56:03.924507  6644 net.cpp:84] Creating Layer relu_conv12
I1211 09:56:03.924507  6644 net.cpp:406] relu_conv12 <- conv12
I1211 09:56:03.924507  6644 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 09:56:03.925006  6644 net.cpp:122] Setting up relu_conv12
I1211 09:56:03.925006  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.925006  6644 net.cpp:137] Memory required for data: 569191600
I1211 09:56:03.925006  6644 layer_factory.cpp:58] Creating layer poolcp6
I1211 09:56:03.925006  6644 net.cpp:84] Creating Layer poolcp6
I1211 09:56:03.925006  6644 net.cpp:406] poolcp6 <- conv12
I1211 09:56:03.925006  6644 net.cpp:380] poolcp6 -> poolcp6
I1211 09:56:03.925006  6644 net.cpp:122] Setting up poolcp6
I1211 09:56:03.925006  6644 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 09:56:03.925006  6644 net.cpp:137] Memory required for data: 569227600
I1211 09:56:03.925006  6644 layer_factory.cpp:58] Creating layer ip1
I1211 09:56:03.925006  6644 net.cpp:84] Creating Layer ip1
I1211 09:56:03.925006  6644 net.cpp:406] ip1 <- poolcp6
I1211 09:56:03.925006  6644 net.cpp:380] ip1 -> ip1
I1211 09:56:03.925006  6644 net.cpp:122] Setting up ip1
I1211 09:56:03.925006  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.925006  6644 net.cpp:137] Memory required for data: 569267600
I1211 09:56:03.925006  6644 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 09:56:03.925006  6644 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 09:56:03.925006  6644 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 09:56:03.925006  6644 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 09:56:03.925006  6644 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 09:56:03.925006  6644 net.cpp:122] Setting up ip1_ip1_0_split
I1211 09:56:03.925006  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.925508  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.925508  6644 net.cpp:137] Memory required for data: 569347600
I1211 09:56:03.925508  6644 layer_factory.cpp:58] Creating layer accuracy_training
I1211 09:56:03.925508  6644 net.cpp:84] Creating Layer accuracy_training
I1211 09:56:03.925508  6644 net.cpp:406] accuracy_training <- ip1_ip1_0_split_0
I1211 09:56:03.925508  6644 net.cpp:406] accuracy_training <- label_cifar_1_split_0
I1211 09:56:03.925508  6644 net.cpp:380] accuracy_training -> accuracy_training
I1211 09:56:03.925508  6644 net.cpp:122] Setting up accuracy_training
I1211 09:56:03.925508  6644 net.cpp:129] Top shape: (1)
I1211 09:56:03.925508  6644 net.cpp:137] Memory required for data: 569347604
I1211 09:56:03.925508  6644 layer_factory.cpp:58] Creating layer loss
I1211 09:56:03.925508  6644 net.cpp:84] Creating Layer loss
I1211 09:56:03.925508  6644 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 09:56:03.925508  6644 net.cpp:406] loss <- label_cifar_1_split_1
I1211 09:56:03.925508  6644 net.cpp:380] loss -> loss
I1211 09:56:03.925508  6644 layer_factory.cpp:58] Creating layer loss
I1211 09:56:03.926007  6644 net.cpp:122] Setting up loss
I1211 09:56:03.926007  6644 net.cpp:129] Top shape: (1)
I1211 09:56:03.926007  6644 net.cpp:132]     with loss weight 1
I1211 09:56:03.926007  6644 net.cpp:137] Memory required for data: 569347608
I1211 09:56:03.926007  6644 net.cpp:198] loss needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:200] accuracy_training does not need backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] ip1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] poolcp6 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu_conv12 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale_conv12 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn_conv12 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv12 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu_conv11 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale_conv11 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn_conv11 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv11 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu4_0 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale4_0 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn4_0 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv4_0 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] pool4_2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu_added2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale_added2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn_added2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] added_new_conv2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu4_2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale4_2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn4_2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv4_2 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu4_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale4_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn4_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv4_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu4 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale4 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn4 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv4 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu3_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale3_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn3_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv3_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu3 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale3 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn3 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] conv3 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] pool2_1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] relu_added1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] scale_added1 needs backward computation.
I1211 09:56:03.926007  6644 net.cpp:198] bn_added1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] newconv_added1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] relu2_2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] scale2_2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] bn2_2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] conv2_2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] relu2_1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] scale2_1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] bn2_1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] conv2_1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] relu2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] scale2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] bn2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] conv2 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] relu1_0 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] scale1_0 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] bn1_0 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] conv1_0 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] relu1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] scale1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] bn1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:198] conv1 needs backward computation.
I1211 09:56:03.926506  6644 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 09:56:03.926506  6644 net.cpp:200] cifar does not need backward computation.
I1211 09:56:03.926506  6644 net.cpp:242] This network produces output accuracy_training
I1211 09:56:03.926506  6644 net.cpp:242] This network produces output loss
I1211 09:56:03.926506  6644 net.cpp:255] Network initialization done.
I1211 09:56:03.927506  6644 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:56:03.927506  6644 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:56:03.927506  6644 solver.cpp:172] Creating test net (#0) specified by net file: examples/cifar100/fcifar100_full_relu_train_test_bn.prototxt
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added1
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3_1
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_added2
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv11
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn_conv12
I1211 09:56:03.927506  6644 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I1211 09:56:03.928006  6644 net.cpp:51] Initializing net from parameters: 
name: "CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    crop_size: 32
  }
  data_param {
    source: "examples/cifar100/cifar100_test_leveldb_padding"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 30
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "conv1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "conv1_0"
  top: "conv1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "conv1_0"
  top: "conv1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "conv2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 40
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "conv2_1"
  top: "conv2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "conv2_2"
  top: "conv2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "newconv_added1"
  type: "Convolution"
  bottom: "conv2_2"
  top: "newconv_added1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added1"
  type: "BatchNorm"
  bottom: "newconv_added1"
  top: "newconv_added1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added1"
  type: "Scale"
  bottom: "newconv_added1"
  top: "newconv_added1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added1"
  type: "ReLU"
  bottom: "newconv_added1"
  top: "newconv_added1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "newconv_added1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "conv3"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3_1"
  type: "BatchNorm"
  bottom: "conv3_1"
  top: "conv3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale3_1"
  type: "Scale"
  bottom: "conv3_1"
  top: "conv3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "conv4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 50
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "conv4_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "conv4_1"
  top: "conv4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "conv4_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "conv4_2"
  top: "conv4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "added_new_conv2"
  type: "Convolution"
  bottom: "conv4_2"
  top: "added_new_conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn_added2"
  type: "BatchNorm"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_added2"
  type: "Scale"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_added2"
  type: "ReLU"
  bottom: "added_new_conv2"
  top: "added_new_conv2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "added_new_conv2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 58
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "conv4_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "conv4_0"
  top: "conv4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "conv4_0"
  top: "conv4_0"
}
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "conv4_0"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 70
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv11"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv11"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv11"
  type: "ReLU"
  bottom: "conv11"
  top: "conv11"
}
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "conv11"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 90
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn_conv12"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.99
  }
}
layer {
  name: "scale_conv12"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_conv12"
  type: "ReLU"
  bottom: "conv12"
  top: "conv12"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "conv12"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    global_pooling: true
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1211 09:56:03.928006  6644 layer_factory.cpp:58] Creating layer cifar
I1211 09:56:03.933517  6644 db_leveldb.cpp:18] Opened leveldb examples/cifar100/cifar100_test_leveldb_padding
I1211 09:56:03.933517  6644 net.cpp:84] Creating Layer cifar
I1211 09:56:03.933517  6644 net.cpp:380] cifar -> data
I1211 09:56:03.933517  6644 net.cpp:380] cifar -> label
I1211 09:56:03.934008  6644 data_layer.cpp:45] output data size: 100,3,32,32
I1211 09:56:03.939508  6644 net.cpp:122] Setting up cifar
I1211 09:56:03.939508  6644 net.cpp:129] Top shape: 100 3 32 32 (307200)
I1211 09:56:03.939508  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.939508  6644 net.cpp:137] Memory required for data: 1229200
I1211 09:56:03.939508  6644 layer_factory.cpp:58] Creating layer label_cifar_1_split
I1211 09:56:03.939508  6644 net.cpp:84] Creating Layer label_cifar_1_split
I1211 09:56:03.939508  6644 net.cpp:406] label_cifar_1_split <- label
I1211 09:56:03.939508  6644 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_0
I1211 09:56:03.939508  6644 net.cpp:380] label_cifar_1_split -> label_cifar_1_split_1
I1211 09:56:03.940006  6644 net.cpp:122] Setting up label_cifar_1_split
I1211 09:56:03.940006  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.940006  6644 net.cpp:129] Top shape: 100 (100)
I1211 09:56:03.940006  6644 net.cpp:137] Memory required for data: 1230000
I1211 09:56:03.940006  6644 layer_factory.cpp:58] Creating layer conv1
I1211 09:56:03.940006  6644 net.cpp:84] Creating Layer conv1
I1211 09:56:03.940006  6644 net.cpp:406] conv1 <- data
I1211 09:56:03.940006  6644 net.cpp:380] conv1 -> conv1
I1211 09:56:03.941517  6644 net.cpp:122] Setting up conv1
I1211 09:56:03.941517 13556 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I1211 09:56:03.941517  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.941517  6644 net.cpp:137] Memory required for data: 13518000
I1211 09:56:03.941517  6644 layer_factory.cpp:58] Creating layer bn1
I1211 09:56:03.941517  6644 net.cpp:84] Creating Layer bn1
I1211 09:56:03.941517  6644 net.cpp:406] bn1 <- conv1
I1211 09:56:03.941517  6644 net.cpp:367] bn1 -> conv1 (in-place)
I1211 09:56:03.942009  6644 net.cpp:122] Setting up bn1
I1211 09:56:03.942009  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.942009  6644 net.cpp:137] Memory required for data: 25806000
I1211 09:56:03.942009  6644 layer_factory.cpp:58] Creating layer scale1
I1211 09:56:03.942009  6644 net.cpp:84] Creating Layer scale1
I1211 09:56:03.942009  6644 net.cpp:406] scale1 <- conv1
I1211 09:56:03.942009  6644 net.cpp:367] scale1 -> conv1 (in-place)
I1211 09:56:03.942009  6644 layer_factory.cpp:58] Creating layer scale1
I1211 09:56:03.942009  6644 net.cpp:122] Setting up scale1
I1211 09:56:03.942507  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.942507  6644 net.cpp:137] Memory required for data: 38094000
I1211 09:56:03.942507  6644 layer_factory.cpp:58] Creating layer relu1
I1211 09:56:03.942507  6644 net.cpp:84] Creating Layer relu1
I1211 09:56:03.942507  6644 net.cpp:406] relu1 <- conv1
I1211 09:56:03.942507  6644 net.cpp:367] relu1 -> conv1 (in-place)
I1211 09:56:03.943008  6644 net.cpp:122] Setting up relu1
I1211 09:56:03.943008  6644 net.cpp:129] Top shape: 100 30 32 32 (3072000)
I1211 09:56:03.943008  6644 net.cpp:137] Memory required for data: 50382000
I1211 09:56:03.943008  6644 layer_factory.cpp:58] Creating layer conv1_0
I1211 09:56:03.943008  6644 net.cpp:84] Creating Layer conv1_0
I1211 09:56:03.943008  6644 net.cpp:406] conv1_0 <- conv1
I1211 09:56:03.943008  6644 net.cpp:380] conv1_0 -> conv1_0
I1211 09:56:03.944010  6644 net.cpp:122] Setting up conv1_0
I1211 09:56:03.944010  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.944010  6644 net.cpp:137] Memory required for data: 66766000
I1211 09:56:03.944010  6644 layer_factory.cpp:58] Creating layer bn1_0
I1211 09:56:03.944010  6644 net.cpp:84] Creating Layer bn1_0
I1211 09:56:03.944010  6644 net.cpp:406] bn1_0 <- conv1_0
I1211 09:56:03.944010  6644 net.cpp:367] bn1_0 -> conv1_0 (in-place)
I1211 09:56:03.944507  6644 net.cpp:122] Setting up bn1_0
I1211 09:56:03.944507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.944507  6644 net.cpp:137] Memory required for data: 83150000
I1211 09:56:03.944507  6644 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:56:03.944507  6644 net.cpp:84] Creating Layer scale1_0
I1211 09:56:03.944507  6644 net.cpp:406] scale1_0 <- conv1_0
I1211 09:56:03.944507  6644 net.cpp:367] scale1_0 -> conv1_0 (in-place)
I1211 09:56:03.944507  6644 layer_factory.cpp:58] Creating layer scale1_0
I1211 09:56:03.944507  6644 net.cpp:122] Setting up scale1_0
I1211 09:56:03.944507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.944507  6644 net.cpp:137] Memory required for data: 99534000
I1211 09:56:03.944507  6644 layer_factory.cpp:58] Creating layer relu1_0
I1211 09:56:03.944507  6644 net.cpp:84] Creating Layer relu1_0
I1211 09:56:03.944507  6644 net.cpp:406] relu1_0 <- conv1_0
I1211 09:56:03.944507  6644 net.cpp:367] relu1_0 -> conv1_0 (in-place)
I1211 09:56:03.945008  6644 net.cpp:122] Setting up relu1_0
I1211 09:56:03.945008  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.945008  6644 net.cpp:137] Memory required for data: 115918000
I1211 09:56:03.945008  6644 layer_factory.cpp:58] Creating layer conv2
I1211 09:56:03.945008  6644 net.cpp:84] Creating Layer conv2
I1211 09:56:03.945008  6644 net.cpp:406] conv2 <- conv1_0
I1211 09:56:03.945008  6644 net.cpp:380] conv2 -> conv2
I1211 09:56:03.946007  6644 net.cpp:122] Setting up conv2
I1211 09:56:03.946007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.946007  6644 net.cpp:137] Memory required for data: 132302000
I1211 09:56:03.946007  6644 layer_factory.cpp:58] Creating layer bn2
I1211 09:56:03.946507  6644 net.cpp:84] Creating Layer bn2
I1211 09:56:03.946507  6644 net.cpp:406] bn2 <- conv2
I1211 09:56:03.946507  6644 net.cpp:367] bn2 -> conv2 (in-place)
I1211 09:56:03.946507  6644 net.cpp:122] Setting up bn2
I1211 09:56:03.946507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.946507  6644 net.cpp:137] Memory required for data: 148686000
I1211 09:56:03.946507  6644 layer_factory.cpp:58] Creating layer scale2
I1211 09:56:03.946507  6644 net.cpp:84] Creating Layer scale2
I1211 09:56:03.946507  6644 net.cpp:406] scale2 <- conv2
I1211 09:56:03.946507  6644 net.cpp:367] scale2 -> conv2 (in-place)
I1211 09:56:03.946507  6644 layer_factory.cpp:58] Creating layer scale2
I1211 09:56:03.946507  6644 net.cpp:122] Setting up scale2
I1211 09:56:03.946507  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.946507  6644 net.cpp:137] Memory required for data: 165070000
I1211 09:56:03.946507  6644 layer_factory.cpp:58] Creating layer relu2
I1211 09:56:03.946507  6644 net.cpp:84] Creating Layer relu2
I1211 09:56:03.946507  6644 net.cpp:406] relu2 <- conv2
I1211 09:56:03.946507  6644 net.cpp:367] relu2 -> conv2 (in-place)
I1211 09:56:03.947006  6644 net.cpp:122] Setting up relu2
I1211 09:56:03.947006  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.947006  6644 net.cpp:137] Memory required for data: 181454000
I1211 09:56:03.947006  6644 layer_factory.cpp:58] Creating layer conv2_1
I1211 09:56:03.947006  6644 net.cpp:84] Creating Layer conv2_1
I1211 09:56:03.947006  6644 net.cpp:406] conv2_1 <- conv2
I1211 09:56:03.947006  6644 net.cpp:380] conv2_1 -> conv2_1
I1211 09:56:03.948007  6644 net.cpp:122] Setting up conv2_1
I1211 09:56:03.948506  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.948506  6644 net.cpp:137] Memory required for data: 197838000
I1211 09:56:03.948506  6644 layer_factory.cpp:58] Creating layer bn2_1
I1211 09:56:03.948506  6644 net.cpp:84] Creating Layer bn2_1
I1211 09:56:03.948506  6644 net.cpp:406] bn2_1 <- conv2_1
I1211 09:56:03.948506  6644 net.cpp:367] bn2_1 -> conv2_1 (in-place)
I1211 09:56:03.948506  6644 net.cpp:122] Setting up bn2_1
I1211 09:56:03.948506  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.948506  6644 net.cpp:137] Memory required for data: 214222000
I1211 09:56:03.948506  6644 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:56:03.948506  6644 net.cpp:84] Creating Layer scale2_1
I1211 09:56:03.948506  6644 net.cpp:406] scale2_1 <- conv2_1
I1211 09:56:03.948506  6644 net.cpp:367] scale2_1 -> conv2_1 (in-place)
I1211 09:56:03.949007  6644 layer_factory.cpp:58] Creating layer scale2_1
I1211 09:56:03.949007  6644 net.cpp:122] Setting up scale2_1
I1211 09:56:03.949007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.949007  6644 net.cpp:137] Memory required for data: 230606000
I1211 09:56:03.949007  6644 layer_factory.cpp:58] Creating layer relu2_1
I1211 09:56:03.949007  6644 net.cpp:84] Creating Layer relu2_1
I1211 09:56:03.949007  6644 net.cpp:406] relu2_1 <- conv2_1
I1211 09:56:03.949007  6644 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I1211 09:56:03.949007  6644 net.cpp:122] Setting up relu2_1
I1211 09:56:03.949007  6644 net.cpp:129] Top shape: 100 40 32 32 (4096000)
I1211 09:56:03.949007  6644 net.cpp:137] Memory required for data: 246990000
I1211 09:56:03.949007  6644 layer_factory.cpp:58] Creating layer conv2_2
I1211 09:56:03.949007  6644 net.cpp:84] Creating Layer conv2_2
I1211 09:56:03.949007  6644 net.cpp:406] conv2_2 <- conv2_1
I1211 09:56:03.949007  6644 net.cpp:380] conv2_2 -> conv2_2
I1211 09:56:03.950510  6644 net.cpp:122] Setting up conv2_2
I1211 09:56:03.950510  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.950510  6644 net.cpp:137] Memory required for data: 267470000
I1211 09:56:03.951009  6644 layer_factory.cpp:58] Creating layer bn2_2
I1211 09:56:03.951009  6644 net.cpp:84] Creating Layer bn2_2
I1211 09:56:03.951009  6644 net.cpp:406] bn2_2 <- conv2_2
I1211 09:56:03.951009  6644 net.cpp:367] bn2_2 -> conv2_2 (in-place)
I1211 09:56:03.951009  6644 net.cpp:122] Setting up bn2_2
I1211 09:56:03.951009  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.951009  6644 net.cpp:137] Memory required for data: 287950000
I1211 09:56:03.951009  6644 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:56:03.951009  6644 net.cpp:84] Creating Layer scale2_2
I1211 09:56:03.951009  6644 net.cpp:406] scale2_2 <- conv2_2
I1211 09:56:03.951009  6644 net.cpp:367] scale2_2 -> conv2_2 (in-place)
I1211 09:56:03.951009  6644 layer_factory.cpp:58] Creating layer scale2_2
I1211 09:56:03.951509  6644 net.cpp:122] Setting up scale2_2
I1211 09:56:03.951509  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.951509  6644 net.cpp:137] Memory required for data: 308430000
I1211 09:56:03.951509  6644 layer_factory.cpp:58] Creating layer relu2_2
I1211 09:56:03.951509  6644 net.cpp:84] Creating Layer relu2_2
I1211 09:56:03.951509  6644 net.cpp:406] relu2_2 <- conv2_2
I1211 09:56:03.951509  6644 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I1211 09:56:03.951509  6644 net.cpp:122] Setting up relu2_2
I1211 09:56:03.951509  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.951509  6644 net.cpp:137] Memory required for data: 328910000
I1211 09:56:03.951509  6644 layer_factory.cpp:58] Creating layer newconv_added1
I1211 09:56:03.951509  6644 net.cpp:84] Creating Layer newconv_added1
I1211 09:56:03.951509  6644 net.cpp:406] newconv_added1 <- conv2_2
I1211 09:56:03.952019  6644 net.cpp:380] newconv_added1 -> newconv_added1
I1211 09:56:03.953507  6644 net.cpp:122] Setting up newconv_added1
I1211 09:56:03.953507  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.953507  6644 net.cpp:137] Memory required for data: 349390000
I1211 09:56:03.953507  6644 layer_factory.cpp:58] Creating layer bn_added1
I1211 09:56:03.953507  6644 net.cpp:84] Creating Layer bn_added1
I1211 09:56:03.953507  6644 net.cpp:406] bn_added1 <- newconv_added1
I1211 09:56:03.953507  6644 net.cpp:367] bn_added1 -> newconv_added1 (in-place)
I1211 09:56:03.953507  6644 net.cpp:122] Setting up bn_added1
I1211 09:56:03.953507  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.953507  6644 net.cpp:137] Memory required for data: 369870000
I1211 09:56:03.953507  6644 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:56:03.953507  6644 net.cpp:84] Creating Layer scale_added1
I1211 09:56:03.953507  6644 net.cpp:406] scale_added1 <- newconv_added1
I1211 09:56:03.953507  6644 net.cpp:367] scale_added1 -> newconv_added1 (in-place)
I1211 09:56:03.953507  6644 layer_factory.cpp:58] Creating layer scale_added1
I1211 09:56:03.954006  6644 net.cpp:122] Setting up scale_added1
I1211 09:56:03.954006  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.954006  6644 net.cpp:137] Memory required for data: 390350000
I1211 09:56:03.954006  6644 layer_factory.cpp:58] Creating layer relu_added1
I1211 09:56:03.954006  6644 net.cpp:84] Creating Layer relu_added1
I1211 09:56:03.954006  6644 net.cpp:406] relu_added1 <- newconv_added1
I1211 09:56:03.954006  6644 net.cpp:367] relu_added1 -> newconv_added1 (in-place)
I1211 09:56:03.954509  6644 net.cpp:122] Setting up relu_added1
I1211 09:56:03.954509  6644 net.cpp:129] Top shape: 100 50 32 32 (5120000)
I1211 09:56:03.954509  6644 net.cpp:137] Memory required for data: 410830000
I1211 09:56:03.954509  6644 layer_factory.cpp:58] Creating layer pool2_1
I1211 09:56:03.954509  6644 net.cpp:84] Creating Layer pool2_1
I1211 09:56:03.954509  6644 net.cpp:406] pool2_1 <- newconv_added1
I1211 09:56:03.954509  6644 net.cpp:380] pool2_1 -> pool2_1
I1211 09:56:03.954509  6644 net.cpp:122] Setting up pool2_1
I1211 09:56:03.954509  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.954509  6644 net.cpp:137] Memory required for data: 415950000
I1211 09:56:03.954509  6644 layer_factory.cpp:58] Creating layer conv3
I1211 09:56:03.954509  6644 net.cpp:84] Creating Layer conv3
I1211 09:56:03.954509  6644 net.cpp:406] conv3 <- pool2_1
I1211 09:56:03.954509  6644 net.cpp:380] conv3 -> conv3
I1211 09:56:03.955507  6644 net.cpp:122] Setting up conv3
I1211 09:56:03.955507  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.955507  6644 net.cpp:137] Memory required for data: 421070000
I1211 09:56:03.955507  6644 layer_factory.cpp:58] Creating layer bn3
I1211 09:56:03.955507  6644 net.cpp:84] Creating Layer bn3
I1211 09:56:03.955507  6644 net.cpp:406] bn3 <- conv3
I1211 09:56:03.955507  6644 net.cpp:367] bn3 -> conv3 (in-place)
I1211 09:56:03.956007  6644 net.cpp:122] Setting up bn3
I1211 09:56:03.956007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.956007  6644 net.cpp:137] Memory required for data: 426190000
I1211 09:56:03.956007  6644 layer_factory.cpp:58] Creating layer scale3
I1211 09:56:03.956007  6644 net.cpp:84] Creating Layer scale3
I1211 09:56:03.956007  6644 net.cpp:406] scale3 <- conv3
I1211 09:56:03.956007  6644 net.cpp:367] scale3 -> conv3 (in-place)
I1211 09:56:03.956007  6644 layer_factory.cpp:58] Creating layer scale3
I1211 09:56:03.956007  6644 net.cpp:122] Setting up scale3
I1211 09:56:03.956007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.956007  6644 net.cpp:137] Memory required for data: 431310000
I1211 09:56:03.956007  6644 layer_factory.cpp:58] Creating layer relu3
I1211 09:56:03.956007  6644 net.cpp:84] Creating Layer relu3
I1211 09:56:03.956007  6644 net.cpp:406] relu3 <- conv3
I1211 09:56:03.956007  6644 net.cpp:367] relu3 -> conv3 (in-place)
I1211 09:56:03.956506  6644 net.cpp:122] Setting up relu3
I1211 09:56:03.956506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.956506  6644 net.cpp:137] Memory required for data: 436430000
I1211 09:56:03.956506  6644 layer_factory.cpp:58] Creating layer conv3_1
I1211 09:56:03.956506  6644 net.cpp:84] Creating Layer conv3_1
I1211 09:56:03.956506  6644 net.cpp:406] conv3_1 <- conv3
I1211 09:56:03.956506  6644 net.cpp:380] conv3_1 -> conv3_1
I1211 09:56:03.958007  6644 net.cpp:122] Setting up conv3_1
I1211 09:56:03.958007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.958007  6644 net.cpp:137] Memory required for data: 441550000
I1211 09:56:03.958007  6644 layer_factory.cpp:58] Creating layer bn3_1
I1211 09:56:03.958007  6644 net.cpp:84] Creating Layer bn3_1
I1211 09:56:03.958007  6644 net.cpp:406] bn3_1 <- conv3_1
I1211 09:56:03.958007  6644 net.cpp:367] bn3_1 -> conv3_1 (in-place)
I1211 09:56:03.958506  6644 net.cpp:122] Setting up bn3_1
I1211 09:56:03.958506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.958506  6644 net.cpp:137] Memory required for data: 446670000
I1211 09:56:03.958506  6644 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:56:03.958506  6644 net.cpp:84] Creating Layer scale3_1
I1211 09:56:03.958506  6644 net.cpp:406] scale3_1 <- conv3_1
I1211 09:56:03.958506  6644 net.cpp:367] scale3_1 -> conv3_1 (in-place)
I1211 09:56:03.958506  6644 layer_factory.cpp:58] Creating layer scale3_1
I1211 09:56:03.958506  6644 net.cpp:122] Setting up scale3_1
I1211 09:56:03.958506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.958506  6644 net.cpp:137] Memory required for data: 451790000
I1211 09:56:03.958506  6644 layer_factory.cpp:58] Creating layer relu3_1
I1211 09:56:03.958506  6644 net.cpp:84] Creating Layer relu3_1
I1211 09:56:03.958506  6644 net.cpp:406] relu3_1 <- conv3_1
I1211 09:56:03.958506  6644 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I1211 09:56:03.958506  6644 net.cpp:122] Setting up relu3_1
I1211 09:56:03.958506  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.958506  6644 net.cpp:137] Memory required for data: 456910000
I1211 09:56:03.958506  6644 layer_factory.cpp:58] Creating layer conv4
I1211 09:56:03.958506  6644 net.cpp:84] Creating Layer conv4
I1211 09:56:03.958506  6644 net.cpp:406] conv4 <- conv3_1
I1211 09:56:03.959007  6644 net.cpp:380] conv4 -> conv4
I1211 09:56:03.960007  6644 net.cpp:122] Setting up conv4
I1211 09:56:03.960007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.960007  6644 net.cpp:137] Memory required for data: 462030000
I1211 09:56:03.960007  6644 layer_factory.cpp:58] Creating layer bn4
I1211 09:56:03.960007  6644 net.cpp:84] Creating Layer bn4
I1211 09:56:03.960007  6644 net.cpp:406] bn4 <- conv4
I1211 09:56:03.960007  6644 net.cpp:367] bn4 -> conv4 (in-place)
I1211 09:56:03.960007  6644 net.cpp:122] Setting up bn4
I1211 09:56:03.960007  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.960507  6644 net.cpp:137] Memory required for data: 467150000
I1211 09:56:03.960507  6644 layer_factory.cpp:58] Creating layer scale4
I1211 09:56:03.960507  6644 net.cpp:84] Creating Layer scale4
I1211 09:56:03.960507  6644 net.cpp:406] scale4 <- conv4
I1211 09:56:03.960507  6644 net.cpp:367] scale4 -> conv4 (in-place)
I1211 09:56:03.960507  6644 layer_factory.cpp:58] Creating layer scale4
I1211 09:56:03.960507  6644 net.cpp:122] Setting up scale4
I1211 09:56:03.960507  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.960507  6644 net.cpp:137] Memory required for data: 472270000
I1211 09:56:03.960507  6644 layer_factory.cpp:58] Creating layer relu4
I1211 09:56:03.960507  6644 net.cpp:84] Creating Layer relu4
I1211 09:56:03.960507  6644 net.cpp:406] relu4 <- conv4
I1211 09:56:03.960507  6644 net.cpp:367] relu4 -> conv4 (in-place)
I1211 09:56:03.960507  6644 net.cpp:122] Setting up relu4
I1211 09:56:03.960507  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.960507  6644 net.cpp:137] Memory required for data: 477390000
I1211 09:56:03.961006  6644 layer_factory.cpp:58] Creating layer conv4_1
I1211 09:56:03.961006  6644 net.cpp:84] Creating Layer conv4_1
I1211 09:56:03.961006  6644 net.cpp:406] conv4_1 <- conv4
I1211 09:56:03.961006  6644 net.cpp:380] conv4_1 -> conv4_1
I1211 09:56:03.962009  6644 net.cpp:122] Setting up conv4_1
I1211 09:56:03.962009  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.962009  6644 net.cpp:137] Memory required for data: 482510000
I1211 09:56:03.962009  6644 layer_factory.cpp:58] Creating layer bn4_1
I1211 09:56:03.962009  6644 net.cpp:84] Creating Layer bn4_1
I1211 09:56:03.962009  6644 net.cpp:406] bn4_1 <- conv4_1
I1211 09:56:03.962009  6644 net.cpp:367] bn4_1 -> conv4_1 (in-place)
I1211 09:56:03.962510  6644 net.cpp:122] Setting up bn4_1
I1211 09:56:03.962510  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.962510  6644 net.cpp:137] Memory required for data: 487630000
I1211 09:56:03.962510  6644 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:56:03.962510  6644 net.cpp:84] Creating Layer scale4_1
I1211 09:56:03.962510  6644 net.cpp:406] scale4_1 <- conv4_1
I1211 09:56:03.962510  6644 net.cpp:367] scale4_1 -> conv4_1 (in-place)
I1211 09:56:03.962510  6644 layer_factory.cpp:58] Creating layer scale4_1
I1211 09:56:03.962510  6644 net.cpp:122] Setting up scale4_1
I1211 09:56:03.962510  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.962510  6644 net.cpp:137] Memory required for data: 492750000
I1211 09:56:03.962510  6644 layer_factory.cpp:58] Creating layer relu4_1
I1211 09:56:03.962510  6644 net.cpp:84] Creating Layer relu4_1
I1211 09:56:03.962510  6644 net.cpp:406] relu4_1 <- conv4_1
I1211 09:56:03.962510  6644 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I1211 09:56:03.963009  6644 net.cpp:122] Setting up relu4_1
I1211 09:56:03.963009  6644 net.cpp:129] Top shape: 100 50 16 16 (1280000)
I1211 09:56:03.963009  6644 net.cpp:137] Memory required for data: 497870000
I1211 09:56:03.963009  6644 layer_factory.cpp:58] Creating layer conv4_2
I1211 09:56:03.963009  6644 net.cpp:84] Creating Layer conv4_2
I1211 09:56:03.963009  6644 net.cpp:406] conv4_2 <- conv4_1
I1211 09:56:03.963009  6644 net.cpp:380] conv4_2 -> conv4_2
I1211 09:56:03.964507  6644 net.cpp:122] Setting up conv4_2
I1211 09:56:03.964507  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.964507  6644 net.cpp:137] Memory required for data: 503809200
I1211 09:56:03.964507  6644 layer_factory.cpp:58] Creating layer bn4_2
I1211 09:56:03.964507  6644 net.cpp:84] Creating Layer bn4_2
I1211 09:56:03.964507  6644 net.cpp:406] bn4_2 <- conv4_2
I1211 09:56:03.964507  6644 net.cpp:367] bn4_2 -> conv4_2 (in-place)
I1211 09:56:03.964507  6644 net.cpp:122] Setting up bn4_2
I1211 09:56:03.964507  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.964507  6644 net.cpp:137] Memory required for data: 509748400
I1211 09:56:03.964507  6644 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:56:03.964507  6644 net.cpp:84] Creating Layer scale4_2
I1211 09:56:03.964507  6644 net.cpp:406] scale4_2 <- conv4_2
I1211 09:56:03.964507  6644 net.cpp:367] scale4_2 -> conv4_2 (in-place)
I1211 09:56:03.964507  6644 layer_factory.cpp:58] Creating layer scale4_2
I1211 09:56:03.965008  6644 net.cpp:122] Setting up scale4_2
I1211 09:56:03.965008  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.965008  6644 net.cpp:137] Memory required for data: 515687600
I1211 09:56:03.965008  6644 layer_factory.cpp:58] Creating layer relu4_2
I1211 09:56:03.965008  6644 net.cpp:84] Creating Layer relu4_2
I1211 09:56:03.965008  6644 net.cpp:406] relu4_2 <- conv4_2
I1211 09:56:03.965008  6644 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I1211 09:56:03.965008  6644 net.cpp:122] Setting up relu4_2
I1211 09:56:03.965008  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.965008  6644 net.cpp:137] Memory required for data: 521626800
I1211 09:56:03.965008  6644 layer_factory.cpp:58] Creating layer added_new_conv2
I1211 09:56:03.965008  6644 net.cpp:84] Creating Layer added_new_conv2
I1211 09:56:03.965008  6644 net.cpp:406] added_new_conv2 <- conv4_2
I1211 09:56:03.965008  6644 net.cpp:380] added_new_conv2 -> added_new_conv2
I1211 09:56:03.966509  6644 net.cpp:122] Setting up added_new_conv2
I1211 09:56:03.966509  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.966509  6644 net.cpp:137] Memory required for data: 527566000
I1211 09:56:03.966509  6644 layer_factory.cpp:58] Creating layer bn_added2
I1211 09:56:03.966509  6644 net.cpp:84] Creating Layer bn_added2
I1211 09:56:03.966509  6644 net.cpp:406] bn_added2 <- added_new_conv2
I1211 09:56:03.966509  6644 net.cpp:367] bn_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.967010  6644 net.cpp:122] Setting up bn_added2
I1211 09:56:03.967010  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.967010  6644 net.cpp:137] Memory required for data: 533505200
I1211 09:56:03.967010  6644 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:56:03.967010  6644 net.cpp:84] Creating Layer scale_added2
I1211 09:56:03.967010  6644 net.cpp:406] scale_added2 <- added_new_conv2
I1211 09:56:03.967010  6644 net.cpp:367] scale_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.967010  6644 layer_factory.cpp:58] Creating layer scale_added2
I1211 09:56:03.967010  6644 net.cpp:122] Setting up scale_added2
I1211 09:56:03.967010  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.967010  6644 net.cpp:137] Memory required for data: 539444400
I1211 09:56:03.967010  6644 layer_factory.cpp:58] Creating layer relu_added2
I1211 09:56:03.967010  6644 net.cpp:84] Creating Layer relu_added2
I1211 09:56:03.967010  6644 net.cpp:406] relu_added2 <- added_new_conv2
I1211 09:56:03.967010  6644 net.cpp:367] relu_added2 -> added_new_conv2 (in-place)
I1211 09:56:03.967509  6644 net.cpp:122] Setting up relu_added2
I1211 09:56:03.967509  6644 net.cpp:129] Top shape: 100 58 16 16 (1484800)
I1211 09:56:03.968010  6644 net.cpp:137] Memory required for data: 545383600
I1211 09:56:03.968010  6644 layer_factory.cpp:58] Creating layer pool4_2
I1211 09:56:03.968010  6644 net.cpp:84] Creating Layer pool4_2
I1211 09:56:03.968010  6644 net.cpp:406] pool4_2 <- added_new_conv2
I1211 09:56:03.968010  6644 net.cpp:380] pool4_2 -> pool4_2
I1211 09:56:03.968010  6644 net.cpp:122] Setting up pool4_2
I1211 09:56:03.968010  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.968010  6644 net.cpp:137] Memory required for data: 546868400
I1211 09:56:03.968010  6644 layer_factory.cpp:58] Creating layer conv4_0
I1211 09:56:03.968010  6644 net.cpp:84] Creating Layer conv4_0
I1211 09:56:03.968010  6644 net.cpp:406] conv4_0 <- pool4_2
I1211 09:56:03.968010  6644 net.cpp:380] conv4_0 -> conv4_0
I1211 09:56:03.970008  6644 net.cpp:122] Setting up conv4_0
I1211 09:56:03.970008  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.970008  6644 net.cpp:137] Memory required for data: 548353200
I1211 09:56:03.970008  6644 layer_factory.cpp:58] Creating layer bn4_0
I1211 09:56:03.970008  6644 net.cpp:84] Creating Layer bn4_0
I1211 09:56:03.970008  6644 net.cpp:406] bn4_0 <- conv4_0
I1211 09:56:03.970008  6644 net.cpp:367] bn4_0 -> conv4_0 (in-place)
I1211 09:56:03.970520  6644 net.cpp:122] Setting up bn4_0
I1211 09:56:03.970520  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.970520  6644 net.cpp:137] Memory required for data: 549838000
I1211 09:56:03.970520  6644 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:56:03.970520  6644 net.cpp:84] Creating Layer scale4_0
I1211 09:56:03.970520  6644 net.cpp:406] scale4_0 <- conv4_0
I1211 09:56:03.970520  6644 net.cpp:367] scale4_0 -> conv4_0 (in-place)
I1211 09:56:03.970520  6644 layer_factory.cpp:58] Creating layer scale4_0
I1211 09:56:03.970520  6644 net.cpp:122] Setting up scale4_0
I1211 09:56:03.970520  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.970520  6644 net.cpp:137] Memory required for data: 551322800
I1211 09:56:03.970520  6644 layer_factory.cpp:58] Creating layer relu4_0
I1211 09:56:03.970520  6644 net.cpp:84] Creating Layer relu4_0
I1211 09:56:03.970520  6644 net.cpp:406] relu4_0 <- conv4_0
I1211 09:56:03.970520  6644 net.cpp:367] relu4_0 -> conv4_0 (in-place)
I1211 09:56:03.971508  6644 net.cpp:122] Setting up relu4_0
I1211 09:56:03.971508  6644 net.cpp:129] Top shape: 100 58 8 8 (371200)
I1211 09:56:03.971508  6644 net.cpp:137] Memory required for data: 552807600
I1211 09:56:03.971508  6644 layer_factory.cpp:58] Creating layer conv11
I1211 09:56:03.971508  6644 net.cpp:84] Creating Layer conv11
I1211 09:56:03.971508  6644 net.cpp:406] conv11 <- conv4_0
I1211 09:56:03.971508  6644 net.cpp:380] conv11 -> conv11
I1211 09:56:03.972520  6644 net.cpp:122] Setting up conv11
I1211 09:56:03.972520  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.972520  6644 net.cpp:137] Memory required for data: 554599600
I1211 09:56:03.973006  6644 layer_factory.cpp:58] Creating layer bn_conv11
I1211 09:56:03.973006  6644 net.cpp:84] Creating Layer bn_conv11
I1211 09:56:03.973006  6644 net.cpp:406] bn_conv11 <- conv11
I1211 09:56:03.973006  6644 net.cpp:367] bn_conv11 -> conv11 (in-place)
I1211 09:56:03.973006  6644 net.cpp:122] Setting up bn_conv11
I1211 09:56:03.973006  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.973006  6644 net.cpp:137] Memory required for data: 556391600
I1211 09:56:03.973006  6644 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:56:03.973006  6644 net.cpp:84] Creating Layer scale_conv11
I1211 09:56:03.973006  6644 net.cpp:406] scale_conv11 <- conv11
I1211 09:56:03.973006  6644 net.cpp:367] scale_conv11 -> conv11 (in-place)
I1211 09:56:03.973006  6644 layer_factory.cpp:58] Creating layer scale_conv11
I1211 09:56:03.973006  6644 net.cpp:122] Setting up scale_conv11
I1211 09:56:03.973006  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.973006  6644 net.cpp:137] Memory required for data: 558183600
I1211 09:56:03.973006  6644 layer_factory.cpp:58] Creating layer relu_conv11
I1211 09:56:03.973006  6644 net.cpp:84] Creating Layer relu_conv11
I1211 09:56:03.973006  6644 net.cpp:406] relu_conv11 <- conv11
I1211 09:56:03.973006  6644 net.cpp:367] relu_conv11 -> conv11 (in-place)
I1211 09:56:03.973505  6644 net.cpp:122] Setting up relu_conv11
I1211 09:56:03.973505  6644 net.cpp:129] Top shape: 100 70 8 8 (448000)
I1211 09:56:03.973505  6644 net.cpp:137] Memory required for data: 559975600
I1211 09:56:03.973505  6644 layer_factory.cpp:58] Creating layer conv12
I1211 09:56:03.973505  6644 net.cpp:84] Creating Layer conv12
I1211 09:56:03.973505  6644 net.cpp:406] conv12 <- conv11
I1211 09:56:03.973505  6644 net.cpp:380] conv12 -> conv12
I1211 09:56:03.975008  6644 net.cpp:122] Setting up conv12
I1211 09:56:03.975008  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.975008  6644 net.cpp:137] Memory required for data: 562279600
I1211 09:56:03.975008  6644 layer_factory.cpp:58] Creating layer bn_conv12
I1211 09:56:03.975008  6644 net.cpp:84] Creating Layer bn_conv12
I1211 09:56:03.975008  6644 net.cpp:406] bn_conv12 <- conv12
I1211 09:56:03.975008  6644 net.cpp:367] bn_conv12 -> conv12 (in-place)
I1211 09:56:03.975507  6644 net.cpp:122] Setting up bn_conv12
I1211 09:56:03.975507  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.975507  6644 net.cpp:137] Memory required for data: 564583600
I1211 09:56:03.975507  6644 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:56:03.975507  6644 net.cpp:84] Creating Layer scale_conv12
I1211 09:56:03.975507  6644 net.cpp:406] scale_conv12 <- conv12
I1211 09:56:03.975507  6644 net.cpp:367] scale_conv12 -> conv12 (in-place)
I1211 09:56:03.975507  6644 layer_factory.cpp:58] Creating layer scale_conv12
I1211 09:56:03.975507  6644 net.cpp:122] Setting up scale_conv12
I1211 09:56:03.975507  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.975507  6644 net.cpp:137] Memory required for data: 566887600
I1211 09:56:03.975507  6644 layer_factory.cpp:58] Creating layer relu_conv12
I1211 09:56:03.975507  6644 net.cpp:84] Creating Layer relu_conv12
I1211 09:56:03.975507  6644 net.cpp:406] relu_conv12 <- conv12
I1211 09:56:03.975507  6644 net.cpp:367] relu_conv12 -> conv12 (in-place)
I1211 09:56:03.976006  6644 net.cpp:122] Setting up relu_conv12
I1211 09:56:03.976006  6644 net.cpp:129] Top shape: 100 90 8 8 (576000)
I1211 09:56:03.976006  6644 net.cpp:137] Memory required for data: 569191600
I1211 09:56:03.976006  6644 layer_factory.cpp:58] Creating layer poolcp6
I1211 09:56:03.976006  6644 net.cpp:84] Creating Layer poolcp6
I1211 09:56:03.976006  6644 net.cpp:406] poolcp6 <- conv12
I1211 09:56:03.976006  6644 net.cpp:380] poolcp6 -> poolcp6
I1211 09:56:03.976006  6644 net.cpp:122] Setting up poolcp6
I1211 09:56:03.976006  6644 net.cpp:129] Top shape: 100 90 1 1 (9000)
I1211 09:56:03.976006  6644 net.cpp:137] Memory required for data: 569227600
I1211 09:56:03.976006  6644 layer_factory.cpp:58] Creating layer ip1
I1211 09:56:03.976006  6644 net.cpp:84] Creating Layer ip1
I1211 09:56:03.976006  6644 net.cpp:406] ip1 <- poolcp6
I1211 09:56:03.976006  6644 net.cpp:380] ip1 -> ip1
I1211 09:56:03.976006  6644 net.cpp:122] Setting up ip1
I1211 09:56:03.976006  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.976006  6644 net.cpp:137] Memory required for data: 569267600
I1211 09:56:03.976506  6644 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I1211 09:56:03.976506  6644 net.cpp:84] Creating Layer ip1_ip1_0_split
I1211 09:56:03.976506  6644 net.cpp:406] ip1_ip1_0_split <- ip1
I1211 09:56:03.976506  6644 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_0
I1211 09:56:03.976506  6644 net.cpp:380] ip1_ip1_0_split -> ip1_ip1_0_split_1
I1211 09:56:03.976506  6644 net.cpp:122] Setting up ip1_ip1_0_split
I1211 09:56:03.976506  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.976506  6644 net.cpp:129] Top shape: 100 100 (10000)
I1211 09:56:03.976506  6644 net.cpp:137] Memory required for data: 569347600
I1211 09:56:03.976506  6644 layer_factory.cpp:58] Creating layer accuracy
I1211 09:56:03.976506  6644 net.cpp:84] Creating Layer accuracy
I1211 09:56:03.976506  6644 net.cpp:406] accuracy <- ip1_ip1_0_split_0
I1211 09:56:03.976506  6644 net.cpp:406] accuracy <- label_cifar_1_split_0
I1211 09:56:03.976506  6644 net.cpp:380] accuracy -> accuracy
I1211 09:56:03.976506  6644 net.cpp:122] Setting up accuracy
I1211 09:56:03.976506  6644 net.cpp:129] Top shape: (1)
I1211 09:56:03.976506  6644 net.cpp:137] Memory required for data: 569347604
I1211 09:56:03.976506  6644 layer_factory.cpp:58] Creating layer loss
I1211 09:56:03.976506  6644 net.cpp:84] Creating Layer loss
I1211 09:56:03.976506  6644 net.cpp:406] loss <- ip1_ip1_0_split_1
I1211 09:56:03.976506  6644 net.cpp:406] loss <- label_cifar_1_split_1
I1211 09:56:03.976506  6644 net.cpp:380] loss -> loss
I1211 09:56:03.976506  6644 layer_factory.cpp:58] Creating layer loss
I1211 09:56:03.976506  6644 net.cpp:122] Setting up loss
I1211 09:56:03.976506  6644 net.cpp:129] Top shape: (1)
I1211 09:56:03.976506  6644 net.cpp:132]     with loss weight 1
I1211 09:56:03.976506  6644 net.cpp:137] Memory required for data: 569347608
I1211 09:56:03.977006  6644 net.cpp:198] loss needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:200] accuracy does not need backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] ip1_ip1_0_split needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] ip1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] poolcp6 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu_conv12 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale_conv12 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn_conv12 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv12 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu_conv11 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale_conv11 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn_conv11 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv11 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu4_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale4_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn4_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv4_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] pool4_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu_added2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale_added2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn_added2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] added_new_conv2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu4_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale4_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn4_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv4_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu4_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale4_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn4_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv4_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu4 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale4 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn4 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv4 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu3_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale3_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn3_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv3_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu3 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale3 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn3 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv3 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] pool2_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu_added1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale_added1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn_added1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] newconv_added1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu2_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale2_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn2_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv2_2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu2_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale2_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn2_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv2_1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv2 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu1_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale1_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn1_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv1_0 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] relu1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] scale1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] bn1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:198] conv1 needs backward computation.
I1211 09:56:03.977006  6644 net.cpp:200] label_cifar_1_split does not need backward computation.
I1211 09:56:03.977006  6644 net.cpp:200] cifar does not need backward computation.
I1211 09:56:03.977006  6644 net.cpp:242] This network produces output accuracy
I1211 09:56:03.977006  6644 net.cpp:242] This network produces output loss
I1211 09:56:03.977505  6644 net.cpp:255] Network initialization done.
I1211 09:56:03.977505  6644 solver.cpp:56] Solver scaffolding done.
I1211 09:56:03.982506  6644 caffe.cpp:243] Resuming from examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.solverstate
I1211 09:56:03.986506  6644 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90000.caffemodel
I1211 09:56:03.986506  6644 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I1211 09:56:03.987006  6644 sgd_solver.cpp:318] SGDSolver: restoring history
I1211 09:56:03.991006  6644 caffe.cpp:249] Starting Optimization
I1211 09:56:03.991006  6644 solver.cpp:272] Solving CIFAR100_SimpleNet_GP_15L_Simple_NoGrpCon_NoDrp_max_360k
I1211 09:56:03.991006  6644 solver.cpp:273] Learning Rate Policy: multistep
I1211 09:56:03.993505  6644 solver.cpp:330] Iteration 90000, Testing net (#0)
I1211 09:56:03.995506  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:56:05.599509 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:56:05.662009  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5908
I1211 09:56:05.662009  6644 solver.cpp:397]     Test net output #1: loss = 1.61171 (* 1 = 1.61171 loss)
I1211 09:56:05.783100  6644 solver.cpp:218] Iteration 90000 (50254.6 iter/s, 1.79088s/100 iters), loss = 0.74886
I1211 09:56:05.783100  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:56:05.783100  6644 solver.cpp:237]     Train net output #1: loss = 0.74886 (* 1 = 0.74886 loss)
I1211 09:56:05.783100  6644 sgd_solver.cpp:105] Iteration 90000, lr = 0.01
I1211 09:56:12.399058  6644 solver.cpp:218] Iteration 90100 (15.1152 iter/s, 6.61584s/100 iters), loss = 0.727553
I1211 09:56:12.399058  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:56:12.399058  6644 solver.cpp:237]     Train net output #1: loss = 0.727553 (* 1 = 0.727553 loss)
I1211 09:56:12.399058  6644 sgd_solver.cpp:105] Iteration 90100, lr = 0.01
I1211 09:56:18.928129  6644 solver.cpp:218] Iteration 90200 (15.3174 iter/s, 6.52854s/100 iters), loss = 0.580457
I1211 09:56:18.928129  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 09:56:18.928129  6644 solver.cpp:237]     Train net output #1: loss = 0.580457 (* 1 = 0.580457 loss)
I1211 09:56:18.928129  6644 sgd_solver.cpp:105] Iteration 90200, lr = 0.01
I1211 09:56:25.365278  6644 solver.cpp:218] Iteration 90300 (15.536 iter/s, 6.43666s/100 iters), loss = 0.787419
I1211 09:56:25.365278  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 09:56:25.365278  6644 solver.cpp:237]     Train net output #1: loss = 0.787419 (* 1 = 0.787419 loss)
I1211 09:56:25.365278  6644 sgd_solver.cpp:105] Iteration 90300, lr = 0.01
I1211 09:56:31.756023  6644 solver.cpp:218] Iteration 90400 (15.6486 iter/s, 6.39036s/100 iters), loss = 0.786855
I1211 09:56:31.756023  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 09:56:31.756023  6644 solver.cpp:237]     Train net output #1: loss = 0.786855 (* 1 = 0.786855 loss)
I1211 09:56:31.756023  6644 sgd_solver.cpp:105] Iteration 90400, lr = 0.01
I1211 09:56:37.924861  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:56:38.179360  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.caffemodel
I1211 09:56:38.196861  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_90500.solverstate
I1211 09:56:38.201867  6644 solver.cpp:330] Iteration 90500, Testing net (#0)
I1211 09:56:38.201867  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:56:39.750955 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:56:39.812454  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5922
I1211 09:56:39.812454  6644 solver.cpp:397]     Test net output #1: loss = 1.65469 (* 1 = 1.65469 loss)
I1211 09:56:39.874469  6644 solver.cpp:218] Iteration 90500 (12.3189 iter/s, 8.11762s/100 iters), loss = 0.780077
I1211 09:56:39.874469  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:56:39.874469  6644 solver.cpp:237]     Train net output #1: loss = 0.780077 (* 1 = 0.780077 loss)
I1211 09:56:39.874469  6644 sgd_solver.cpp:105] Iteration 90500, lr = 0.01
I1211 09:56:46.333156  6644 solver.cpp:218] Iteration 90600 (15.4839 iter/s, 6.45831s/100 iters), loss = 0.706577
I1211 09:56:46.333156  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:56:46.333156  6644 solver.cpp:237]     Train net output #1: loss = 0.706577 (* 1 = 0.706577 loss)
I1211 09:56:46.333156  6644 sgd_solver.cpp:105] Iteration 90600, lr = 0.01
I1211 09:56:52.794472  6644 solver.cpp:218] Iteration 90700 (15.4782 iter/s, 6.46069s/100 iters), loss = 0.674699
I1211 09:56:52.794472  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:56:52.794472  6644 solver.cpp:237]     Train net output #1: loss = 0.674699 (* 1 = 0.674699 loss)
I1211 09:56:52.794472  6644 sgd_solver.cpp:105] Iteration 90700, lr = 0.01
I1211 09:56:59.193801  6644 solver.cpp:218] Iteration 90800 (15.6258 iter/s, 6.39966s/100 iters), loss = 0.764325
I1211 09:56:59.193801  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:56:59.193801  6644 solver.cpp:237]     Train net output #1: loss = 0.764325 (* 1 = 0.764325 loss)
I1211 09:56:59.193801  6644 sgd_solver.cpp:105] Iteration 90800, lr = 0.01
I1211 09:57:05.582619  6644 solver.cpp:218] Iteration 90900 (15.6547 iter/s, 6.38786s/100 iters), loss = 0.704639
I1211 09:57:05.582619  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:57:05.582619  6644 solver.cpp:237]     Train net output #1: loss = 0.704639 (* 1 = 0.704639 loss)
I1211 09:57:05.582619  6644 sgd_solver.cpp:105] Iteration 90900, lr = 0.01
I1211 09:57:11.779556  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:57:12.037056  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.caffemodel
I1211 09:57:12.052556  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91000.solverstate
I1211 09:57:12.058055  6644 solver.cpp:330] Iteration 91000, Testing net (#0)
I1211 09:57:12.058055  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:57:13.618571 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:57:13.681059  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5799
I1211 09:57:13.681059  6644 solver.cpp:397]     Test net output #1: loss = 1.68893 (* 1 = 1.68893 loss)
I1211 09:57:13.743556  6644 solver.cpp:218] Iteration 91000 (12.254 iter/s, 8.16061s/100 iters), loss = 0.644151
I1211 09:57:13.743556  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:57:13.744055  6644 solver.cpp:237]     Train net output #1: loss = 0.644151 (* 1 = 0.644151 loss)
I1211 09:57:13.744055  6644 sgd_solver.cpp:105] Iteration 91000, lr = 0.01
I1211 09:57:20.206327  6644 solver.cpp:218] Iteration 91100 (15.4749 iter/s, 6.46207s/100 iters), loss = 0.716134
I1211 09:57:20.206327  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:57:20.206327  6644 solver.cpp:237]     Train net output #1: loss = 0.716134 (* 1 = 0.716134 loss)
I1211 09:57:20.206327  6644 sgd_solver.cpp:105] Iteration 91100, lr = 0.01
I1211 09:57:26.655711  6644 solver.cpp:218] Iteration 91200 (15.5059 iter/s, 6.44914s/100 iters), loss = 0.64518
I1211 09:57:26.656213  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:57:26.656213  6644 solver.cpp:237]     Train net output #1: loss = 0.64518 (* 1 = 0.64518 loss)
I1211 09:57:26.656213  6644 sgd_solver.cpp:105] Iteration 91200, lr = 0.01
I1211 09:57:33.109910  6644 solver.cpp:218] Iteration 91300 (15.4954 iter/s, 6.45354s/100 iters), loss = 0.70031
I1211 09:57:33.109910  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:57:33.109910  6644 solver.cpp:237]     Train net output #1: loss = 0.70031 (* 1 = 0.70031 loss)
I1211 09:57:33.109910  6644 sgd_solver.cpp:105] Iteration 91300, lr = 0.01
I1211 09:57:39.566751  6644 solver.cpp:218] Iteration 91400 (15.4889 iter/s, 6.45622s/100 iters), loss = 0.72607
I1211 09:57:39.566751  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:57:39.566751  6644 solver.cpp:237]     Train net output #1: loss = 0.72607 (* 1 = 0.72607 loss)
I1211 09:57:39.566751  6644 sgd_solver.cpp:105] Iteration 91400, lr = 0.01
I1211 09:57:45.703399  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:57:45.959398  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.caffemodel
I1211 09:57:45.974397  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_91500.solverstate
I1211 09:57:45.979398  6644 solver.cpp:330] Iteration 91500, Testing net (#0)
I1211 09:57:45.979897  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:57:47.520401 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:57:47.581899  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5875
I1211 09:57:47.581899  6644 solver.cpp:397]     Test net output #1: loss = 1.68442 (* 1 = 1.68442 loss)
I1211 09:57:47.643396  6644 solver.cpp:218] Iteration 91500 (12.3819 iter/s, 8.0763s/100 iters), loss = 0.609484
I1211 09:57:47.643396  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 09:57:47.643396  6644 solver.cpp:237]     Train net output #1: loss = 0.609484 (* 1 = 0.609484 loss)
I1211 09:57:47.643396  6644 sgd_solver.cpp:105] Iteration 91500, lr = 0.01
I1211 09:57:54.124364  6644 solver.cpp:218] Iteration 91600 (15.4312 iter/s, 6.48037s/100 iters), loss = 0.779886
I1211 09:57:54.124364  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1211 09:57:54.124364  6644 solver.cpp:237]     Train net output #1: loss = 0.779886 (* 1 = 0.779886 loss)
I1211 09:57:54.124364  6644 sgd_solver.cpp:105] Iteration 91600, lr = 0.01
I1211 09:58:00.561038  6644 solver.cpp:218] Iteration 91700 (15.5371 iter/s, 6.43622s/100 iters), loss = 0.589501
I1211 09:58:00.561038  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:58:00.561038  6644 solver.cpp:237]     Train net output #1: loss = 0.589501 (* 1 = 0.589501 loss)
I1211 09:58:00.561038  6644 sgd_solver.cpp:105] Iteration 91700, lr = 0.01
I1211 09:58:07.009543  6644 solver.cpp:218] Iteration 91800 (15.5083 iter/s, 6.44814s/100 iters), loss = 0.743481
I1211 09:58:07.009543  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.75
I1211 09:58:07.009543  6644 solver.cpp:237]     Train net output #1: loss = 0.743481 (* 1 = 0.743481 loss)
I1211 09:58:07.009543  6644 sgd_solver.cpp:105] Iteration 91800, lr = 0.01
I1211 09:58:13.485121  6644 solver.cpp:218] Iteration 91900 (15.4437 iter/s, 6.47512s/100 iters), loss = 0.748372
I1211 09:58:13.485622  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:58:13.485622  6644 solver.cpp:237]     Train net output #1: loss = 0.748372 (* 1 = 0.748372 loss)
I1211 09:58:13.485622  6644 sgd_solver.cpp:105] Iteration 91900, lr = 0.01
I1211 09:58:19.692080  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:58:19.952109  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.caffemodel
I1211 09:58:19.969110  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92000.solverstate
I1211 09:58:19.975109  6644 solver.cpp:330] Iteration 92000, Testing net (#0)
I1211 09:58:19.975109  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:58:21.535851 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:58:21.597357  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5941
I1211 09:58:21.598357  6644 solver.cpp:397]     Test net output #1: loss = 1.63361 (* 1 = 1.63361 loss)
I1211 09:58:21.659364  6644 solver.cpp:218] Iteration 92000 (12.2338 iter/s, 8.17404s/100 iters), loss = 0.536928
I1211 09:58:21.659364  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 09:58:21.659364  6644 solver.cpp:237]     Train net output #1: loss = 0.536928 (* 1 = 0.536928 loss)
I1211 09:58:21.659364  6644 sgd_solver.cpp:105] Iteration 92000, lr = 0.01
I1211 09:58:28.043905  6644 solver.cpp:218] Iteration 92100 (15.6637 iter/s, 6.38419s/100 iters), loss = 0.750844
I1211 09:58:28.044906  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:58:28.044906  6644 solver.cpp:237]     Train net output #1: loss = 0.750844 (* 1 = 0.750844 loss)
I1211 09:58:28.044906  6644 sgd_solver.cpp:105] Iteration 92100, lr = 0.01
I1211 09:58:34.453764  6644 solver.cpp:218] Iteration 92200 (15.6043 iter/s, 6.40847s/100 iters), loss = 0.627812
I1211 09:58:34.453764  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 09:58:34.453764  6644 solver.cpp:237]     Train net output #1: loss = 0.627812 (* 1 = 0.627812 loss)
I1211 09:58:34.453764  6644 sgd_solver.cpp:105] Iteration 92200, lr = 0.01
I1211 09:58:40.866192  6644 solver.cpp:218] Iteration 92300 (15.5956 iter/s, 6.41205s/100 iters), loss = 0.805765
I1211 09:58:40.866192  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 09:58:40.866192  6644 solver.cpp:237]     Train net output #1: loss = 0.805765 (* 1 = 0.805765 loss)
I1211 09:58:40.866192  6644 sgd_solver.cpp:105] Iteration 92300, lr = 0.01
I1211 09:58:47.280699  6644 solver.cpp:218] Iteration 92400 (15.5896 iter/s, 6.41455s/100 iters), loss = 0.786207
I1211 09:58:47.280699  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:58:47.280699  6644 solver.cpp:237]     Train net output #1: loss = 0.786207 (* 1 = 0.786207 loss)
I1211 09:58:47.280699  6644 sgd_solver.cpp:105] Iteration 92400, lr = 0.01
I1211 09:58:53.385179  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:58:53.634690  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.caffemodel
I1211 09:58:53.650192  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_92500.solverstate
I1211 09:58:53.655194  6644 solver.cpp:330] Iteration 92500, Testing net (#0)
I1211 09:58:53.655194  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:58:55.187332 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:58:55.248337  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5983
I1211 09:58:55.248337  6644 solver.cpp:397]     Test net output #1: loss = 1.52032 (* 1 = 1.52032 loss)
I1211 09:58:55.310336  6644 solver.cpp:218] Iteration 92500 (12.4551 iter/s, 8.02885s/100 iters), loss = 0.561092
I1211 09:58:55.310336  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 09:58:55.310336  6644 solver.cpp:237]     Train net output #1: loss = 0.561092 (* 1 = 0.561092 loss)
I1211 09:58:55.310336  6644 sgd_solver.cpp:105] Iteration 92500, lr = 0.01
I1211 09:59:01.702808  6644 solver.cpp:218] Iteration 92600 (15.644 iter/s, 6.39223s/100 iters), loss = 0.804297
I1211 09:59:01.702808  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.76
I1211 09:59:01.702808  6644 solver.cpp:237]     Train net output #1: loss = 0.804297 (* 1 = 0.804297 loss)
I1211 09:59:01.702808  6644 sgd_solver.cpp:105] Iteration 92600, lr = 0.01
I1211 09:59:08.043275  6644 solver.cpp:218] Iteration 92700 (15.774 iter/s, 6.33954s/100 iters), loss = 0.637826
I1211 09:59:08.043275  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 09:59:08.043275  6644 solver.cpp:237]     Train net output #1: loss = 0.637826 (* 1 = 0.637826 loss)
I1211 09:59:08.043275  6644 sgd_solver.cpp:105] Iteration 92700, lr = 0.01
I1211 09:59:14.372737  6644 solver.cpp:218] Iteration 92800 (15.8004 iter/s, 6.32897s/100 iters), loss = 0.893816
I1211 09:59:14.372737  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.71
I1211 09:59:14.372737  6644 solver.cpp:237]     Train net output #1: loss = 0.893816 (* 1 = 0.893816 loss)
I1211 09:59:14.372737  6644 sgd_solver.cpp:105] Iteration 92800, lr = 0.01
I1211 09:59:20.713228  6644 solver.cpp:218] Iteration 92900 (15.7712 iter/s, 6.34069s/100 iters), loss = 0.648221
I1211 09:59:20.713228  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 09:59:20.713228  6644 solver.cpp:237]     Train net output #1: loss = 0.648221 (* 1 = 0.648221 loss)
I1211 09:59:20.713228  6644 sgd_solver.cpp:105] Iteration 92900, lr = 0.01
I1211 09:59:26.731652  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:59:26.981164  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.caffemodel
I1211 09:59:26.995668  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93000.solverstate
I1211 09:59:27.000669  6644 solver.cpp:330] Iteration 93000, Testing net (#0)
I1211 09:59:27.000669  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 09:59:28.518770 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 09:59:28.578773  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5883
I1211 09:59:28.579272  6644 solver.cpp:397]     Test net output #1: loss = 1.64734 (* 1 = 1.64734 loss)
I1211 09:59:28.639775  6644 solver.cpp:218] Iteration 93000 (12.6165 iter/s, 7.92613s/100 iters), loss = 0.524396
I1211 09:59:28.639775  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 09:59:28.639775  6644 solver.cpp:237]     Train net output #1: loss = 0.524396 (* 1 = 0.524396 loss)
I1211 09:59:28.639775  6644 sgd_solver.cpp:105] Iteration 93000, lr = 0.01
I1211 09:59:34.987257  6644 solver.cpp:218] Iteration 93100 (15.7552 iter/s, 6.34711s/100 iters), loss = 0.759937
I1211 09:59:34.987257  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 09:59:34.987257  6644 solver.cpp:237]     Train net output #1: loss = 0.759937 (* 1 = 0.759937 loss)
I1211 09:59:34.987257  6644 sgd_solver.cpp:105] Iteration 93100, lr = 0.01
I1211 09:59:41.327692  6644 solver.cpp:218] Iteration 93200 (15.7748 iter/s, 6.33922s/100 iters), loss = 0.640482
I1211 09:59:41.327692  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 09:59:41.327692  6644 solver.cpp:237]     Train net output #1: loss = 0.640482 (* 1 = 0.640482 loss)
I1211 09:59:41.327692  6644 sgd_solver.cpp:105] Iteration 93200, lr = 0.01
I1211 09:59:47.662170  6644 solver.cpp:218] Iteration 93300 (15.787 iter/s, 6.33434s/100 iters), loss = 0.680894
I1211 09:59:47.662170  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.77
I1211 09:59:47.662170  6644 solver.cpp:237]     Train net output #1: loss = 0.680894 (* 1 = 0.680894 loss)
I1211 09:59:47.662170  6644 sgd_solver.cpp:105] Iteration 93300, lr = 0.01
I1211 09:59:54.001634  6644 solver.cpp:218] Iteration 93400 (15.7748 iter/s, 6.33923s/100 iters), loss = 0.805161
I1211 09:59:54.001634  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.73
I1211 09:59:54.001634  6644 solver.cpp:237]     Train net output #1: loss = 0.805161 (* 1 = 0.805161 loss)
I1211 09:59:54.001634  6644 sgd_solver.cpp:105] Iteration 93400, lr = 0.01
I1211 10:00:00.031076  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:00:00.286105  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.caffemodel
I1211 10:00:00.301107  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_93500.solverstate
I1211 10:00:00.306107  6644 solver.cpp:330] Iteration 93500, Testing net (#0)
I1211 10:00:00.306107  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:00:01.849839 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:00:01.911350  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5693
I1211 10:00:01.911350  6644 solver.cpp:397]     Test net output #1: loss = 1.71227 (* 1 = 1.71227 loss)
I1211 10:00:01.971853  6644 solver.cpp:218] Iteration 93500 (12.5484 iter/s, 7.96912s/100 iters), loss = 0.616335
I1211 10:00:01.971853  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 10:00:01.971853  6644 solver.cpp:237]     Train net output #1: loss = 0.616335 (* 1 = 0.616335 loss)
I1211 10:00:01.971853  6644 sgd_solver.cpp:105] Iteration 93500, lr = 0.01
I1211 10:00:08.318861  6644 solver.cpp:218] Iteration 93600 (15.7543 iter/s, 6.34749s/100 iters), loss = 0.694786
I1211 10:00:08.318861  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:00:08.319862  6644 solver.cpp:237]     Train net output #1: loss = 0.694786 (* 1 = 0.694786 loss)
I1211 10:00:08.319862  6644 sgd_solver.cpp:105] Iteration 93600, lr = 0.01
I1211 10:00:14.665357  6644 solver.cpp:218] Iteration 93700 (15.7593 iter/s, 6.34547s/100 iters), loss = 0.567738
I1211 10:00:14.665357  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 10:00:14.665357  6644 solver.cpp:237]     Train net output #1: loss = 0.567738 (* 1 = 0.567738 loss)
I1211 10:00:14.665357  6644 sgd_solver.cpp:105] Iteration 93700, lr = 0.01
I1211 10:00:21.013762  6644 solver.cpp:218] Iteration 93800 (15.7528 iter/s, 6.34806s/100 iters), loss = 0.853514
I1211 10:00:21.013762  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.72
I1211 10:00:21.013762  6644 solver.cpp:237]     Train net output #1: loss = 0.853514 (* 1 = 0.853514 loss)
I1211 10:00:21.013762  6644 sgd_solver.cpp:105] Iteration 93800, lr = 0.01
I1211 10:00:27.353176  6644 solver.cpp:218] Iteration 93900 (15.7756 iter/s, 6.33889s/100 iters), loss = 0.762829
I1211 10:00:27.353176  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.74
I1211 10:00:27.353176  6644 solver.cpp:237]     Train net output #1: loss = 0.762829 (* 1 = 0.762829 loss)
I1211 10:00:27.353176  6644 sgd_solver.cpp:105] Iteration 93900, lr = 0.01
I1211 10:00:33.386593  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:00:33.636612  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.caffemodel
I1211 10:00:33.653607  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94000.solverstate
I1211 10:00:33.658607  6644 solver.cpp:330] Iteration 94000, Testing net (#0)
I1211 10:00:33.658607  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:00:35.176707 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:00:35.236706  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5947
I1211 10:00:35.236706  6644 solver.cpp:397]     Test net output #1: loss = 1.62396 (* 1 = 1.62396 loss)
I1211 10:00:35.297718  6644 solver.cpp:218] Iteration 94000 (12.5876 iter/s, 7.94431s/100 iters), loss = 0.753848
I1211 10:00:35.297718  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:00:35.297718  6644 solver.cpp:237]     Train net output #1: loss = 0.753848 (* 1 = 0.753848 loss)
I1211 10:00:35.297718  6644 sgd_solver.cpp:105] Iteration 94000, lr = 0.01
I1211 10:00:41.644161  6644 solver.cpp:218] Iteration 94100 (15.7598 iter/s, 6.34528s/100 iters), loss = 0.690162
I1211 10:00:41.644161  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:00:41.644161  6644 solver.cpp:237]     Train net output #1: loss = 0.690162 (* 1 = 0.690162 loss)
I1211 10:00:41.644161  6644 sgd_solver.cpp:105] Iteration 94100, lr = 0.01
I1211 10:00:47.988628  6644 solver.cpp:218] Iteration 94200 (15.7613 iter/s, 6.34465s/100 iters), loss = 0.600768
I1211 10:00:47.988628  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:00:47.988628  6644 solver.cpp:237]     Train net output #1: loss = 0.600768 (* 1 = 0.600768 loss)
I1211 10:00:47.988628  6644 sgd_solver.cpp:105] Iteration 94200, lr = 0.01
I1211 10:00:54.330129  6644 solver.cpp:218] Iteration 94300 (15.7714 iter/s, 6.34058s/100 iters), loss = 0.767881
I1211 10:00:54.330129  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.79
I1211 10:00:54.330129  6644 solver.cpp:237]     Train net output #1: loss = 0.767881 (* 1 = 0.767881 loss)
I1211 10:00:54.330129  6644 sgd_solver.cpp:105] Iteration 94300, lr = 0.01
I1211 10:01:00.680559  6644 solver.cpp:218] Iteration 94400 (15.7475 iter/s, 6.35021s/100 iters), loss = 0.864602
I1211 10:01:00.680559  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.7
I1211 10:01:00.680559  6644 solver.cpp:237]     Train net output #1: loss = 0.864602 (* 1 = 0.864602 loss)
I1211 10:01:00.680559  6644 sgd_solver.cpp:105] Iteration 94400, lr = 0.01
I1211 10:01:06.717970  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:01:06.968484  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.caffemodel
I1211 10:01:06.982987  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_94500.solverstate
I1211 10:01:06.987987  6644 solver.cpp:330] Iteration 94500, Testing net (#0)
I1211 10:01:06.987987  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:01:08.506067 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:01:08.566570  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5774
I1211 10:01:08.566570  6644 solver.cpp:397]     Test net output #1: loss = 1.68988 (* 1 = 1.68988 loss)
I1211 10:01:08.627079  6644 solver.cpp:218] Iteration 94500 (12.5839 iter/s, 7.94663s/100 iters), loss = 0.702665
I1211 10:01:08.628072  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:01:08.628072  6644 solver.cpp:237]     Train net output #1: loss = 0.702665 (* 1 = 0.702665 loss)
I1211 10:01:08.628072  6644 sgd_solver.cpp:105] Iteration 94500, lr = 0.01
I1211 10:01:14.980957  6644 solver.cpp:218] Iteration 94600 (15.7411 iter/s, 6.3528s/100 iters), loss = 0.574747
I1211 10:01:14.980957  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:01:14.980957  6644 solver.cpp:237]     Train net output #1: loss = 0.574747 (* 1 = 0.574747 loss)
I1211 10:01:14.980957  6644 sgd_solver.cpp:105] Iteration 94600, lr = 0.01
I1211 10:01:21.328667  6644 solver.cpp:218] Iteration 94700 (15.7539 iter/s, 6.34762s/100 iters), loss = 0.643087
I1211 10:01:21.328667  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:01:21.328667  6644 solver.cpp:237]     Train net output #1: loss = 0.643087 (* 1 = 0.643087 loss)
I1211 10:01:21.328667  6644 sgd_solver.cpp:105] Iteration 94700, lr = 0.01
I1211 10:01:27.678211  6644 solver.cpp:218] Iteration 94800 (15.7505 iter/s, 6.349s/100 iters), loss = 0.668323
I1211 10:01:27.678211  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:01:27.678211  6644 solver.cpp:237]     Train net output #1: loss = 0.668323 (* 1 = 0.668323 loss)
I1211 10:01:27.678211  6644 sgd_solver.cpp:105] Iteration 94800, lr = 0.01
I1211 10:01:34.035459  6644 solver.cpp:218] Iteration 94900 (15.7318 iter/s, 6.35656s/100 iters), loss = 0.788767
I1211 10:01:34.035459  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.78
I1211 10:01:34.035459  6644 solver.cpp:237]     Train net output #1: loss = 0.788767 (* 1 = 0.788767 loss)
I1211 10:01:34.035459  6644 sgd_solver.cpp:105] Iteration 94900, lr = 0.01
I1211 10:01:40.079919  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:01:40.331933  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.caffemodel
I1211 10:01:40.348932  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95000.solverstate
I1211 10:01:40.353932  6644 solver.cpp:330] Iteration 95000, Testing net (#0)
I1211 10:01:40.353932  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:01:41.873034 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:01:41.934037  6644 solver.cpp:397]     Test net output #0: accuracy = 0.5906
I1211 10:01:41.934037  6644 solver.cpp:397]     Test net output #1: loss = 1.61369 (* 1 = 1.61369 loss)
I1211 10:01:41.994035  6644 solver.cpp:218] Iteration 95000 (12.5647 iter/s, 7.95877s/100 iters), loss = 0.681698
I1211 10:01:41.994035  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:01:41.994035  6644 solver.cpp:237]     Train net output #1: loss = 0.681698 (* 1 = 0.681698 loss)
I1211 10:01:41.995038  6644 sgd_solver.cpp:46] MultiStep Status: Iteration 95000, step = 2
I1211 10:01:41.995038  6644 sgd_solver.cpp:105] Iteration 95000, lr = 0.001
I1211 10:01:48.336535  6644 solver.cpp:218] Iteration 95100 (15.7683 iter/s, 6.34183s/100 iters), loss = 0.676165
I1211 10:01:48.336535  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:01:48.336535  6644 solver.cpp:237]     Train net output #1: loss = 0.676165 (* 1 = 0.676165 loss)
I1211 10:01:48.336535  6644 sgd_solver.cpp:105] Iteration 95100, lr = 0.001
I1211 10:01:54.680008  6644 solver.cpp:218] Iteration 95200 (15.7663 iter/s, 6.34263s/100 iters), loss = 0.495116
I1211 10:01:54.680008  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:01:54.680008  6644 solver.cpp:237]     Train net output #1: loss = 0.495116 (* 1 = 0.495116 loss)
I1211 10:01:54.680008  6644 sgd_solver.cpp:105] Iteration 95200, lr = 0.001
I1211 10:02:01.015475  6644 solver.cpp:218] Iteration 95300 (15.7843 iter/s, 6.33543s/100 iters), loss = 0.584928
I1211 10:02:01.015475  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:02:01.015475  6644 solver.cpp:237]     Train net output #1: loss = 0.584928 (* 1 = 0.584928 loss)
I1211 10:02:01.015475  6644 sgd_solver.cpp:105] Iteration 95300, lr = 0.001
I1211 10:02:07.363934  6644 solver.cpp:218] Iteration 95400 (15.7537 iter/s, 6.34771s/100 iters), loss = 0.484258
I1211 10:02:07.363934  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:02:07.363934  6644 solver.cpp:237]     Train net output #1: loss = 0.484258 (* 1 = 0.484258 loss)
I1211 10:02:07.363934  6644 sgd_solver.cpp:105] Iteration 95400, lr = 0.001
I1211 10:02:13.394389  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:02:13.644408  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.caffemodel
I1211 10:02:13.659409  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_95500.solverstate
I1211 10:02:13.664408  6644 solver.cpp:330] Iteration 95500, Testing net (#0)
I1211 10:02:13.664408  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:02:15.183508 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:02:15.243511  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6758
I1211 10:02:15.243511  6644 solver.cpp:397]     Test net output #1: loss = 1.18629 (* 1 = 1.18629 loss)
I1211 10:02:15.303510  6644 solver.cpp:218] Iteration 95500 (12.5954 iter/s, 7.93941s/100 iters), loss = 0.524959
I1211 10:02:15.303510  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:02:15.303510  6644 solver.cpp:237]     Train net output #1: loss = 0.524958 (* 1 = 0.524958 loss)
I1211 10:02:15.303510  6644 sgd_solver.cpp:105] Iteration 95500, lr = 0.001
I1211 10:02:21.640959  6644 solver.cpp:218] Iteration 95600 (15.7796 iter/s, 6.33729s/100 iters), loss = 0.456213
I1211 10:02:21.640959  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:02:21.640959  6644 solver.cpp:237]     Train net output #1: loss = 0.456213 (* 1 = 0.456213 loss)
I1211 10:02:21.640959  6644 sgd_solver.cpp:105] Iteration 95600, lr = 0.001
I1211 10:02:27.979399  6644 solver.cpp:218] Iteration 95700 (15.779 iter/s, 6.33754s/100 iters), loss = 0.393115
I1211 10:02:27.979399  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:02:27.979399  6644 solver.cpp:237]     Train net output #1: loss = 0.393115 (* 1 = 0.393115 loss)
I1211 10:02:27.979399  6644 sgd_solver.cpp:105] Iteration 95700, lr = 0.001
I1211 10:02:34.303902  6644 solver.cpp:218] Iteration 95800 (15.8117 iter/s, 6.32444s/100 iters), loss = 0.480946
I1211 10:02:34.303902  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:02:34.303902  6644 solver.cpp:237]     Train net output #1: loss = 0.480945 (* 1 = 0.480945 loss)
I1211 10:02:34.303902  6644 sgd_solver.cpp:105] Iteration 95800, lr = 0.001
I1211 10:02:40.640419  6644 solver.cpp:218] Iteration 95900 (15.7823 iter/s, 6.3362s/100 iters), loss = 0.46324
I1211 10:02:40.640419  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.83
I1211 10:02:40.640419  6644 solver.cpp:237]     Train net output #1: loss = 0.46324 (* 1 = 0.46324 loss)
I1211 10:02:40.640419  6644 sgd_solver.cpp:105] Iteration 95900, lr = 0.001
I1211 10:02:46.656945  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:02:46.904965  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.caffemodel
I1211 10:02:46.920970  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96000.solverstate
I1211 10:02:46.926471  6644 solver.cpp:330] Iteration 96000, Testing net (#0)
I1211 10:02:46.926471  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:02:48.443087 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:02:48.504087  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6777
I1211 10:02:48.504087  6644 solver.cpp:397]     Test net output #1: loss = 1.17983 (* 1 = 1.17983 loss)
I1211 10:02:48.565093  6644 solver.cpp:218] Iteration 96000 (12.6208 iter/s, 7.92344s/100 iters), loss = 0.496982
I1211 10:02:48.565093  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:02:48.565093  6644 solver.cpp:237]     Train net output #1: loss = 0.496982 (* 1 = 0.496982 loss)
I1211 10:02:48.565093  6644 sgd_solver.cpp:105] Iteration 96000, lr = 0.001
I1211 10:02:54.907501  6644 solver.cpp:218] Iteration 96100 (15.768 iter/s, 6.34196s/100 iters), loss = 0.506349
I1211 10:02:54.907501  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:02:54.907501  6644 solver.cpp:237]     Train net output #1: loss = 0.506349 (* 1 = 0.506349 loss)
I1211 10:02:54.907501  6644 sgd_solver.cpp:105] Iteration 96100, lr = 0.001
I1211 10:03:01.246012  6644 solver.cpp:218] Iteration 96200 (15.7757 iter/s, 6.33885s/100 iters), loss = 0.397288
I1211 10:03:01.246012  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:03:01.246012  6644 solver.cpp:237]     Train net output #1: loss = 0.397288 (* 1 = 0.397288 loss)
I1211 10:03:01.246012  6644 sgd_solver.cpp:105] Iteration 96200, lr = 0.001
I1211 10:03:07.590525  6644 solver.cpp:218] Iteration 96300 (15.7629 iter/s, 6.34403s/100 iters), loss = 0.526796
I1211 10:03:07.590525  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:03:07.590525  6644 solver.cpp:237]     Train net output #1: loss = 0.526796 (* 1 = 0.526796 loss)
I1211 10:03:07.590525  6644 sgd_solver.cpp:105] Iteration 96300, lr = 0.001
I1211 10:03:13.938993  6644 solver.cpp:218] Iteration 96400 (15.753 iter/s, 6.348s/100 iters), loss = 0.476171
I1211 10:03:13.938993  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:03:13.938993  6644 solver.cpp:237]     Train net output #1: loss = 0.476171 (* 1 = 0.476171 loss)
I1211 10:03:13.938993  6644 sgd_solver.cpp:105] Iteration 96400, lr = 0.001
I1211 10:03:19.965418  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:03:20.214942  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.caffemodel
I1211 10:03:20.230446  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_96500.solverstate
I1211 10:03:20.235446  6644 solver.cpp:330] Iteration 96500, Testing net (#0)
I1211 10:03:20.235446  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:03:21.755566 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:03:21.815562  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 10:03:21.816062  6644 solver.cpp:397]     Test net output #1: loss = 1.18134 (* 1 = 1.18134 loss)
I1211 10:03:21.875563  6644 solver.cpp:218] Iteration 96500 (12.6007 iter/s, 7.93605s/100 iters), loss = 0.556003
I1211 10:03:21.875563  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:03:21.875563  6644 solver.cpp:237]     Train net output #1: loss = 0.556003 (* 1 = 0.556003 loss)
I1211 10:03:21.875563  6644 sgd_solver.cpp:105] Iteration 96500, lr = 0.001
I1211 10:03:28.216997  6644 solver.cpp:218] Iteration 96600 (15.7723 iter/s, 6.34025s/100 iters), loss = 0.456107
I1211 10:03:28.216997  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:03:28.216997  6644 solver.cpp:237]     Train net output #1: loss = 0.456107 (* 1 = 0.456107 loss)
I1211 10:03:28.216997  6644 sgd_solver.cpp:105] Iteration 96600, lr = 0.001
I1211 10:03:34.562444  6644 solver.cpp:218] Iteration 96700 (15.7588 iter/s, 6.34566s/100 iters), loss = 0.362061
I1211 10:03:34.562444  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:03:34.562444  6644 solver.cpp:237]     Train net output #1: loss = 0.362061 (* 1 = 0.362061 loss)
I1211 10:03:34.562444  6644 sgd_solver.cpp:105] Iteration 96700, lr = 0.001
I1211 10:03:40.902909  6644 solver.cpp:218] Iteration 96800 (15.7743 iter/s, 6.33942s/100 iters), loss = 0.585129
I1211 10:03:40.902909  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.81
I1211 10:03:40.902909  6644 solver.cpp:237]     Train net output #1: loss = 0.585129 (* 1 = 0.585129 loss)
I1211 10:03:40.902909  6644 sgd_solver.cpp:105] Iteration 96800, lr = 0.001
I1211 10:03:47.240381  6644 solver.cpp:218] Iteration 96900 (15.7781 iter/s, 6.33789s/100 iters), loss = 0.437907
I1211 10:03:47.240381  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:03:47.241380  6644 solver.cpp:237]     Train net output #1: loss = 0.437907 (* 1 = 0.437907 loss)
I1211 10:03:47.241380  6644 sgd_solver.cpp:105] Iteration 96900, lr = 0.001
I1211 10:03:53.269840  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:03:53.520864  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.caffemodel
I1211 10:03:53.535869  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97000.solverstate
I1211 10:03:53.540870  6644 solver.cpp:330] Iteration 97000, Testing net (#0)
I1211 10:03:53.540870  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:03:55.059988 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:03:55.119993  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 10:03:55.119993  6644 solver.cpp:397]     Test net output #1: loss = 1.17891 (* 1 = 1.17891 loss)
I1211 10:03:55.179996  6644 solver.cpp:218] Iteration 97000 (12.5963 iter/s, 7.93886s/100 iters), loss = 0.482123
I1211 10:03:55.179996  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:03:55.179996  6644 solver.cpp:237]     Train net output #1: loss = 0.482123 (* 1 = 0.482123 loss)
I1211 10:03:55.179996  6644 sgd_solver.cpp:105] Iteration 97000, lr = 0.001
I1211 10:04:01.532418  6644 solver.cpp:218] Iteration 97100 (15.7432 iter/s, 6.35195s/100 iters), loss = 0.442895
I1211 10:04:01.532418  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:04:01.532418  6644 solver.cpp:237]     Train net output #1: loss = 0.442895 (* 1 = 0.442895 loss)
I1211 10:04:01.532418  6644 sgd_solver.cpp:105] Iteration 97100, lr = 0.001
I1211 10:04:07.886925  6644 solver.cpp:218] Iteration 97200 (15.7395 iter/s, 6.35345s/100 iters), loss = 0.334572
I1211 10:04:07.886925  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:04:07.886925  6644 solver.cpp:237]     Train net output #1: loss = 0.334572 (* 1 = 0.334572 loss)
I1211 10:04:07.886925  6644 sgd_solver.cpp:105] Iteration 97200, lr = 0.001
I1211 10:04:14.237413  6644 solver.cpp:218] Iteration 97300 (15.7479 iter/s, 6.35003s/100 iters), loss = 0.327292
I1211 10:04:14.237413  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:04:14.237413  6644 solver.cpp:237]     Train net output #1: loss = 0.327292 (* 1 = 0.327292 loss)
I1211 10:04:14.237413  6644 sgd_solver.cpp:105] Iteration 97300, lr = 0.001
I1211 10:04:20.592828  6644 solver.cpp:218] Iteration 97400 (15.7341 iter/s, 6.35563s/100 iters), loss = 0.395793
I1211 10:04:20.592828  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:04:20.592828  6644 solver.cpp:237]     Train net output #1: loss = 0.395793 (* 1 = 0.395793 loss)
I1211 10:04:20.592828  6644 sgd_solver.cpp:105] Iteration 97400, lr = 0.001
I1211 10:04:26.629204  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:04:26.879222  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.caffemodel
I1211 10:04:26.894223  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_97500.solverstate
I1211 10:04:26.899224  6644 solver.cpp:330] Iteration 97500, Testing net (#0)
I1211 10:04:26.899224  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:04:28.418306 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:04:28.478307  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6816
I1211 10:04:28.478307  6644 solver.cpp:397]     Test net output #1: loss = 1.18094 (* 1 = 1.18094 loss)
I1211 10:04:28.539312  6644 solver.cpp:218] Iteration 97500 (12.5856 iter/s, 7.94557s/100 iters), loss = 0.498888
I1211 10:04:28.539312  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:04:28.539312  6644 solver.cpp:237]     Train net output #1: loss = 0.498888 (* 1 = 0.498888 loss)
I1211 10:04:28.539312  6644 sgd_solver.cpp:105] Iteration 97500, lr = 0.001
I1211 10:04:34.893142  6644 solver.cpp:218] Iteration 97600 (15.7387 iter/s, 6.35375s/100 iters), loss = 0.420745
I1211 10:04:34.893142  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:04:34.893142  6644 solver.cpp:237]     Train net output #1: loss = 0.420745 (* 1 = 0.420745 loss)
I1211 10:04:34.893142  6644 sgd_solver.cpp:105] Iteration 97600, lr = 0.001
I1211 10:04:41.246628  6644 solver.cpp:218] Iteration 97700 (15.7401 iter/s, 6.35319s/100 iters), loss = 0.388279
I1211 10:04:41.246628  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:04:41.246628  6644 solver.cpp:237]     Train net output #1: loss = 0.388279 (* 1 = 0.388279 loss)
I1211 10:04:41.246628  6644 sgd_solver.cpp:105] Iteration 97700, lr = 0.001
I1211 10:04:47.639024  6644 solver.cpp:218] Iteration 97800 (15.6459 iter/s, 6.39143s/100 iters), loss = 0.438991
I1211 10:04:47.639024  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:04:47.639024  6644 solver.cpp:237]     Train net output #1: loss = 0.438991 (* 1 = 0.438991 loss)
I1211 10:04:47.639024  6644 sgd_solver.cpp:105] Iteration 97800, lr = 0.001
I1211 10:04:53.995939  6644 solver.cpp:218] Iteration 97900 (15.7315 iter/s, 6.35668s/100 iters), loss = 0.416944
I1211 10:04:53.995939  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:04:53.996440  6644 solver.cpp:237]     Train net output #1: loss = 0.416944 (* 1 = 0.416944 loss)
I1211 10:04:53.996440  6644 sgd_solver.cpp:105] Iteration 97900, lr = 0.001
I1211 10:05:00.042964  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:05:00.292480  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.caffemodel
I1211 10:05:00.308980  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98000.solverstate
I1211 10:05:00.313980  6644 solver.cpp:330] Iteration 98000, Testing net (#0)
I1211 10:05:00.313980  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:05:01.835494 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:05:01.895994  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6809
I1211 10:05:01.895994  6644 solver.cpp:397]     Test net output #1: loss = 1.18794 (* 1 = 1.18794 loss)
I1211 10:05:01.956493  6644 solver.cpp:218] Iteration 98000 (12.5626 iter/s, 7.96012s/100 iters), loss = 0.472799
I1211 10:05:01.956995  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:05:01.956995  6644 solver.cpp:237]     Train net output #1: loss = 0.472799 (* 1 = 0.472799 loss)
I1211 10:05:01.956995  6644 sgd_solver.cpp:105] Iteration 98000, lr = 0.001
I1211 10:05:08.313367  6644 solver.cpp:218] Iteration 98100 (15.7324 iter/s, 6.35632s/100 iters), loss = 0.510087
I1211 10:05:08.313367  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:05:08.313367  6644 solver.cpp:237]     Train net output #1: loss = 0.510087 (* 1 = 0.510087 loss)
I1211 10:05:08.313367  6644 sgd_solver.cpp:105] Iteration 98100, lr = 0.001
I1211 10:05:14.667806  6644 solver.cpp:218] Iteration 98200 (15.7369 iter/s, 6.35447s/100 iters), loss = 0.329498
I1211 10:05:14.667806  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:05:14.667806  6644 solver.cpp:237]     Train net output #1: loss = 0.329498 (* 1 = 0.329498 loss)
I1211 10:05:14.667806  6644 sgd_solver.cpp:105] Iteration 98200, lr = 0.001
I1211 10:05:21.020756  6644 solver.cpp:218] Iteration 98300 (15.7436 iter/s, 6.35179s/100 iters), loss = 0.392177
I1211 10:05:21.020756  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:05:21.020756  6644 solver.cpp:237]     Train net output #1: loss = 0.392177 (* 1 = 0.392177 loss)
I1211 10:05:21.020756  6644 sgd_solver.cpp:105] Iteration 98300, lr = 0.001
I1211 10:05:27.364759  6644 solver.cpp:218] Iteration 98400 (15.762 iter/s, 6.34436s/100 iters), loss = 0.49884
I1211 10:05:27.364759  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.82
I1211 10:05:27.364759  6644 solver.cpp:237]     Train net output #1: loss = 0.49884 (* 1 = 0.49884 loss)
I1211 10:05:27.364759  6644 sgd_solver.cpp:105] Iteration 98400, lr = 0.001
I1211 10:05:33.406158  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:05:33.656173  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.caffemodel
I1211 10:05:33.671175  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_98500.solverstate
I1211 10:05:33.676175  6644 solver.cpp:330] Iteration 98500, Testing net (#0)
I1211 10:05:33.676175  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:05:35.194293 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:05:35.254297  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6805
I1211 10:05:35.254297  6644 solver.cpp:397]     Test net output #1: loss = 1.19023 (* 1 = 1.19023 loss)
I1211 10:05:35.315299  6644 solver.cpp:218] Iteration 98500 (12.5795 iter/s, 7.94947s/100 iters), loss = 0.461699
I1211 10:05:35.315299  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:05:35.315299  6644 solver.cpp:237]     Train net output #1: loss = 0.461699 (* 1 = 0.461699 loss)
I1211 10:05:35.315299  6644 sgd_solver.cpp:105] Iteration 98500, lr = 0.001
I1211 10:05:41.652740  6644 solver.cpp:218] Iteration 98600 (15.7791 iter/s, 6.33748s/100 iters), loss = 0.363999
I1211 10:05:41.652740  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:05:41.652740  6644 solver.cpp:237]     Train net output #1: loss = 0.363999 (* 1 = 0.363999 loss)
I1211 10:05:41.652740  6644 sgd_solver.cpp:105] Iteration 98600, lr = 0.001
I1211 10:05:47.992188  6644 solver.cpp:218] Iteration 98700 (15.7749 iter/s, 6.33918s/100 iters), loss = 0.41444
I1211 10:05:47.992188  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:05:47.992188  6644 solver.cpp:237]     Train net output #1: loss = 0.41444 (* 1 = 0.41444 loss)
I1211 10:05:47.992188  6644 sgd_solver.cpp:105] Iteration 98700, lr = 0.001
I1211 10:05:54.332656  6644 solver.cpp:218] Iteration 98800 (15.7742 iter/s, 6.33947s/100 iters), loss = 0.572265
I1211 10:05:54.332656  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.8
I1211 10:05:54.332656  6644 solver.cpp:237]     Train net output #1: loss = 0.572265 (* 1 = 0.572265 loss)
I1211 10:05:54.332656  6644 sgd_solver.cpp:105] Iteration 98800, lr = 0.001
I1211 10:06:00.677095  6644 solver.cpp:218] Iteration 98900 (15.7623 iter/s, 6.34425s/100 iters), loss = 0.366393
I1211 10:06:00.677095  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:06:00.677095  6644 solver.cpp:237]     Train net output #1: loss = 0.366393 (* 1 = 0.366393 loss)
I1211 10:06:00.677095  6644 sgd_solver.cpp:105] Iteration 98900, lr = 0.001
I1211 10:06:06.707916  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:06:06.956971  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.caffemodel
I1211 10:06:06.970973  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99000.solverstate
I1211 10:06:06.975975  6644 solver.cpp:330] Iteration 99000, Testing net (#0)
I1211 10:06:06.976971  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:06:08.494014 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:06:08.553977  6644 solver.cpp:397]     Test net output #0: accuracy = 0.679
I1211 10:06:08.554968  6644 solver.cpp:397]     Test net output #1: loss = 1.20054 (* 1 = 1.20054 loss)
I1211 10:06:08.614977  6644 solver.cpp:218] Iteration 99000 (12.5991 iter/s, 7.93706s/100 iters), loss = 0.433461
I1211 10:06:08.614977  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:06:08.614977  6644 solver.cpp:237]     Train net output #1: loss = 0.433461 (* 1 = 0.433461 loss)
I1211 10:06:08.614977  6644 sgd_solver.cpp:105] Iteration 99000, lr = 0.001
I1211 10:06:14.953392  6644 solver.cpp:218] Iteration 99100 (15.7777 iter/s, 6.33808s/100 iters), loss = 0.352523
I1211 10:06:14.953392  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:06:14.953392  6644 solver.cpp:237]     Train net output #1: loss = 0.352522 (* 1 = 0.352522 loss)
I1211 10:06:14.953392  6644 sgd_solver.cpp:105] Iteration 99100, lr = 0.001
I1211 10:06:21.293083  6644 solver.cpp:218] Iteration 99200 (15.7743 iter/s, 6.33942s/100 iters), loss = 0.294716
I1211 10:06:21.293083  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:06:21.293083  6644 solver.cpp:237]     Train net output #1: loss = 0.294716 (* 1 = 0.294716 loss)
I1211 10:06:21.293083  6644 sgd_solver.cpp:105] Iteration 99200, lr = 0.001
I1211 10:06:27.631233  6644 solver.cpp:218] Iteration 99300 (15.778 iter/s, 6.33795s/100 iters), loss = 0.362435
I1211 10:06:27.631233  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:06:27.631233  6644 solver.cpp:237]     Train net output #1: loss = 0.362435 (* 1 = 0.362435 loss)
I1211 10:06:27.631233  6644 sgd_solver.cpp:105] Iteration 99300, lr = 0.001
I1211 10:06:33.975594  6644 solver.cpp:218] Iteration 99400 (15.7632 iter/s, 6.34387s/100 iters), loss = 0.416684
I1211 10:06:33.975594  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:06:33.975594  6644 solver.cpp:237]     Train net output #1: loss = 0.416683 (* 1 = 0.416683 loss)
I1211 10:06:33.975594  6644 sgd_solver.cpp:105] Iteration 99400, lr = 0.001
I1211 10:06:40.006394  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:06:40.258438  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.caffemodel
I1211 10:06:40.274436  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_99500.solverstate
I1211 10:06:40.279441  6644 solver.cpp:330] Iteration 99500, Testing net (#0)
I1211 10:06:40.279441  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:06:41.798377 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:06:41.858412  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6813
I1211 10:06:41.858412  6644 solver.cpp:397]     Test net output #1: loss = 1.19051 (* 1 = 1.19051 loss)
I1211 10:06:41.920447  6644 solver.cpp:218] Iteration 99500 (12.5886 iter/s, 7.9437s/100 iters), loss = 0.463007
I1211 10:06:41.920447  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:06:41.920447  6644 solver.cpp:237]     Train net output #1: loss = 0.463007 (* 1 = 0.463007 loss)
I1211 10:06:41.920447  6644 sgd_solver.cpp:105] Iteration 99500, lr = 0.001
I1211 10:06:48.270373  6644 solver.cpp:218] Iteration 99600 (15.7472 iter/s, 6.35036s/100 iters), loss = 0.347712
I1211 10:06:48.270373  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:06:48.271374  6644 solver.cpp:237]     Train net output #1: loss = 0.347712 (* 1 = 0.347712 loss)
I1211 10:06:48.271374  6644 sgd_solver.cpp:105] Iteration 99600, lr = 0.001
I1211 10:06:54.610730  6644 solver.cpp:218] Iteration 99700 (15.7751 iter/s, 6.3391s/100 iters), loss = 0.381512
I1211 10:06:54.610730  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:06:54.610730  6644 solver.cpp:237]     Train net output #1: loss = 0.381512 (* 1 = 0.381512 loss)
I1211 10:06:54.610730  6644 sgd_solver.cpp:105] Iteration 99700, lr = 0.001
I1211 10:07:00.958290  6644 solver.cpp:218] Iteration 99800 (15.7545 iter/s, 6.34739s/100 iters), loss = 0.386672
I1211 10:07:00.958290  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:07:00.958791  6644 solver.cpp:237]     Train net output #1: loss = 0.386672 (* 1 = 0.386672 loss)
I1211 10:07:00.958791  6644 sgd_solver.cpp:105] Iteration 99800, lr = 0.001
I1211 10:07:07.307715  6644 solver.cpp:218] Iteration 99900 (15.75 iter/s, 6.34922s/100 iters), loss = 0.363467
I1211 10:07:07.307715  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:07:07.307715  6644 solver.cpp:237]     Train net output #1: loss = 0.363467 (* 1 = 0.363467 loss)
I1211 10:07:07.307715  6644 sgd_solver.cpp:105] Iteration 99900, lr = 0.001
I1211 10:07:13.350142  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:07:13.599164  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.caffemodel
I1211 10:07:13.614164  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100000.solverstate
I1211 10:07:13.619164  6644 solver.cpp:330] Iteration 100000, Testing net (#0)
I1211 10:07:13.619164  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:07:15.138262 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:07:15.199273  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 10:07:15.199273  6644 solver.cpp:397]     Test net output #1: loss = 1.19733 (* 1 = 1.19733 loss)
I1211 10:07:15.259774  6644 solver.cpp:218] Iteration 100000 (12.5765 iter/s, 7.95136s/100 iters), loss = 0.399208
I1211 10:07:15.259774  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:07:15.259774  6644 solver.cpp:237]     Train net output #1: loss = 0.399208 (* 1 = 0.399208 loss)
I1211 10:07:15.260275  6644 sgd_solver.cpp:105] Iteration 100000, lr = 0.001
I1211 10:07:21.607694  6644 solver.cpp:218] Iteration 100100 (15.7536 iter/s, 6.34777s/100 iters), loss = 0.317094
I1211 10:07:21.607694  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:07:21.607694  6644 solver.cpp:237]     Train net output #1: loss = 0.317094 (* 1 = 0.317094 loss)
I1211 10:07:21.607694  6644 sgd_solver.cpp:105] Iteration 100100, lr = 0.001
I1211 10:07:27.947135  6644 solver.cpp:218] Iteration 100200 (15.7748 iter/s, 6.33924s/100 iters), loss = 0.382903
I1211 10:07:27.947135  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:07:27.947135  6644 solver.cpp:237]     Train net output #1: loss = 0.382903 (* 1 = 0.382903 loss)
I1211 10:07:27.947135  6644 sgd_solver.cpp:105] Iteration 100200, lr = 0.001
I1211 10:07:34.282578  6644 solver.cpp:218] Iteration 100300 (15.7853 iter/s, 6.33502s/100 iters), loss = 0.400278
I1211 10:07:34.282578  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:07:34.282578  6644 solver.cpp:237]     Train net output #1: loss = 0.400278 (* 1 = 0.400278 loss)
I1211 10:07:34.282578  6644 sgd_solver.cpp:105] Iteration 100300, lr = 0.001
I1211 10:07:40.618994  6644 solver.cpp:218] Iteration 100400 (15.7829 iter/s, 6.33599s/100 iters), loss = 0.407343
I1211 10:07:40.618994  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:07:40.618994  6644 solver.cpp:237]     Train net output #1: loss = 0.407343 (* 1 = 0.407343 loss)
I1211 10:07:40.618994  6644 sgd_solver.cpp:105] Iteration 100400, lr = 0.001
I1211 10:07:46.649413  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:07:46.898442  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.caffemodel
I1211 10:07:46.913442  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_100500.solverstate
I1211 10:07:46.918442  6644 solver.cpp:330] Iteration 100500, Testing net (#0)
I1211 10:07:46.918442  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:07:48.435544 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:07:48.495554  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 10:07:48.495554  6644 solver.cpp:397]     Test net output #1: loss = 1.20387 (* 1 = 1.20387 loss)
I1211 10:07:48.556546  6644 solver.cpp:218] Iteration 100500 (12.6002 iter/s, 7.93639s/100 iters), loss = 0.466088
I1211 10:07:48.556546  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:07:48.556546  6644 solver.cpp:237]     Train net output #1: loss = 0.466088 (* 1 = 0.466088 loss)
I1211 10:07:48.556546  6644 sgd_solver.cpp:105] Iteration 100500, lr = 0.001
I1211 10:07:54.885985  6644 solver.cpp:218] Iteration 100600 (15.7992 iter/s, 6.32943s/100 iters), loss = 0.391142
I1211 10:07:54.885985  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:07:54.885985  6644 solver.cpp:237]     Train net output #1: loss = 0.391142 (* 1 = 0.391142 loss)
I1211 10:07:54.885985  6644 sgd_solver.cpp:105] Iteration 100600, lr = 0.001
I1211 10:08:01.218441  6644 solver.cpp:218] Iteration 100700 (15.7923 iter/s, 6.33218s/100 iters), loss = 0.345069
I1211 10:08:01.218441  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:08:01.218441  6644 solver.cpp:237]     Train net output #1: loss = 0.345069 (* 1 = 0.345069 loss)
I1211 10:08:01.218441  6644 sgd_solver.cpp:105] Iteration 100700, lr = 0.001
I1211 10:08:07.564398  6644 solver.cpp:218] Iteration 100800 (15.7603 iter/s, 6.34508s/100 iters), loss = 0.35903
I1211 10:08:07.564398  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:08:07.564398  6644 solver.cpp:237]     Train net output #1: loss = 0.35903 (* 1 = 0.35903 loss)
I1211 10:08:07.564898  6644 sgd_solver.cpp:105] Iteration 100800, lr = 0.001
I1211 10:08:13.908342  6644 solver.cpp:218] Iteration 100900 (15.7629 iter/s, 6.344s/100 iters), loss = 0.386465
I1211 10:08:13.908342  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:08:13.908342  6644 solver.cpp:237]     Train net output #1: loss = 0.386464 (* 1 = 0.386464 loss)
I1211 10:08:13.908342  6644 sgd_solver.cpp:105] Iteration 100900, lr = 0.001
I1211 10:08:19.938745  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:08:20.188760  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.caffemodel
I1211 10:08:20.203759  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101000.solverstate
I1211 10:08:20.208760  6644 solver.cpp:330] Iteration 101000, Testing net (#0)
I1211 10:08:20.208760  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:08:21.726856 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:08:21.786859  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6789
I1211 10:08:21.786859  6644 solver.cpp:397]     Test net output #1: loss = 1.21092 (* 1 = 1.21092 loss)
I1211 10:08:21.847859  6644 solver.cpp:218] Iteration 101000 (12.5964 iter/s, 7.93879s/100 iters), loss = 0.42248
I1211 10:08:21.847859  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:08:21.847859  6644 solver.cpp:237]     Train net output #1: loss = 0.42248 (* 1 = 0.42248 loss)
I1211 10:08:21.847859  6644 sgd_solver.cpp:105] Iteration 101000, lr = 0.001
I1211 10:08:28.185360  6644 solver.cpp:218] Iteration 101100 (15.7813 iter/s, 6.33662s/100 iters), loss = 0.373748
I1211 10:08:28.185360  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:08:28.185360  6644 solver.cpp:237]     Train net output #1: loss = 0.373748 (* 1 = 0.373748 loss)
I1211 10:08:28.185360  6644 sgd_solver.cpp:105] Iteration 101100, lr = 0.001
I1211 10:08:34.523784  6644 solver.cpp:218] Iteration 101200 (15.778 iter/s, 6.33794s/100 iters), loss = 0.341698
I1211 10:08:34.523784  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:08:34.523784  6644 solver.cpp:237]     Train net output #1: loss = 0.341698 (* 1 = 0.341698 loss)
I1211 10:08:34.523784  6644 sgd_solver.cpp:105] Iteration 101200, lr = 0.001
I1211 10:08:40.867262  6644 solver.cpp:218] Iteration 101300 (15.7648 iter/s, 6.34326s/100 iters), loss = 0.455973
I1211 10:08:40.867262  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:08:40.867262  6644 solver.cpp:237]     Train net output #1: loss = 0.455973 (* 1 = 0.455973 loss)
I1211 10:08:40.867262  6644 sgd_solver.cpp:105] Iteration 101300, lr = 0.001
I1211 10:08:47.203729  6644 solver.cpp:218] Iteration 101400 (15.7819 iter/s, 6.33638s/100 iters), loss = 0.352725
I1211 10:08:47.203729  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:08:47.203729  6644 solver.cpp:237]     Train net output #1: loss = 0.352725 (* 1 = 0.352725 loss)
I1211 10:08:47.203729  6644 sgd_solver.cpp:105] Iteration 101400, lr = 0.001
I1211 10:08:53.231197  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:08:53.480216  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.caffemodel
I1211 10:08:53.497216  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_101500.solverstate
I1211 10:08:53.502216  6644 solver.cpp:330] Iteration 101500, Testing net (#0)
I1211 10:08:53.502216  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:08:55.021877 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:08:55.082382  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6829
I1211 10:08:55.082382  6644 solver.cpp:397]     Test net output #1: loss = 1.20729 (* 1 = 1.20729 loss)
I1211 10:08:55.142387  6644 solver.cpp:218] Iteration 101500 (12.5969 iter/s, 7.93844s/100 iters), loss = 0.379569
I1211 10:08:55.143388  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:08:55.143388  6644 solver.cpp:237]     Train net output #1: loss = 0.379568 (* 1 = 0.379568 loss)
I1211 10:08:55.143388  6644 sgd_solver.cpp:105] Iteration 101500, lr = 0.001
I1211 10:09:01.496820  6644 solver.cpp:218] Iteration 101600 (15.7396 iter/s, 6.35342s/100 iters), loss = 0.435307
I1211 10:09:01.496820  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:09:01.496820  6644 solver.cpp:237]     Train net output #1: loss = 0.435307 (* 1 = 0.435307 loss)
I1211 10:09:01.496820  6644 sgd_solver.cpp:105] Iteration 101600, lr = 0.001
I1211 10:09:07.853350  6644 solver.cpp:218] Iteration 101700 (15.7331 iter/s, 6.35602s/100 iters), loss = 0.350481
I1211 10:09:07.853350  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:09:07.853350  6644 solver.cpp:237]     Train net output #1: loss = 0.350481 (* 1 = 0.350481 loss)
I1211 10:09:07.853350  6644 sgd_solver.cpp:105] Iteration 101700, lr = 0.001
I1211 10:09:14.211793  6644 solver.cpp:218] Iteration 101800 (15.7289 iter/s, 6.35771s/100 iters), loss = 0.345835
I1211 10:09:14.211793  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:09:14.211793  6644 solver.cpp:237]     Train net output #1: loss = 0.345834 (* 1 = 0.345834 loss)
I1211 10:09:14.211793  6644 sgd_solver.cpp:105] Iteration 101800, lr = 0.001
I1211 10:09:20.574264  6644 solver.cpp:218] Iteration 101900 (15.7161 iter/s, 6.3629s/100 iters), loss = 0.416724
I1211 10:09:20.574264  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:09:20.574264  6644 solver.cpp:237]     Train net output #1: loss = 0.416724 (* 1 = 0.416724 loss)
I1211 10:09:20.574264  6644 sgd_solver.cpp:105] Iteration 101900, lr = 0.001
I1211 10:09:26.618707  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:09:26.870731  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.caffemodel
I1211 10:09:26.886731  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102000.solverstate
I1211 10:09:26.891731  6644 solver.cpp:330] Iteration 102000, Testing net (#0)
I1211 10:09:26.891731  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:09:28.410856 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:09:28.470866  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6812
I1211 10:09:28.470866  6644 solver.cpp:397]     Test net output #1: loss = 1.2103 (* 1 = 1.2103 loss)
I1211 10:09:28.531868  6644 solver.cpp:218] Iteration 102000 (12.5683 iter/s, 7.95652s/100 iters), loss = 0.367522
I1211 10:09:28.531868  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:09:28.531868  6644 solver.cpp:237]     Train net output #1: loss = 0.367522 (* 1 = 0.367522 loss)
I1211 10:09:28.531868  6644 sgd_solver.cpp:105] Iteration 102000, lr = 0.001
I1211 10:09:34.889273  6644 solver.cpp:218] Iteration 102100 (15.7305 iter/s, 6.35708s/100 iters), loss = 0.401887
I1211 10:09:34.889273  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:09:34.889273  6644 solver.cpp:237]     Train net output #1: loss = 0.401887 (* 1 = 0.401887 loss)
I1211 10:09:34.889273  6644 sgd_solver.cpp:105] Iteration 102100, lr = 0.001
I1211 10:09:41.239223  6644 solver.cpp:218] Iteration 102200 (15.7492 iter/s, 6.34954s/100 iters), loss = 0.338857
I1211 10:09:41.239223  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:09:41.239223  6644 solver.cpp:237]     Train net output #1: loss = 0.338857 (* 1 = 0.338857 loss)
I1211 10:09:41.239223  6644 sgd_solver.cpp:105] Iteration 102200, lr = 0.001
I1211 10:09:47.588209  6644 solver.cpp:218] Iteration 102300 (15.7525 iter/s, 6.34819s/100 iters), loss = 0.437456
I1211 10:09:47.588209  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:09:47.588209  6644 solver.cpp:237]     Train net output #1: loss = 0.437455 (* 1 = 0.437455 loss)
I1211 10:09:47.588209  6644 sgd_solver.cpp:105] Iteration 102300, lr = 0.001
I1211 10:09:53.941666  6644 solver.cpp:218] Iteration 102400 (15.7395 iter/s, 6.35344s/100 iters), loss = 0.395498
I1211 10:09:53.941666  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:09:53.941666  6644 solver.cpp:237]     Train net output #1: loss = 0.395498 (* 1 = 0.395498 loss)
I1211 10:09:53.941666  6644 sgd_solver.cpp:105] Iteration 102400, lr = 0.001
I1211 10:09:59.979162  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:10:00.230690  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.caffemodel
I1211 10:10:00.248198  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_102500.solverstate
I1211 10:10:00.253196  6644 solver.cpp:330] Iteration 102500, Testing net (#0)
I1211 10:10:00.253196  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:10:01.799580 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:10:01.861089  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6822
I1211 10:10:01.861591  6644 solver.cpp:397]     Test net output #1: loss = 1.21368 (* 1 = 1.21368 loss)
I1211 10:10:01.922593  6644 solver.cpp:218] Iteration 102500 (12.5309 iter/s, 7.98025s/100 iters), loss = 0.375595
I1211 10:10:01.922593  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:10:01.922593  6644 solver.cpp:237]     Train net output #1: loss = 0.375595 (* 1 = 0.375595 loss)
I1211 10:10:01.922593  6644 sgd_solver.cpp:105] Iteration 102500, lr = 0.001
I1211 10:10:08.271342  6644 solver.cpp:218] Iteration 102600 (15.751 iter/s, 6.34879s/100 iters), loss = 0.442758
I1211 10:10:08.271342  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:10:08.271342  6644 solver.cpp:237]     Train net output #1: loss = 0.442758 (* 1 = 0.442758 loss)
I1211 10:10:08.271342  6644 sgd_solver.cpp:105] Iteration 102600, lr = 0.001
I1211 10:10:14.613786  6644 solver.cpp:218] Iteration 102700 (15.7694 iter/s, 6.34138s/100 iters), loss = 0.348805
I1211 10:10:14.613786  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:10:14.613786  6644 solver.cpp:237]     Train net output #1: loss = 0.348804 (* 1 = 0.348804 loss)
I1211 10:10:14.613786  6644 sgd_solver.cpp:105] Iteration 102700, lr = 0.001
I1211 10:10:20.960294  6644 solver.cpp:218] Iteration 102800 (15.7577 iter/s, 6.34612s/100 iters), loss = 0.373159
I1211 10:10:20.960294  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:10:20.960294  6644 solver.cpp:237]     Train net output #1: loss = 0.373159 (* 1 = 0.373159 loss)
I1211 10:10:20.960294  6644 sgd_solver.cpp:105] Iteration 102800, lr = 0.001
I1211 10:10:27.303753  6644 solver.cpp:218] Iteration 102900 (15.7646 iter/s, 6.34334s/100 iters), loss = 0.482402
I1211 10:10:27.303753  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:10:27.303753  6644 solver.cpp:237]     Train net output #1: loss = 0.482402 (* 1 = 0.482402 loss)
I1211 10:10:27.303753  6644 sgd_solver.cpp:105] Iteration 102900, lr = 0.001
I1211 10:10:33.336233  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:10:33.587249  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.caffemodel
I1211 10:10:33.602250  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103000.solverstate
I1211 10:10:33.607251  6644 solver.cpp:330] Iteration 103000, Testing net (#0)
I1211 10:10:33.607251  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:10:35.127353 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:10:35.188357  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6809
I1211 10:10:35.188357  6644 solver.cpp:397]     Test net output #1: loss = 1.21665 (* 1 = 1.21665 loss)
I1211 10:10:35.249357  6644 solver.cpp:218] Iteration 103000 (12.5869 iter/s, 7.94474s/100 iters), loss = 0.449664
I1211 10:10:35.249357  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:10:35.249357  6644 solver.cpp:237]     Train net output #1: loss = 0.449664 (* 1 = 0.449664 loss)
I1211 10:10:35.249357  6644 sgd_solver.cpp:105] Iteration 103000, lr = 0.001
I1211 10:10:41.592768  6644 solver.cpp:218] Iteration 103100 (15.7634 iter/s, 6.34381s/100 iters), loss = 0.393976
I1211 10:10:41.593768  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:10:41.593768  6644 solver.cpp:237]     Train net output #1: loss = 0.393976 (* 1 = 0.393976 loss)
I1211 10:10:41.593768  6644 sgd_solver.cpp:105] Iteration 103100, lr = 0.001
I1211 10:10:47.939380  6644 solver.cpp:218] Iteration 103200 (15.7579 iter/s, 6.34601s/100 iters), loss = 0.314971
I1211 10:10:47.939380  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:10:47.939380  6644 solver.cpp:237]     Train net output #1: loss = 0.314971 (* 1 = 0.314971 loss)
I1211 10:10:47.939380  6644 sgd_solver.cpp:105] Iteration 103200, lr = 0.001
I1211 10:10:54.286861  6644 solver.cpp:218] Iteration 103300 (15.757 iter/s, 6.3464s/100 iters), loss = 0.315263
I1211 10:10:54.286861  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:10:54.286861  6644 solver.cpp:237]     Train net output #1: loss = 0.315263 (* 1 = 0.315263 loss)
I1211 10:10:54.286861  6644 sgd_solver.cpp:105] Iteration 103300, lr = 0.001
I1211 10:11:00.633289  6644 solver.cpp:218] Iteration 103400 (15.7576 iter/s, 6.34613s/100 iters), loss = 0.398744
I1211 10:11:00.633289  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:11:00.633289  6644 solver.cpp:237]     Train net output #1: loss = 0.398744 (* 1 = 0.398744 loss)
I1211 10:11:00.633289  6644 sgd_solver.cpp:105] Iteration 103400, lr = 0.001
I1211 10:11:06.673707  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:11:06.925724  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.caffemodel
I1211 10:11:06.939723  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_103500.solverstate
I1211 10:11:06.944723  6644 solver.cpp:330] Iteration 103500, Testing net (#0)
I1211 10:11:06.944723  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:11:08.463847 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:11:08.523852  6644 solver.cpp:397]     Test net output #0: accuracy = 0.681
I1211 10:11:08.523852  6644 solver.cpp:397]     Test net output #1: loss = 1.227 (* 1 = 1.227 loss)
I1211 10:11:08.584851  6644 solver.cpp:218] Iteration 103500 (12.5768 iter/s, 7.95113s/100 iters), loss = 0.445415
I1211 10:11:08.584851  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:11:08.584851  6644 solver.cpp:237]     Train net output #1: loss = 0.445415 (* 1 = 0.445415 loss)
I1211 10:11:08.584851  6644 sgd_solver.cpp:105] Iteration 103500, lr = 0.001
I1211 10:11:14.935251  6644 solver.cpp:218] Iteration 103600 (15.7469 iter/s, 6.35047s/100 iters), loss = 0.370363
I1211 10:11:14.935251  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:11:14.935251  6644 solver.cpp:237]     Train net output #1: loss = 0.370363 (* 1 = 0.370363 loss)
I1211 10:11:14.936252  6644 sgd_solver.cpp:105] Iteration 103600, lr = 0.001
I1211 10:11:21.286308  6644 solver.cpp:218] Iteration 103700 (15.7482 iter/s, 6.34992s/100 iters), loss = 0.346656
I1211 10:11:21.286308  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:11:21.286308  6644 solver.cpp:237]     Train net output #1: loss = 0.346656 (* 1 = 0.346656 loss)
I1211 10:11:21.286308  6644 sgd_solver.cpp:105] Iteration 103700, lr = 0.001
I1211 10:11:27.637255  6644 solver.cpp:218] Iteration 103800 (15.746 iter/s, 6.35081s/100 iters), loss = 0.352654
I1211 10:11:27.637255  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:11:27.637255  6644 solver.cpp:237]     Train net output #1: loss = 0.352654 (* 1 = 0.352654 loss)
I1211 10:11:27.637255  6644 sgd_solver.cpp:105] Iteration 103800, lr = 0.001
I1211 10:11:33.988749  6644 solver.cpp:218] Iteration 103900 (15.7449 iter/s, 6.35128s/100 iters), loss = 0.451742
I1211 10:11:33.988749  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:11:33.988749  6644 solver.cpp:237]     Train net output #1: loss = 0.451741 (* 1 = 0.451741 loss)
I1211 10:11:33.988749  6644 sgd_solver.cpp:105] Iteration 103900, lr = 0.001
I1211 10:11:40.025159  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:11:40.276170  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.caffemodel
I1211 10:11:40.292170  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104000.solverstate
I1211 10:11:40.297173  6644 solver.cpp:330] Iteration 104000, Testing net (#0)
I1211 10:11:40.297173  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:11:41.814273 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:11:41.874275  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6807
I1211 10:11:41.874275  6644 solver.cpp:397]     Test net output #1: loss = 1.23553 (* 1 = 1.23553 loss)
I1211 10:11:41.935279  6644 solver.cpp:218] Iteration 104000 (12.5849 iter/s, 7.94602s/100 iters), loss = 0.456492
I1211 10:11:41.935279  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:11:41.935279  6644 solver.cpp:237]     Train net output #1: loss = 0.456492 (* 1 = 0.456492 loss)
I1211 10:11:41.935279  6644 sgd_solver.cpp:105] Iteration 104000, lr = 0.001
I1211 10:11:48.326797  6644 solver.cpp:218] Iteration 104100 (15.6478 iter/s, 6.39069s/100 iters), loss = 0.357192
I1211 10:11:48.326797  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.85
I1211 10:11:48.326797  6644 solver.cpp:237]     Train net output #1: loss = 0.357192 (* 1 = 0.357192 loss)
I1211 10:11:48.326797  6644 sgd_solver.cpp:105] Iteration 104100, lr = 0.001
I1211 10:11:54.725258  6644 solver.cpp:218] Iteration 104200 (15.6297 iter/s, 6.39809s/100 iters), loss = 0.270346
I1211 10:11:54.725258  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:11:54.725258  6644 solver.cpp:237]     Train net output #1: loss = 0.270346 (* 1 = 0.270346 loss)
I1211 10:11:54.725258  6644 sgd_solver.cpp:105] Iteration 104200, lr = 0.001
I1211 10:12:01.120784  6644 solver.cpp:218] Iteration 104300 (15.6368 iter/s, 6.39515s/100 iters), loss = 0.321573
I1211 10:12:01.120784  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:12:01.120784  6644 solver.cpp:237]     Train net output #1: loss = 0.321573 (* 1 = 0.321573 loss)
I1211 10:12:01.120784  6644 sgd_solver.cpp:105] Iteration 104300, lr = 0.001
I1211 10:12:07.481267  6644 solver.cpp:218] Iteration 104400 (15.7225 iter/s, 6.36033s/100 iters), loss = 0.399391
I1211 10:12:07.481267  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:12:07.481267  6644 solver.cpp:237]     Train net output #1: loss = 0.399391 (* 1 = 0.399391 loss)
I1211 10:12:07.481267  6644 sgd_solver.cpp:105] Iteration 104400, lr = 0.001
I1211 10:12:13.524210  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:12:13.773224  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.caffemodel
I1211 10:12:13.788224  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_104500.solverstate
I1211 10:12:13.793225  6644 solver.cpp:330] Iteration 104500, Testing net (#0)
I1211 10:12:13.793225  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:12:15.315328 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:12:15.376360  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 10:12:15.376360  6644 solver.cpp:397]     Test net output #1: loss = 1.2202 (* 1 = 1.2202 loss)
I1211 10:12:15.436847  6644 solver.cpp:218] Iteration 104500 (12.571 iter/s, 7.95479s/100 iters), loss = 0.390788
I1211 10:12:15.437350  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:12:15.437350  6644 solver.cpp:237]     Train net output #1: loss = 0.390788 (* 1 = 0.390788 loss)
I1211 10:12:15.437350  6644 sgd_solver.cpp:105] Iteration 104500, lr = 0.001
I1211 10:12:21.803872  6644 solver.cpp:218] Iteration 104600 (15.7063 iter/s, 6.36686s/100 iters), loss = 0.444159
I1211 10:12:21.803872  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:12:21.803872  6644 solver.cpp:237]     Train net output #1: loss = 0.444158 (* 1 = 0.444158 loss)
I1211 10:12:21.803872  6644 sgd_solver.cpp:105] Iteration 104600, lr = 0.001
I1211 10:12:28.172343  6644 solver.cpp:218] Iteration 104700 (15.7048 iter/s, 6.36748s/100 iters), loss = 0.290106
I1211 10:12:28.172343  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:12:28.172343  6644 solver.cpp:237]     Train net output #1: loss = 0.290106 (* 1 = 0.290106 loss)
I1211 10:12:28.172343  6644 sgd_solver.cpp:105] Iteration 104700, lr = 0.001
I1211 10:12:34.542831  6644 solver.cpp:218] Iteration 104800 (15.6969 iter/s, 6.37068s/100 iters), loss = 0.384004
I1211 10:12:34.542831  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:12:34.542831  6644 solver.cpp:237]     Train net output #1: loss = 0.384004 (* 1 = 0.384004 loss)
I1211 10:12:34.542831  6644 sgd_solver.cpp:105] Iteration 104800, lr = 0.001
I1211 10:12:40.909304  6644 solver.cpp:218] Iteration 104900 (15.7084 iter/s, 6.36602s/100 iters), loss = 0.292362
I1211 10:12:40.909304  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:12:40.909304  6644 solver.cpp:237]     Train net output #1: loss = 0.292362 (* 1 = 0.292362 loss)
I1211 10:12:40.909304  6644 sgd_solver.cpp:105] Iteration 104900, lr = 0.001
I1211 10:12:46.956696  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:12:47.206727  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.caffemodel
I1211 10:12:47.221726  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105000.solverstate
I1211 10:12:47.226727  6644 solver.cpp:330] Iteration 105000, Testing net (#0)
I1211 10:12:47.226727  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:12:48.749825 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:12:48.810834  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6801
I1211 10:12:48.810834  6644 solver.cpp:397]     Test net output #1: loss = 1.22617 (* 1 = 1.22617 loss)
I1211 10:12:48.872833  6644 solver.cpp:218] Iteration 105000 (12.5592 iter/s, 7.96227s/100 iters), loss = 0.335564
I1211 10:12:48.872833  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:12:48.872833  6644 solver.cpp:237]     Train net output #1: loss = 0.335564 (* 1 = 0.335564 loss)
I1211 10:12:48.872833  6644 sgd_solver.cpp:105] Iteration 105000, lr = 0.001
I1211 10:12:55.324487  6644 solver.cpp:218] Iteration 105100 (15.4999 iter/s, 6.45164s/100 iters), loss = 0.265782
I1211 10:12:55.324487  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:12:55.324487  6644 solver.cpp:237]     Train net output #1: loss = 0.265782 (* 1 = 0.265782 loss)
I1211 10:12:55.324487  6644 sgd_solver.cpp:105] Iteration 105100, lr = 0.001
I1211 10:13:01.816212  6644 solver.cpp:218] Iteration 105200 (15.406 iter/s, 6.49098s/100 iters), loss = 0.295148
I1211 10:13:01.816212  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:13:01.816212  6644 solver.cpp:237]     Train net output #1: loss = 0.295148 (* 1 = 0.295148 loss)
I1211 10:13:01.816212  6644 sgd_solver.cpp:105] Iteration 105200, lr = 0.001
I1211 10:13:08.239550  6644 solver.cpp:218] Iteration 105300 (15.5692 iter/s, 6.42295s/100 iters), loss = 0.34581
I1211 10:13:08.239550  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:13:08.239550  6644 solver.cpp:237]     Train net output #1: loss = 0.34581 (* 1 = 0.34581 loss)
I1211 10:13:08.239550  6644 sgd_solver.cpp:105] Iteration 105300, lr = 0.001
I1211 10:13:14.603502  6644 solver.cpp:218] Iteration 105400 (15.7146 iter/s, 6.36351s/100 iters), loss = 0.333907
I1211 10:13:14.603502  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:13:14.603502  6644 solver.cpp:237]     Train net output #1: loss = 0.333907 (* 1 = 0.333907 loss)
I1211 10:13:14.603502  6644 sgd_solver.cpp:105] Iteration 105400, lr = 0.001
I1211 10:13:20.683202  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:13:20.934232  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.caffemodel
I1211 10:13:20.950233  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_105500.solverstate
I1211 10:13:20.955235  6644 solver.cpp:330] Iteration 105500, Testing net (#0)
I1211 10:13:20.955235  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:13:22.496939 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:13:22.557443  6644 solver.cpp:397]     Test net output #0: accuracy = 0.68
I1211 10:13:22.557443  6644 solver.cpp:397]     Test net output #1: loss = 1.22469 (* 1 = 1.22469 loss)
I1211 10:13:22.620447  6644 solver.cpp:218] Iteration 105500 (12.4749 iter/s, 8.01613s/100 iters), loss = 0.429039
I1211 10:13:22.620447  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:13:22.620447  6644 solver.cpp:237]     Train net output #1: loss = 0.429039 (* 1 = 0.429039 loss)
I1211 10:13:22.620447  6644 sgd_solver.cpp:105] Iteration 105500, lr = 0.001
I1211 10:13:29.009975  6644 solver.cpp:218] Iteration 105600 (15.6514 iter/s, 6.38922s/100 iters), loss = 0.329911
I1211 10:13:29.009975  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:13:29.009975  6644 solver.cpp:237]     Train net output #1: loss = 0.329911 (* 1 = 0.329911 loss)
I1211 10:13:29.009975  6644 sgd_solver.cpp:105] Iteration 105600, lr = 0.001
I1211 10:13:35.374629  6644 solver.cpp:218] Iteration 105700 (15.711 iter/s, 6.36496s/100 iters), loss = 0.289482
I1211 10:13:35.375629  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:13:35.375629  6644 solver.cpp:237]     Train net output #1: loss = 0.289482 (* 1 = 0.289482 loss)
I1211 10:13:35.375629  6644 sgd_solver.cpp:105] Iteration 105700, lr = 0.001
I1211 10:13:41.791255  6644 solver.cpp:218] Iteration 105800 (15.5868 iter/s, 6.41567s/100 iters), loss = 0.338371
I1211 10:13:41.791255  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:13:41.791255  6644 solver.cpp:237]     Train net output #1: loss = 0.338371 (* 1 = 0.338371 loss)
I1211 10:13:41.791255  6644 sgd_solver.cpp:105] Iteration 105800, lr = 0.001
I1211 10:13:48.193889  6644 solver.cpp:218] Iteration 105900 (15.6209 iter/s, 6.40169s/100 iters), loss = 0.354705
I1211 10:13:48.193889  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:13:48.193889  6644 solver.cpp:237]     Train net output #1: loss = 0.354705 (* 1 = 0.354705 loss)
I1211 10:13:48.193889  6644 sgd_solver.cpp:105] Iteration 105900, lr = 0.001
I1211 10:13:54.320494  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:13:54.573541  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.caffemodel
I1211 10:13:54.588541  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106000.solverstate
I1211 10:13:54.593542  6644 solver.cpp:330] Iteration 106000, Testing net (#0)
I1211 10:13:54.593542  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:13:56.117877 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:13:56.177883  6644 solver.cpp:397]     Test net output #0: accuracy = 0.679
I1211 10:13:56.177883  6644 solver.cpp:397]     Test net output #1: loss = 1.2367 (* 1 = 1.2367 loss)
I1211 10:13:56.238885  6644 solver.cpp:218] Iteration 106000 (12.4305 iter/s, 8.04474s/100 iters), loss = 0.424525
I1211 10:13:56.238885  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:13:56.238885  6644 solver.cpp:237]     Train net output #1: loss = 0.424525 (* 1 = 0.424525 loss)
I1211 10:13:56.238885  6644 sgd_solver.cpp:105] Iteration 106000, lr = 0.001
I1211 10:14:02.628484  6644 solver.cpp:218] Iteration 106100 (15.6506 iter/s, 6.38953s/100 iters), loss = 0.350795
I1211 10:14:02.628484  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:14:02.628484  6644 solver.cpp:237]     Train net output #1: loss = 0.350795 (* 1 = 0.350795 loss)
I1211 10:14:02.628484  6644 sgd_solver.cpp:105] Iteration 106100, lr = 0.001
I1211 10:14:09.104949  6644 solver.cpp:218] Iteration 106200 (15.4434 iter/s, 6.47527s/100 iters), loss = 0.255818
I1211 10:14:09.104949  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:14:09.104949  6644 solver.cpp:237]     Train net output #1: loss = 0.255818 (* 1 = 0.255818 loss)
I1211 10:14:09.104949  6644 sgd_solver.cpp:105] Iteration 106200, lr = 0.001
I1211 10:14:15.488831  6644 solver.cpp:218] Iteration 106300 (15.6652 iter/s, 6.38357s/100 iters), loss = 0.376911
I1211 10:14:15.488831  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:14:15.488831  6644 solver.cpp:237]     Train net output #1: loss = 0.376911 (* 1 = 0.376911 loss)
I1211 10:14:15.488831  6644 sgd_solver.cpp:105] Iteration 106300, lr = 0.001
I1211 10:14:21.892943  6644 solver.cpp:218] Iteration 106400 (15.6165 iter/s, 6.40346s/100 iters), loss = 0.348225
I1211 10:14:21.892943  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.88
I1211 10:14:21.892943  6644 solver.cpp:237]     Train net output #1: loss = 0.348225 (* 1 = 0.348225 loss)
I1211 10:14:21.892943  6644 sgd_solver.cpp:105] Iteration 106400, lr = 0.001
I1211 10:14:27.974454  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:14:28.227015  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.caffemodel
I1211 10:14:28.243513  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_106500.solverstate
I1211 10:14:28.248514  6644 solver.cpp:330] Iteration 106500, Testing net (#0)
I1211 10:14:28.248514  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:14:29.774513 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:14:29.835016  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6834
I1211 10:14:29.835016  6644 solver.cpp:397]     Test net output #1: loss = 1.22711 (* 1 = 1.22711 loss)
I1211 10:14:29.896011  6644 solver.cpp:218] Iteration 106500 (12.4961 iter/s, 8.00249s/100 iters), loss = 0.333319
I1211 10:14:29.896011  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:14:29.896011  6644 solver.cpp:237]     Train net output #1: loss = 0.333319 (* 1 = 0.333319 loss)
I1211 10:14:29.896011  6644 sgd_solver.cpp:105] Iteration 106500, lr = 0.001
I1211 10:14:36.305482  6644 solver.cpp:218] Iteration 106600 (15.6025 iter/s, 6.40924s/100 iters), loss = 0.424218
I1211 10:14:36.305482  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:14:36.305999  6644 solver.cpp:237]     Train net output #1: loss = 0.424218 (* 1 = 0.424218 loss)
I1211 10:14:36.305999  6644 sgd_solver.cpp:105] Iteration 106600, lr = 0.001
I1211 10:14:42.699357  6644 solver.cpp:218] Iteration 106700 (15.6416 iter/s, 6.39321s/100 iters), loss = 0.250124
I1211 10:14:42.699357  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 10:14:42.699357  6644 solver.cpp:237]     Train net output #1: loss = 0.250124 (* 1 = 0.250124 loss)
I1211 10:14:42.699357  6644 sgd_solver.cpp:105] Iteration 106700, lr = 0.001
I1211 10:14:49.106000  6644 solver.cpp:218] Iteration 106800 (15.6097 iter/s, 6.40629s/100 iters), loss = 0.375284
I1211 10:14:49.106000  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:14:49.106000  6644 solver.cpp:237]     Train net output #1: loss = 0.375284 (* 1 = 0.375284 loss)
I1211 10:14:49.106000  6644 sgd_solver.cpp:105] Iteration 106800, lr = 0.001
I1211 10:14:55.555274  6644 solver.cpp:218] Iteration 106900 (15.5069 iter/s, 6.44876s/100 iters), loss = 0.37835
I1211 10:14:55.555274  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:14:55.555274  6644 solver.cpp:237]     Train net output #1: loss = 0.37835 (* 1 = 0.37835 loss)
I1211 10:14:55.555274  6644 sgd_solver.cpp:105] Iteration 106900, lr = 0.001
I1211 10:15:01.679770  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:15:01.928771  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.caffemodel
I1211 10:15:01.944269  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107000.solverstate
I1211 10:15:01.949270  6644 solver.cpp:330] Iteration 107000, Testing net (#0)
I1211 10:15:01.949769  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:15:03.485770 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:15:03.546772  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6792
I1211 10:15:03.546772  6644 solver.cpp:397]     Test net output #1: loss = 1.23172 (* 1 = 1.23172 loss)
I1211 10:15:03.607269  6644 solver.cpp:218] Iteration 107000 (12.4204 iter/s, 8.05126s/100 iters), loss = 0.47288
I1211 10:15:03.607269  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:15:03.607269  6644 solver.cpp:237]     Train net output #1: loss = 0.47288 (* 1 = 0.47288 loss)
I1211 10:15:03.607269  6644 sgd_solver.cpp:105] Iteration 107000, lr = 0.001
I1211 10:15:10.021522  6644 solver.cpp:218] Iteration 107100 (15.5911 iter/s, 6.41393s/100 iters), loss = 0.385286
I1211 10:15:10.021522  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:15:10.021522  6644 solver.cpp:237]     Train net output #1: loss = 0.385286 (* 1 = 0.385286 loss)
I1211 10:15:10.021522  6644 sgd_solver.cpp:105] Iteration 107100, lr = 0.001
I1211 10:15:16.534977  6644 solver.cpp:218] Iteration 107200 (15.3548 iter/s, 6.51262s/100 iters), loss = 0.31093
I1211 10:15:16.534977  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:15:16.534977  6644 solver.cpp:237]     Train net output #1: loss = 0.31093 (* 1 = 0.31093 loss)
I1211 10:15:16.534977  6644 sgd_solver.cpp:105] Iteration 107200, lr = 0.001
I1211 10:15:23.097579  6644 solver.cpp:218] Iteration 107300 (15.2387 iter/s, 6.56224s/100 iters), loss = 0.337638
I1211 10:15:23.097579  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:15:23.097579  6644 solver.cpp:237]     Train net output #1: loss = 0.337638 (* 1 = 0.337638 loss)
I1211 10:15:23.097579  6644 sgd_solver.cpp:105] Iteration 107300, lr = 0.001
I1211 10:15:29.629936  6644 solver.cpp:218] Iteration 107400 (15.3093 iter/s, 6.53196s/100 iters), loss = 0.37414
I1211 10:15:29.629936  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:15:29.629936  6644 solver.cpp:237]     Train net output #1: loss = 0.37414 (* 1 = 0.37414 loss)
I1211 10:15:29.629936  6644 sgd_solver.cpp:105] Iteration 107400, lr = 0.001
I1211 10:15:35.844100  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:15:36.100123  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.caffemodel
I1211 10:15:36.115123  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_107500.solverstate
I1211 10:15:36.120122  6644 solver.cpp:330] Iteration 107500, Testing net (#0)
I1211 10:15:36.120122  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:15:37.664768 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:15:37.726271  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6791
I1211 10:15:37.726271  6644 solver.cpp:397]     Test net output #1: loss = 1.24464 (* 1 = 1.24464 loss)
I1211 10:15:37.789283  6644 solver.cpp:218] Iteration 107500 (12.2566 iter/s, 8.15887s/100 iters), loss = 0.308502
I1211 10:15:37.789283  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:15:37.789283  6644 solver.cpp:237]     Train net output #1: loss = 0.308502 (* 1 = 0.308502 loss)
I1211 10:15:37.789283  6644 sgd_solver.cpp:105] Iteration 107500, lr = 0.001
I1211 10:15:44.212687  6644 solver.cpp:218] Iteration 107600 (15.57 iter/s, 6.4226s/100 iters), loss = 0.321895
I1211 10:15:44.212687  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:15:44.212687  6644 solver.cpp:237]     Train net output #1: loss = 0.321895 (* 1 = 0.321895 loss)
I1211 10:15:44.212687  6644 sgd_solver.cpp:105] Iteration 107600, lr = 0.001
I1211 10:15:50.659642  6644 solver.cpp:218] Iteration 107700 (15.5131 iter/s, 6.44618s/100 iters), loss = 0.233464
I1211 10:15:50.659642  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 10:15:50.659642  6644 solver.cpp:237]     Train net output #1: loss = 0.233464 (* 1 = 0.233464 loss)
I1211 10:15:50.659642  6644 sgd_solver.cpp:105] Iteration 107700, lr = 0.001
I1211 10:15:57.008127  6644 solver.cpp:218] Iteration 107800 (15.7523 iter/s, 6.34826s/100 iters), loss = 0.321577
I1211 10:15:57.008127  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:15:57.008127  6644 solver.cpp:237]     Train net output #1: loss = 0.321577 (* 1 = 0.321577 loss)
I1211 10:15:57.008127  6644 sgd_solver.cpp:105] Iteration 107800, lr = 0.001
I1211 10:16:03.365599  6644 solver.cpp:218] Iteration 107900 (15.7304 iter/s, 6.35712s/100 iters), loss = 0.363638
I1211 10:16:03.365599  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:16:03.365599  6644 solver.cpp:237]     Train net output #1: loss = 0.363638 (* 1 = 0.363638 loss)
I1211 10:16:03.365599  6644 sgd_solver.cpp:105] Iteration 107900, lr = 0.001
I1211 10:16:09.410123  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:16:09.661635  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.caffemodel
I1211 10:16:09.677135  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108000.solverstate
I1211 10:16:09.682139  6644 solver.cpp:330] Iteration 108000, Testing net (#0)
I1211 10:16:09.682139  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:16:11.203256 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:16:11.263257  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6814
I1211 10:16:11.263257  6644 solver.cpp:397]     Test net output #1: loss = 1.2371 (* 1 = 1.2371 loss)
I1211 10:16:11.324261  6644 solver.cpp:218] Iteration 108000 (12.5663 iter/s, 7.95781s/100 iters), loss = 0.337789
I1211 10:16:11.324261  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:16:11.324261  6644 solver.cpp:237]     Train net output #1: loss = 0.337789 (* 1 = 0.337789 loss)
I1211 10:16:11.324261  6644 sgd_solver.cpp:105] Iteration 108000, lr = 0.001
I1211 10:16:17.672255  6644 solver.cpp:218] Iteration 108100 (15.7534 iter/s, 6.34782s/100 iters), loss = 0.310327
I1211 10:16:17.672255  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:16:17.672255  6644 solver.cpp:237]     Train net output #1: loss = 0.310327 (* 1 = 0.310327 loss)
I1211 10:16:17.672255  6644 sgd_solver.cpp:105] Iteration 108100, lr = 0.001
I1211 10:16:24.017228  6644 solver.cpp:218] Iteration 108200 (15.7615 iter/s, 6.34458s/100 iters), loss = 0.292408
I1211 10:16:24.017228  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:16:24.017228  6644 solver.cpp:237]     Train net output #1: loss = 0.292408 (* 1 = 0.292408 loss)
I1211 10:16:24.017228  6644 sgd_solver.cpp:105] Iteration 108200, lr = 0.001
I1211 10:16:30.368244  6644 solver.cpp:218] Iteration 108300 (15.7471 iter/s, 6.35038s/100 iters), loss = 0.35242
I1211 10:16:30.368244  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:16:30.368244  6644 solver.cpp:237]     Train net output #1: loss = 0.35242 (* 1 = 0.35242 loss)
I1211 10:16:30.368244  6644 sgd_solver.cpp:105] Iteration 108300, lr = 0.001
I1211 10:16:36.711603  6644 solver.cpp:218] Iteration 108400 (15.7647 iter/s, 6.34329s/100 iters), loss = 0.345687
I1211 10:16:36.711603  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:16:36.711603  6644 solver.cpp:237]     Train net output #1: loss = 0.345687 (* 1 = 0.345687 loss)
I1211 10:16:36.711603  6644 sgd_solver.cpp:105] Iteration 108400, lr = 0.001
I1211 10:16:42.739020  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:16:42.989051  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.caffemodel
I1211 10:16:43.004051  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_108500.solverstate
I1211 10:16:43.009052  6644 solver.cpp:330] Iteration 108500, Testing net (#0)
I1211 10:16:43.009052  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:16:44.530158 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:16:44.590164  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6745
I1211 10:16:44.590164  6644 solver.cpp:397]     Test net output #1: loss = 1.25345 (* 1 = 1.25345 loss)
I1211 10:16:44.650162  6644 solver.cpp:218] Iteration 108500 (12.5966 iter/s, 7.93864s/100 iters), loss = 0.305608
I1211 10:16:44.651162  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:16:44.651162  6644 solver.cpp:237]     Train net output #1: loss = 0.305608 (* 1 = 0.305608 loss)
I1211 10:16:44.651162  6644 sgd_solver.cpp:105] Iteration 108500, lr = 0.001
I1211 10:16:51.000669  6644 solver.cpp:218] Iteration 108600 (15.7492 iter/s, 6.34951s/100 iters), loss = 0.296416
I1211 10:16:51.000669  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:16:51.000669  6644 solver.cpp:237]     Train net output #1: loss = 0.296416 (* 1 = 0.296416 loss)
I1211 10:16:51.000669  6644 sgd_solver.cpp:105] Iteration 108600, lr = 0.001
I1211 10:16:57.343988  6644 solver.cpp:218] Iteration 108700 (15.7666 iter/s, 6.34253s/100 iters), loss = 0.27391
I1211 10:16:57.343988  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:16:57.343988  6644 solver.cpp:237]     Train net output #1: loss = 0.27391 (* 1 = 0.27391 loss)
I1211 10:16:57.343988  6644 sgd_solver.cpp:105] Iteration 108700, lr = 0.001
I1211 10:17:03.684491  6644 solver.cpp:218] Iteration 108800 (15.7721 iter/s, 6.34031s/100 iters), loss = 0.327021
I1211 10:17:03.684491  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:17:03.684491  6644 solver.cpp:237]     Train net output #1: loss = 0.327021 (* 1 = 0.327021 loss)
I1211 10:17:03.684491  6644 sgd_solver.cpp:105] Iteration 108800, lr = 0.001
I1211 10:17:10.022948  6644 solver.cpp:218] Iteration 108900 (15.7761 iter/s, 6.3387s/100 iters), loss = 0.323532
I1211 10:17:10.023947  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:17:10.023947  6644 solver.cpp:237]     Train net output #1: loss = 0.323532 (* 1 = 0.323532 loss)
I1211 10:17:10.023947  6644 sgd_solver.cpp:105] Iteration 108900, lr = 0.001
I1211 10:17:16.051390  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:17:16.302404  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.caffemodel
I1211 10:17:16.317409  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109000.solverstate
I1211 10:17:16.321409  6644 solver.cpp:330] Iteration 109000, Testing net (#0)
I1211 10:17:16.321409  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:17:17.841531 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:17:17.902532  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6797
I1211 10:17:17.902532  6644 solver.cpp:397]     Test net output #1: loss = 1.24738 (* 1 = 1.24738 loss)
I1211 10:17:17.963534  6644 solver.cpp:218] Iteration 109000 (12.5951 iter/s, 7.93958s/100 iters), loss = 0.306373
I1211 10:17:17.963534  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.95
I1211 10:17:17.963534  6644 solver.cpp:237]     Train net output #1: loss = 0.306373 (* 1 = 0.306373 loss)
I1211 10:17:17.963534  6644 sgd_solver.cpp:105] Iteration 109000, lr = 0.001
I1211 10:17:24.314981  6644 solver.cpp:218] Iteration 109100 (15.7444 iter/s, 6.35147s/100 iters), loss = 0.319676
I1211 10:17:24.314981  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:17:24.314981  6644 solver.cpp:237]     Train net output #1: loss = 0.319676 (* 1 = 0.319676 loss)
I1211 10:17:24.314981  6644 sgd_solver.cpp:105] Iteration 109100, lr = 0.001
I1211 10:17:30.667521  6644 solver.cpp:218] Iteration 109200 (15.7435 iter/s, 6.35183s/100 iters), loss = 0.35311
I1211 10:17:30.667521  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:17:30.667521  6644 solver.cpp:237]     Train net output #1: loss = 0.35311 (* 1 = 0.35311 loss)
I1211 10:17:30.667521  6644 sgd_solver.cpp:105] Iteration 109200, lr = 0.001
I1211 10:17:37.024026  6644 solver.cpp:218] Iteration 109300 (15.7343 iter/s, 6.35552s/100 iters), loss = 0.385982
I1211 10:17:37.024026  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:17:37.024026  6644 solver.cpp:237]     Train net output #1: loss = 0.385982 (* 1 = 0.385982 loss)
I1211 10:17:37.024026  6644 sgd_solver.cpp:105] Iteration 109300, lr = 0.001
I1211 10:17:43.380578  6644 solver.cpp:218] Iteration 109400 (15.7329 iter/s, 6.35612s/100 iters), loss = 0.297299
I1211 10:17:43.380578  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:17:43.380578  6644 solver.cpp:237]     Train net output #1: loss = 0.297299 (* 1 = 0.297299 loss)
I1211 10:17:43.380578  6644 sgd_solver.cpp:105] Iteration 109400, lr = 0.001
I1211 10:17:49.419998  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:17:49.669000  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.caffemodel
I1211 10:17:49.685000  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_109500.solverstate
I1211 10:17:49.689000  6644 solver.cpp:330] Iteration 109500, Testing net (#0)
I1211 10:17:49.689000  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:17:51.210101 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:17:51.269101  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6795
I1211 10:17:51.269101  6644 solver.cpp:397]     Test net output #1: loss = 1.24359 (* 1 = 1.24359 loss)
I1211 10:17:51.330106  6644 solver.cpp:218] Iteration 109500 (12.5798 iter/s, 7.94926s/100 iters), loss = 0.354256
I1211 10:17:51.330106  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:17:51.330106  6644 solver.cpp:237]     Train net output #1: loss = 0.354256 (* 1 = 0.354256 loss)
I1211 10:17:51.330106  6644 sgd_solver.cpp:105] Iteration 109500, lr = 0.001
I1211 10:17:57.681593  6644 solver.cpp:218] Iteration 109600 (15.7462 iter/s, 6.35072s/100 iters), loss = 0.371697
I1211 10:17:57.681593  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:17:57.681593  6644 solver.cpp:237]     Train net output #1: loss = 0.371697 (* 1 = 0.371697 loss)
I1211 10:17:57.681593  6644 sgd_solver.cpp:105] Iteration 109600, lr = 0.001
I1211 10:18:04.033064  6644 solver.cpp:218] Iteration 109700 (15.7447 iter/s, 6.35136s/100 iters), loss = 0.260316
I1211 10:18:04.033064  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:18:04.033064  6644 solver.cpp:237]     Train net output #1: loss = 0.260316 (* 1 = 0.260316 loss)
I1211 10:18:04.033064  6644 sgd_solver.cpp:105] Iteration 109700, lr = 0.001
I1211 10:18:10.376523  6644 solver.cpp:218] Iteration 109800 (15.7658 iter/s, 6.34283s/100 iters), loss = 0.352681
I1211 10:18:10.376523  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:18:10.376523  6644 solver.cpp:237]     Train net output #1: loss = 0.352681 (* 1 = 0.352681 loss)
I1211 10:18:10.376523  6644 sgd_solver.cpp:105] Iteration 109800, lr = 0.001
I1211 10:18:16.722010  6644 solver.cpp:218] Iteration 109900 (15.7582 iter/s, 6.34591s/100 iters), loss = 0.420943
I1211 10:18:16.722010  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.84
I1211 10:18:16.723011  6644 solver.cpp:237]     Train net output #1: loss = 0.420943 (* 1 = 0.420943 loss)
I1211 10:18:16.723011  6644 sgd_solver.cpp:105] Iteration 109900, lr = 0.001
I1211 10:18:22.755468  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:18:23.005983  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.caffemodel
I1211 10:18:23.021486  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110000.solverstate
I1211 10:18:23.026487  6644 solver.cpp:330] Iteration 110000, Testing net (#0)
I1211 10:18:23.026487  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:18:24.546591 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:18:24.607095  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6772
I1211 10:18:24.607095  6644 solver.cpp:397]     Test net output #1: loss = 1.25607 (* 1 = 1.25607 loss)
I1211 10:18:24.667595  6644 solver.cpp:218] Iteration 110000 (12.5876 iter/s, 7.94436s/100 iters), loss = 0.386858
I1211 10:18:24.667595  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:18:24.667595  6644 solver.cpp:237]     Train net output #1: loss = 0.386858 (* 1 = 0.386858 loss)
I1211 10:18:24.667595  6644 sgd_solver.cpp:105] Iteration 110000, lr = 0.001
I1211 10:18:31.018019  6644 solver.cpp:218] Iteration 110100 (15.7463 iter/s, 6.35068s/100 iters), loss = 0.34275
I1211 10:18:31.018019  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:18:31.018019  6644 solver.cpp:237]     Train net output #1: loss = 0.34275 (* 1 = 0.34275 loss)
I1211 10:18:31.018019  6644 sgd_solver.cpp:105] Iteration 110100, lr = 0.001
I1211 10:18:37.366499  6644 solver.cpp:218] Iteration 110200 (15.7542 iter/s, 6.34751s/100 iters), loss = 0.230133
I1211 10:18:37.366499  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.97
I1211 10:18:37.366499  6644 solver.cpp:237]     Train net output #1: loss = 0.230133 (* 1 = 0.230133 loss)
I1211 10:18:37.366499  6644 sgd_solver.cpp:105] Iteration 110200, lr = 0.001
I1211 10:18:43.712051  6644 solver.cpp:218] Iteration 110300 (15.7607 iter/s, 6.3449s/100 iters), loss = 0.298169
I1211 10:18:43.712051  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:18:43.712051  6644 solver.cpp:237]     Train net output #1: loss = 0.298169 (* 1 = 0.298169 loss)
I1211 10:18:43.712051  6644 sgd_solver.cpp:105] Iteration 110300, lr = 0.001
I1211 10:18:50.047511  6644 solver.cpp:218] Iteration 110400 (15.7853 iter/s, 6.33502s/100 iters), loss = 0.356459
I1211 10:18:50.047511  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:18:50.047511  6644 solver.cpp:237]     Train net output #1: loss = 0.356459 (* 1 = 0.356459 loss)
I1211 10:18:50.047511  6644 sgd_solver.cpp:105] Iteration 110400, lr = 0.001
I1211 10:18:56.074982  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:18:56.324002  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.caffemodel
I1211 10:18:56.340003  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_110500.solverstate
I1211 10:18:56.344002  6644 solver.cpp:330] Iteration 110500, Testing net (#0)
I1211 10:18:56.344002  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:18:57.866132 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:18:57.927134  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6797
I1211 10:18:57.927134  6644 solver.cpp:397]     Test net output #1: loss = 1.24409 (* 1 = 1.24409 loss)
I1211 10:18:57.988133  6644 solver.cpp:218] Iteration 110500 (12.5938 iter/s, 7.94042s/100 iters), loss = 0.238602
I1211 10:18:57.988133  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.96
I1211 10:18:57.988133  6644 solver.cpp:237]     Train net output #1: loss = 0.238602 (* 1 = 0.238602 loss)
I1211 10:18:57.988133  6644 sgd_solver.cpp:105] Iteration 110500, lr = 0.001
I1211 10:19:04.334609  6644 solver.cpp:218] Iteration 110600 (15.7587 iter/s, 6.34571s/100 iters), loss = 0.337314
I1211 10:19:04.334609  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:19:04.334609  6644 solver.cpp:237]     Train net output #1: loss = 0.337313 (* 1 = 0.337313 loss)
I1211 10:19:04.334609  6644 sgd_solver.cpp:105] Iteration 110600, lr = 0.001
I1211 10:19:10.674088  6644 solver.cpp:218] Iteration 110700 (15.7759 iter/s, 6.33877s/100 iters), loss = 0.240404
I1211 10:19:10.674088  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:19:10.674088  6644 solver.cpp:237]     Train net output #1: loss = 0.240404 (* 1 = 0.240404 loss)
I1211 10:19:10.674088  6644 sgd_solver.cpp:105] Iteration 110700, lr = 0.001
I1211 10:19:17.009574  6644 solver.cpp:218] Iteration 110800 (15.7844 iter/s, 6.33538s/100 iters), loss = 0.296777
I1211 10:19:17.009574  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:19:17.009574  6644 solver.cpp:237]     Train net output #1: loss = 0.296777 (* 1 = 0.296777 loss)
I1211 10:19:17.009574  6644 sgd_solver.cpp:105] Iteration 110800, lr = 0.001
I1211 10:19:23.348031  6644 solver.cpp:218] Iteration 110900 (15.778 iter/s, 6.33793s/100 iters), loss = 0.429437
I1211 10:19:23.348031  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.87
I1211 10:19:23.348031  6644 solver.cpp:237]     Train net output #1: loss = 0.429437 (* 1 = 0.429437 loss)
I1211 10:19:23.348031  6644 sgd_solver.cpp:105] Iteration 110900, lr = 0.001
I1211 10:19:29.373440  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:19:29.623473  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.caffemodel
I1211 10:19:29.641475  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111000.solverstate
I1211 10:19:29.645474  6644 solver.cpp:330] Iteration 111000, Testing net (#0)
I1211 10:19:29.645474  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:19:31.167585 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:19:31.226588  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6773
I1211 10:19:31.226588  6644 solver.cpp:397]     Test net output #1: loss = 1.26712 (* 1 = 1.26712 loss)
I1211 10:19:31.287591  6644 solver.cpp:218] Iteration 111000 (12.5954 iter/s, 7.93941s/100 iters), loss = 0.293995
I1211 10:19:31.287591  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:19:31.287591  6644 solver.cpp:237]     Train net output #1: loss = 0.293995 (* 1 = 0.293995 loss)
I1211 10:19:31.287591  6644 sgd_solver.cpp:105] Iteration 111000, lr = 0.001
I1211 10:19:37.640004  6644 solver.cpp:218] Iteration 111100 (15.7432 iter/s, 6.35195s/100 iters), loss = 0.304206
I1211 10:19:37.640004  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.9
I1211 10:19:37.640004  6644 solver.cpp:237]     Train net output #1: loss = 0.304205 (* 1 = 0.304205 loss)
I1211 10:19:37.640004  6644 sgd_solver.cpp:105] Iteration 111100, lr = 0.001
I1211 10:19:43.992470  6644 solver.cpp:218] Iteration 111200 (15.7426 iter/s, 6.35221s/100 iters), loss = 0.246672
I1211 10:19:43.992470  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:19:43.992470  6644 solver.cpp:237]     Train net output #1: loss = 0.246672 (* 1 = 0.246672 loss)
I1211 10:19:43.992470  6644 sgd_solver.cpp:105] Iteration 111200, lr = 0.001
I1211 10:19:50.340919  6644 solver.cpp:218] Iteration 111300 (15.7535 iter/s, 6.34779s/100 iters), loss = 0.277026
I1211 10:19:50.340919  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.89
I1211 10:19:50.340919  6644 solver.cpp:237]     Train net output #1: loss = 0.277026 (* 1 = 0.277026 loss)
I1211 10:19:50.340919  6644 sgd_solver.cpp:105] Iteration 111300, lr = 0.001
I1211 10:19:56.686357  6644 solver.cpp:218] Iteration 111400 (15.7595 iter/s, 6.34536s/100 iters), loss = 0.335271
I1211 10:19:56.686357  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:19:56.686357  6644 solver.cpp:237]     Train net output #1: loss = 0.335271 (* 1 = 0.335271 loss)
I1211 10:19:56.686357  6644 sgd_solver.cpp:105] Iteration 111400, lr = 0.001
I1211 10:20:02.748291  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:20:02.998808  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.caffemodel
I1211 10:20:03.013314  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_111500.solverstate
I1211 10:20:03.017313  6644 solver.cpp:330] Iteration 111500, Testing net (#0)
I1211 10:20:03.017313  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:20:04.539446 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:20:04.599449  6644 solver.cpp:397]     Test net output #0: accuracy = 0.6768
I1211 10:20:04.599449  6644 solver.cpp:397]     Test net output #1: loss = 1.25578 (* 1 = 1.25578 loss)
I1211 10:20:04.660451  6644 solver.cpp:218] Iteration 111500 (12.5415 iter/s, 7.97354s/100 iters), loss = 0.364827
I1211 10:20:04.660451  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.86
I1211 10:20:04.660451  6644 solver.cpp:237]     Train net output #1: loss = 0.364826 (* 1 = 0.364826 loss)
I1211 10:20:04.660451  6644 sgd_solver.cpp:105] Iteration 111500, lr = 0.001
I1211 10:20:11.014902  6644 solver.cpp:218] Iteration 111600 (15.7399 iter/s, 6.35328s/100 iters), loss = 0.301048
I1211 10:20:11.014902  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:20:11.014902  6644 solver.cpp:237]     Train net output #1: loss = 0.301048 (* 1 = 0.301048 loss)
I1211 10:20:11.014902  6644 sgd_solver.cpp:105] Iteration 111600, lr = 0.001
I1211 10:20:17.366355  6644 solver.cpp:218] Iteration 111700 (15.744 iter/s, 6.35164s/100 iters), loss = 0.26866
I1211 10:20:17.366355  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.93
I1211 10:20:17.366355  6644 solver.cpp:237]     Train net output #1: loss = 0.26866 (* 1 = 0.26866 loss)
I1211 10:20:17.366355  6644 sgd_solver.cpp:105] Iteration 111700, lr = 0.001
I1211 10:20:23.713809  6644 solver.cpp:218] Iteration 111800 (15.7567 iter/s, 6.34652s/100 iters), loss = 0.281849
I1211 10:20:23.713809  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.92
I1211 10:20:23.713809  6644 solver.cpp:237]     Train net output #1: loss = 0.281849 (* 1 = 0.281849 loss)
I1211 10:20:23.713809  6644 sgd_solver.cpp:105] Iteration 111800, lr = 0.001
I1211 10:20:30.061360  6644 solver.cpp:218] Iteration 111900 (15.7544 iter/s, 6.34743s/100 iters), loss = 0.291556
I1211 10:20:30.061360  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.94
I1211 10:20:30.061360  6644 solver.cpp:237]     Train net output #1: loss = 0.291556 (* 1 = 0.291556 loss)
I1211 10:20:30.061360  6644 sgd_solver.cpp:105] Iteration 111900, lr = 0.001
I1211 10:20:36.103917  2860 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:20:36.352932  6644 solver.cpp:447] Snapshotting to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.caffemodel
I1211 10:20:36.367933  6644 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/cifar100/snaps/cifar100_slimnet_300k_max_L15_v2_withnonlin_iter_112000.solverstate
I1211 10:20:36.372933  6644 solver.cpp:330] Iteration 112000, Testing net (#0)
I1211 10:20:36.372933  6644 net.cpp:676] Ignoring source layer accuracy_training
I1211 10:20:37.893040 13556 data_layer.cpp:73] Restarting data prefetching from start.
I1211 10:20:37.953042  6644 solver.cpp:397]     Test net output #0: accuracy = 0.677
I1211 10:20:37.953042  6644 solver.cpp:397]     Test net output #1: loss = 1.26254 (* 1 = 1.26254 loss)
I1211 10:20:38.015044  6644 solver.cpp:218] Iteration 112000 (12.5741 iter/s, 7.95286s/100 iters), loss = 0.366947
I1211 10:20:38.015044  6644 solver.cpp:237]     Train net output #0: accuracy_training = 0.91
I1211 10:20:38.015044  6644 solver.cpp:237]     Train net output #1: loss = 0.366947 (* 1 = 0.366947 loss)
I1211 10:20:38.015044  6644 sgd_solver.cpp:105] Iteration 112000, lr = 0.001
I1211 10:20:44